{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to ShapeWorks! An Integrated Suite for Shape Representation and Analysis & more... What is ShapeWorks? ShapeWorks is a free, open-source suite of software tools that uses a flexible method for automated construction of compact statistical landmark-based shape models of ensembles of anatomical shapes that do not rely on any specific surface parameterization. The method requires very little preprocessing or parameter tuning and applies to a wide range of shape analysis problems, including nonmanifold surfaces and arbitrary topology objects. ShapeWorks includes tools for preprocessing data, computing landmark-based shape models, and visualizing the results. Latest & Greatest Release Notes Please visit Release Notes to know about the ShapeWorks' cutting-edge features and improvements. Why use ShapeWorks? ShapeWorks is the only publicly available tool that learns a population-specific anatomical mapping in a way that respects population variability without relying on a user-defined template/atlas. It produces more detailed surface-to-surface correspondences than traditional landmark-based approaches. Its optimized approach offers increased power for testing biological hypotheses of shape and shape differences, captures subtle shape variations, and decreases observer bias for reproducible scientific results. ShapeWorks can discover the underlying mode of variation in the box-bump ensemble in comparison to existing publicly available shape modeling software packages ShapeWorks Get-Togethers Where to start? What are the latest features? How to customize existing workflows? A monthly virtual get-together meeting for ShapeWorks users is organized to demonstrate new features, discuss the customization of the shape modeling workflow to users' own use cases, and get suggestions to improve the software and documentation. If you would like to be added to the invitation list, please send an email to shireen-at-sci-dot-utah-dot-edu . ShapeWorks in Action! The underlying scientific premise of ShapeWorks, particle-based shape modeling, is a groupwise approach to placing landmarks (i.e., correspondences) that consider variability in the entire cohort of images. ShapeWorks uses a set of interacting particle systems, one for each shape, to produce optimal sets of surface correspondences in an ensemble. Particles interact with one another via mutually repelling forces to cover and, therefore, describe surface geometry optimally. Particles are positioned on surfaces automatically by optimizing the model's information content via an entropy optimization scheme. In particular, ShapeWorks explicitly models the inherent trade-off between the model's statistical simplicity (i.e., compactness) in the shape space and the accuracy of the shape representations (i.e., good surface samplings) in the configuration space. ShapeWorks has been effective in various applications, including psychology, biological phenotyping, cardiology, and orthopedics. See relevant papers and ShapeWorks Success Stories .","title":"Home"},{"location":"index.html#welcome-to-shapeworks","text":"An Integrated Suite for Shape Representation and Analysis & more...","title":"Welcome to ShapeWorks!"},{"location":"index.html#what-is-shapeworks","text":"ShapeWorks is a free, open-source suite of software tools that uses a flexible method for automated construction of compact statistical landmark-based shape models of ensembles of anatomical shapes that do not rely on any specific surface parameterization. The method requires very little preprocessing or parameter tuning and applies to a wide range of shape analysis problems, including nonmanifold surfaces and arbitrary topology objects. ShapeWorks includes tools for preprocessing data, computing landmark-based shape models, and visualizing the results.","title":"What is ShapeWorks?"},{"location":"index.html#latest-greatest","text":"Release Notes Please visit Release Notes to know about the ShapeWorks' cutting-edge features and improvements.","title":"Latest &amp; Greatest"},{"location":"index.html#why-use-shapeworks","text":"ShapeWorks is the only publicly available tool that learns a population-specific anatomical mapping in a way that respects population variability without relying on a user-defined template/atlas. It produces more detailed surface-to-surface correspondences than traditional landmark-based approaches. Its optimized approach offers increased power for testing biological hypotheses of shape and shape differences, captures subtle shape variations, and decreases observer bias for reproducible scientific results. ShapeWorks can discover the underlying mode of variation in the box-bump ensemble in comparison to existing publicly available shape modeling software packages","title":"Why use ShapeWorks?"},{"location":"index.html#shapeworks-get-togethers","text":"Where to start? What are the latest features? How to customize existing workflows? A monthly virtual get-together meeting for ShapeWorks users is organized to demonstrate new features, discuss the customization of the shape modeling workflow to users' own use cases, and get suggestions to improve the software and documentation. If you would like to be added to the invitation list, please send an email to shireen-at-sci-dot-utah-dot-edu .","title":"ShapeWorks Get-Togethers"},{"location":"index.html#shapeworks-in-action","text":"The underlying scientific premise of ShapeWorks, particle-based shape modeling, is a groupwise approach to placing landmarks (i.e., correspondences) that consider variability in the entire cohort of images. ShapeWorks uses a set of interacting particle systems, one for each shape, to produce optimal sets of surface correspondences in an ensemble. Particles interact with one another via mutually repelling forces to cover and, therefore, describe surface geometry optimally. Particles are positioned on surfaces automatically by optimizing the model's information content via an entropy optimization scheme. In particular, ShapeWorks explicitly models the inherent trade-off between the model's statistical simplicity (i.e., compactness) in the shape space and the accuracy of the shape representations (i.e., good surface samplings) in the configuration space. ShapeWorks has been effective in various applications, including psychology, biological phenotyping, cardiology, and orthopedics. See relevant papers and ShapeWorks Success Stories .","title":"ShapeWorks in Action!"},{"location":"todo.html","text":"Documentation ToDo List To add/edit for docs index.md#shapeworks-in-action : Add the box bump examples and reference our benchmark study. index.md#shapeworks-in-action : Add video for illustration index.md#with-shapeworks-you-can : show case ShapeWorks in different studies (e.g., ortho and cardilogy) and cite relevant papers index.md and repo readme: Showcase our SSM benchmark study users/papers.md : Add links to the papers and update the list of papers with recent ones about/release-notes.md : Fix links in release notes about/team.md : May be some pics dev/autodoc.md : Which dir to use for autodoc for commands? Spell and grammar check all markdown files use-cases/ellipsoid.md : Add a figure showing some samples of the dataset that highlight the mode of variation Be consistent: multi-scale vs multiscale in the md files Be consistent: we vs you (e.g., in use cases) Add illustrating images to the groom steps in groom and specific use cases Update md files for groom/optimize and use cases to reflect the new workflow (icp, bounding box, crop) that does not apply explicitly resample images/segmentations beyond the isoresample step and use transforamtions as input to the optimization For groom and use cases, update the reference selection documentation to reflect any recent updates in this process (e.g., meshes, pairwise distance matrix ... etc) workflow/XX.md : Review and edit the workflow md files. Add/update the decription of single vs multiscale. use-cases/ellipsoid.md : Add video/snapshots for the optimized model and groomed data use-cases/XX.md : Update optimization parameters based on Examples/Python use-cases/XX.md : Update use cases to reflect multi-scale being integrated into the optimizer Add documentation for femur_mesh and lumps use cases use-cases/left-atrium.md :Check the left atrium use case for saving txt files to carry over images to reflect recent changes (consolidation) Consolidate docs/pdfs Consolidate docs/backlog Add to use cases how to run with prepped data Add mean and PCA visuals for ellipsoid and fixed domain ellipsoid use cases Review and edit ellipsoid use cases Review and edit femur use case Review and edit right ventricle use case Once meshes work, update the femur use case to indicate that this is optional and add femur_mesh use case Instructions on how to add videos to the documentation (thumbnails that link to youtube) Instructions to add a new use case Revise the right ventricle use case after release it. Might need more results to illustrate group differences. Instructions for Doxygen and building doxygen. dev/commands.md :How to add shapeworks commands? dev/gh-actions.md : getting started with github actions dev/python-apis.md : How to Add Python APIs? dev/tests.md : How to Add and Run Unit Tests? Remove all hard-coded links outside docs (relative paths) except for (1) web links (e.g. segmentation softwares, markdown edits ... etc) and (2) shapeworks releases, and shapeworks data portal. To investigate for docs How can we use repo_url in markdown files to link to repo files? -- not needed any more, we won't include any links outside docs except for weblinks, shapeworks releases and data portal. How to use google_analytics? For ShapeWorks repo Change the autodoc dir for commands in github action and release, see 'dev/autodoc.md' Update DocumentationUtilsPackage to generate md for mkdocs (ShapeworksCommand.md) Add mkdocs build for autodoc (deploy) to github actions and release When a use case launches Studio, does it load groomed data? It does, based on the launch function that takes in distance transforms, local, and world point files. Review and edit comments in the .py A fixed domain use case that include grooming the new samples Revisit the fixed domain use case, why do we need the path for the mean shape? why not use the mean of the fixed domains? Release and update the right ventricle use case Misc Add link to documentation http://sciinstitute.github.io/ShapeWorks/ to shapeworks.sci.utah.edu Prepped vs groomed? Be consistent in documentation, tags for RunUseCase, and output folders from running use cases","title":"Documentation ToDo List"},{"location":"todo.html#documentation-todo-list","text":"","title":"Documentation ToDo List"},{"location":"todo.html#to-addedit-for-docs","text":"index.md#shapeworks-in-action : Add the box bump examples and reference our benchmark study. index.md#shapeworks-in-action : Add video for illustration index.md#with-shapeworks-you-can : show case ShapeWorks in different studies (e.g., ortho and cardilogy) and cite relevant papers index.md and repo readme: Showcase our SSM benchmark study users/papers.md : Add links to the papers and update the list of papers with recent ones about/release-notes.md : Fix links in release notes about/team.md : May be some pics dev/autodoc.md : Which dir to use for autodoc for commands? Spell and grammar check all markdown files use-cases/ellipsoid.md : Add a figure showing some samples of the dataset that highlight the mode of variation Be consistent: multi-scale vs multiscale in the md files Be consistent: we vs you (e.g., in use cases) Add illustrating images to the groom steps in groom and specific use cases Update md files for groom/optimize and use cases to reflect the new workflow (icp, bounding box, crop) that does not apply explicitly resample images/segmentations beyond the isoresample step and use transforamtions as input to the optimization For groom and use cases, update the reference selection documentation to reflect any recent updates in this process (e.g., meshes, pairwise distance matrix ... etc) workflow/XX.md : Review and edit the workflow md files. Add/update the decription of single vs multiscale. use-cases/ellipsoid.md : Add video/snapshots for the optimized model and groomed data use-cases/XX.md : Update optimization parameters based on Examples/Python use-cases/XX.md : Update use cases to reflect multi-scale being integrated into the optimizer Add documentation for femur_mesh and lumps use cases use-cases/left-atrium.md :Check the left atrium use case for saving txt files to carry over images to reflect recent changes (consolidation) Consolidate docs/pdfs Consolidate docs/backlog Add to use cases how to run with prepped data Add mean and PCA visuals for ellipsoid and fixed domain ellipsoid use cases Review and edit ellipsoid use cases Review and edit femur use case Review and edit right ventricle use case Once meshes work, update the femur use case to indicate that this is optional and add femur_mesh use case Instructions on how to add videos to the documentation (thumbnails that link to youtube) Instructions to add a new use case Revise the right ventricle use case after release it. Might need more results to illustrate group differences. Instructions for Doxygen and building doxygen. dev/commands.md :How to add shapeworks commands? dev/gh-actions.md : getting started with github actions dev/python-apis.md : How to Add Python APIs? dev/tests.md : How to Add and Run Unit Tests? Remove all hard-coded links outside docs (relative paths) except for (1) web links (e.g. segmentation softwares, markdown edits ... etc) and (2) shapeworks releases, and shapeworks data portal.","title":"To add/edit for docs"},{"location":"todo.html#to-investigate-for-docs","text":"How can we use repo_url in markdown files to link to repo files? -- not needed any more, we won't include any links outside docs except for weblinks, shapeworks releases and data portal. How to use google_analytics?","title":"To investigate for docs"},{"location":"todo.html#for-shapeworks-repo","text":"Change the autodoc dir for commands in github action and release, see 'dev/autodoc.md' Update DocumentationUtilsPackage to generate md for mkdocs (ShapeworksCommand.md) Add mkdocs build for autodoc (deploy) to github actions and release When a use case launches Studio, does it load groomed data? It does, based on the launch function that takes in distance transforms, local, and world point files. Review and edit comments in the .py A fixed domain use case that include grooming the new samples Revisit the fixed domain use case, why do we need the path for the mean shape? why not use the mean of the fixed domains? Release and update the right ventricle use case","title":"For ShapeWorks repo"},{"location":"todo.html#misc","text":"Add link to documentation http://sciinstitute.github.io/ShapeWorks/ to shapeworks.sci.utah.edu Prepped vs groomed? Be consistent in documentation, tags for RunUseCase, and output folders from running use cases","title":"Misc"},{"location":"about/contact.html","text":"Contact Us Users Forum Our user forum is located on Gitter ShapeWorks User Forum . You can access the ShapeWorks Gitter using the link above or download the Gitter app for any platform or your phone. It provides easy integration with GitHub and has no limitation on the number of messages, as is the case with other services. Users Mailing List Please join our mailing list by sending a message to sympa@sci.utah.edu with the subject subscribe shapeworks-users and an empty body. You can also email any questions, bugs, or feature requests to shapeworks-users@sci.utah.edu. Developers Mailing List You can join our developer support mailing list by sending a message to sympa@sci.utah.edu with the subject subscribe shapeworks-dev-support and an empty body. As a developer, if you encounter any problems or bugs, please report them using the issue tracker on GitHub . This includes feature requests. Feel free to add improvements using git pull requests. You can also email ShapeWorkers at shapeworks-dev-support@sci.utah.edu.","title":"Contact Us"},{"location":"about/contact.html#contact-us","text":"","title":"Contact Us"},{"location":"about/contact.html#users-forum","text":"Our user forum is located on Gitter ShapeWorks User Forum . You can access the ShapeWorks Gitter using the link above or download the Gitter app for any platform or your phone. It provides easy integration with GitHub and has no limitation on the number of messages, as is the case with other services.","title":"Users Forum"},{"location":"about/contact.html#users-mailing-list","text":"Please join our mailing list by sending a message to sympa@sci.utah.edu with the subject subscribe shapeworks-users and an empty body. You can also email any questions, bugs, or feature requests to shapeworks-users@sci.utah.edu.","title":"Users Mailing List"},{"location":"about/contact.html#developers-mailing-list","text":"You can join our developer support mailing list by sending a message to sympa@sci.utah.edu with the subject subscribe shapeworks-dev-support and an empty body. As a developer, if you encounter any problems or bugs, please report them using the issue tracker on GitHub . This includes feature requests. Feel free to add improvements using git pull requests. You can also email ShapeWorkers at shapeworks-dev-support@sci.utah.edu.","title":"Developers Mailing List"},{"location":"about/license.html","text":"ShapeWorks License ShapeWorks is available for free and is open source under the MIT License. The MIT License Copyright (c) 2012 Scientific Computing and Imaging Institute, University of Utah. License for the specific language governing rights and limitations under Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/license.html#shapeworks-license","text":"ShapeWorks is available for free and is open source under the MIT License. The MIT License Copyright (c) 2012 Scientific Computing and Imaging Institute, University of Utah. License for the specific language governing rights and limitations under Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"ShapeWorks License"},{"location":"about/release-notes.html","text":"Release Notes ShapeWorks 5.5.0 - 2020-10-15 What's New User's Support Revamped documentation: New documentation to support both end-users and open-source developer community in one easily navigable place. This documentation includes background information about statistical shape modeling, the scientific premise of ShapeWorks, and how to get started. It also demonstrates the latest software features, exemplar use cases, and instructions to build/install ShapeWorks. Optimized shape models for use cases: All datasets on the ShapeWorks Data Portal now have the shape model output from running the use cases with a corresponding analyze.xml for launching Studio. Users can cd to where the data is extracted and call ShapeWorksStudio analyze.xml to visualize these shape models. ShapeWorks Back-end ShapeWorks directly on meshes: ShapeWorks now supports particle optimization directly on triangular surface meshes. This mesh support also enables working with open meshes without additional user inputs. See ShapeWorks Directly on Meshes for more details. For exemplar use cases, see Femur Mesh: SSM directly from meshes and Lumps: SSM directly from meshes to learn how to get started. This mesh support allows for significant memory savings (9.2GB to 53MB in the femur use case) . New ShapeWorks API: Consolidation of image-based and segmentation-based grooming tools that creates a stable and reusable API making it much easier and more flexible for users to groom their datasets. This includes a full complement of unit tests. See ShapeWorks API for more details. All-in-one Studio Front-end Feature maps support: Studio supports the integration of 3d volume feature maps to map imaging data to the optimized shape model. See New in ShapeWorksStudio for more details. New interface for group analysis: Studio supports group definitions from spreadsheets. The new interface supports multiple group sets within the same project file and categorical groups compared to the old binary groups (i.e., yes/no) setting. See New in ShapeWorksStudio for more details. User notes in Studio: Studio stores/loads a rich text notes section in the spreadsheet. Deep Learning & Shape Modeling New Python package for model-based data augmentation: A Python package for data augmentation has been added. See Data Augmentation for Deep Learning for more details. DeepSSM Python package: A Python package has been added for a deep learning framework that estimates statistical representations of shape directly from unsegmented images once trained. See SSMs Directly from Images for more details. New DeepSSM use case: We added a new use case called deep_ssm that demonstrates data augmentation and deep learning on the femur data. See Femur SSM Directly from Images for more details. Improvements User's Support Improved data/output organization for use cases: Use case organization has been updated such that downloaded data goes into one folder ( ShapeWorks/Examples/Python/Data/ ) and use case output goes into another ( ShapeWorks/Examples/Python/Output/ ). This organization will avoid re-downloading use case datasets if the user deleted the output folder. Enable only-shape data for the femur use case: Femur use case demonstrates the processing workflow starting from surface meshes and can now be run without grooming images (in case they are not available). To groom with images, use the --start_with_image_and_segmentation_data tag. If this tag is not included, images will not be used in grooming. ShapeWorks Back-end Improved constrained particle optimization: An improved implementation for cutting planes that support single and multiple cutting planes per sample has been added. This can be used in modeling scenarios where statistical modeling/analysis is needed for a region-of-interest on the anatomy/object-class at hand without having to affect the input data. See Ellipsoid: Cutting Planes and Femur with Cutting Planes for exemplar use cases. All-in-one Studio Front-end Improved interface design for Studio: User interface improvements to Studio have been added. These improvements include collapsable analysis panels to improve screen usability, a cleaner file menu, an about box that shows website information and software version, and a splash screen that enables the opening of new/recent projects. Fixes ShapeWorks Back-end Constraint-aware particles initialization and optimization: The initialization and optimization steps now respect the user-defined constraints when using signed distance transforms. Hence, at no point, particles will violate the constraints, allowing for arbitrarily defined cutting planes. Constraint-aware particles splitting: Added constraint aware particle splitting for signed distance transforms. The particle splitting only occurred towards a single quadrant. This is fixed by allowing particle splits to shift in a different direction for each particle in every domain. Constraint-aware particle projection: Projecting particles on the surface while manipulating/optimizing particle position now respects the user-defined constraints. ShapeWorks 5.4.1 - 2020-06-15 Fixes ShapeWorksStudio: Fixed crash when importing data on a new/blank project. ShapeWorks 5.4.0 - 2020-06-10 What's New New, flexible ShapeWorks project file format: New spreadsheet (XLSX) based project file format that can easily handle multiple shape modeling scenarios. It is now fully integrated into Studio. See the ellipsoid studio example in Examples/Studio/ellipsoid.xlsx for an example. Exporting shape parameters: Added PCA Raw Component Score Export to Studio. New Getting Started documentation: New Getting started documentation goes over shape modeling workflow documentation, different ShapeWorks interfaces with a video illustration for Studio. Lower memory footprint and faster optimization: ShapeWorks is now using OpenVDB , a more memory-efficient data structure, for signed distance transforms. Along with other refactoring and code optimizations, ShapeWorks now uses 85% less memory (from 57.09GB to 9.67GB in one use case). Additionally, the particle optimizer is now 2X faster . Automated development builds: We now offer up-to-date development binary builds that track the master branch, available here . Please understand that these are in-progress development builds, not official releases. Improvements Improved scalability for Studio: Files now loaded on-demand as necessary. Restructured datasets portal: Better and more consistent directory structure for use cases datasets. See datasets guidelines for more details. Visit our ShapeWorks Portal to register and download datasets to run use cases . Lower memory footprint for estimating correspondences for new shapes on existing shape models: For usage, only distance transforms for the new shapes will be loaded. Improved use case documentation: Better documentation for the RunUseCase input arguments. Documentation for running existing shape models: Added instructions on how to load a pre-trained shape model without running the full pipeline. Consistent cross-platform splitting direction: Correspondence point splits take a random direction, but are now consistent and repeatable across platforms for reproducible shape models. Fixes Mesh export in Studio: Changed exported meshes to be compatible with CloudCompare ShapeWorks 5.3.0 - 2020-02-20 What's New Use cases: Added fixed domains use case that demonstrates adding a single shape to an existing shape model Use cases: Added ellipsoid evaluation use that demonstrates the quantitative evaluation of the ellipsoid New commands: Added commands to the shapeworks executable to quantitatively evaluate shape models: read-particle-system, compactness, generalization, specificity New shapeworks executables commands: read-image, write-image, antialias, isoresample, binarize, recenter-image, pad. New build method: Separate build_dependencies script (see Documentation/Build/BUILD.md) Improvements Use cases: Updated femur use case with an interactive cutting plane selection Studio: Replaced bar chart with explained variance chart ShapeWorks 5.2.2 - 2020-01-09 Fixes Returned to superbuild.sh build process Fixed non-Qt build Fixed Windows conda_installs.bat Studio: Fixed import and processing of non-RAI image volumes Studio: Fix centering of groomed and reconstructed volumes ShapeWorks 5.2.1 - 2019-11-09 Fixes Fix examples, binaries ShapeWorks 5.2.0 - 2019-11-07 What's New Studio: Added live particle optimization updates Studio: Added \"Stop optimization\" button Use cases: Added a femur use case that demonstrates grooming surface meshes along with imaging data for shape modeling Use cases: Added a left atrium use case that demonstrates grooming images data along with shape data and build multi-scale shape models Use cases: New portal downloadable example data (ellipsoid, left atrium, and femurs) Windows installer and binary releases for Mac and Linux CMake-based superbuild with all dependencies built automatically Improvements Studio: Added support for reading View2 parameter files Studio: Added legacy (View2) surface reconstructor Studio: Replaced optimization library with same used by ShapeWorksRun Fixes Studio: Fixes crashes on exit during optimizations (clean shutdown of threads)","title":"Release Notes"},{"location":"about/release-notes.html#release-notes","text":"","title":"Release Notes"},{"location":"about/release-notes.html#shapeworks-550-2020-10-15","text":"","title":"ShapeWorks 5.5.0 - 2020-10-15"},{"location":"about/release-notes.html#whats-new","text":"","title":"What's New"},{"location":"about/release-notes.html#users-support","text":"Revamped documentation: New documentation to support both end-users and open-source developer community in one easily navigable place. This documentation includes background information about statistical shape modeling, the scientific premise of ShapeWorks, and how to get started. It also demonstrates the latest software features, exemplar use cases, and instructions to build/install ShapeWorks. Optimized shape models for use cases: All datasets on the ShapeWorks Data Portal now have the shape model output from running the use cases with a corresponding analyze.xml for launching Studio. Users can cd to where the data is extracted and call ShapeWorksStudio analyze.xml to visualize these shape models.","title":"User's Support"},{"location":"about/release-notes.html#shapeworks-back-end","text":"ShapeWorks directly on meshes: ShapeWorks now supports particle optimization directly on triangular surface meshes. This mesh support also enables working with open meshes without additional user inputs. See ShapeWorks Directly on Meshes for more details. For exemplar use cases, see Femur Mesh: SSM directly from meshes and Lumps: SSM directly from meshes to learn how to get started. This mesh support allows for significant memory savings (9.2GB to 53MB in the femur use case) . New ShapeWorks API: Consolidation of image-based and segmentation-based grooming tools that creates a stable and reusable API making it much easier and more flexible for users to groom their datasets. This includes a full complement of unit tests. See ShapeWorks API for more details.","title":"ShapeWorks Back-end"},{"location":"about/release-notes.html#all-in-one-studio-front-end","text":"Feature maps support: Studio supports the integration of 3d volume feature maps to map imaging data to the optimized shape model. See New in ShapeWorksStudio for more details. New interface for group analysis: Studio supports group definitions from spreadsheets. The new interface supports multiple group sets within the same project file and categorical groups compared to the old binary groups (i.e., yes/no) setting. See New in ShapeWorksStudio for more details. User notes in Studio: Studio stores/loads a rich text notes section in the spreadsheet.","title":"All-in-one Studio Front-end"},{"location":"about/release-notes.html#deep-learning-shape-modeling","text":"New Python package for model-based data augmentation: A Python package for data augmentation has been added. See Data Augmentation for Deep Learning for more details. DeepSSM Python package: A Python package has been added for a deep learning framework that estimates statistical representations of shape directly from unsegmented images once trained. See SSMs Directly from Images for more details. New DeepSSM use case: We added a new use case called deep_ssm that demonstrates data augmentation and deep learning on the femur data. See Femur SSM Directly from Images for more details.","title":"Deep Learning &amp; Shape Modeling"},{"location":"about/release-notes.html#improvements","text":"","title":"Improvements"},{"location":"about/release-notes.html#users-support_1","text":"Improved data/output organization for use cases: Use case organization has been updated such that downloaded data goes into one folder ( ShapeWorks/Examples/Python/Data/ ) and use case output goes into another ( ShapeWorks/Examples/Python/Output/ ). This organization will avoid re-downloading use case datasets if the user deleted the output folder. Enable only-shape data for the femur use case: Femur use case demonstrates the processing workflow starting from surface meshes and can now be run without grooming images (in case they are not available). To groom with images, use the --start_with_image_and_segmentation_data tag. If this tag is not included, images will not be used in grooming.","title":"User's Support"},{"location":"about/release-notes.html#shapeworks-back-end_1","text":"Improved constrained particle optimization: An improved implementation for cutting planes that support single and multiple cutting planes per sample has been added. This can be used in modeling scenarios where statistical modeling/analysis is needed for a region-of-interest on the anatomy/object-class at hand without having to affect the input data. See Ellipsoid: Cutting Planes and Femur with Cutting Planes for exemplar use cases.","title":"ShapeWorks Back-end"},{"location":"about/release-notes.html#all-in-one-studio-front-end_1","text":"Improved interface design for Studio: User interface improvements to Studio have been added. These improvements include collapsable analysis panels to improve screen usability, a cleaner file menu, an about box that shows website information and software version, and a splash screen that enables the opening of new/recent projects.","title":"All-in-one Studio Front-end"},{"location":"about/release-notes.html#fixes","text":"","title":"Fixes"},{"location":"about/release-notes.html#shapeworks-back-end_2","text":"Constraint-aware particles initialization and optimization: The initialization and optimization steps now respect the user-defined constraints when using signed distance transforms. Hence, at no point, particles will violate the constraints, allowing for arbitrarily defined cutting planes. Constraint-aware particles splitting: Added constraint aware particle splitting for signed distance transforms. The particle splitting only occurred towards a single quadrant. This is fixed by allowing particle splits to shift in a different direction for each particle in every domain. Constraint-aware particle projection: Projecting particles on the surface while manipulating/optimizing particle position now respects the user-defined constraints.","title":"ShapeWorks Back-end"},{"location":"about/release-notes.html#shapeworks-541-2020-06-15","text":"","title":"ShapeWorks 5.4.1 - 2020-06-15"},{"location":"about/release-notes.html#fixes_1","text":"ShapeWorksStudio: Fixed crash when importing data on a new/blank project.","title":"Fixes"},{"location":"about/release-notes.html#shapeworks-540-2020-06-10","text":"","title":"ShapeWorks 5.4.0 - 2020-06-10"},{"location":"about/release-notes.html#whats-new_1","text":"New, flexible ShapeWorks project file format: New spreadsheet (XLSX) based project file format that can easily handle multiple shape modeling scenarios. It is now fully integrated into Studio. See the ellipsoid studio example in Examples/Studio/ellipsoid.xlsx for an example. Exporting shape parameters: Added PCA Raw Component Score Export to Studio. New Getting Started documentation: New Getting started documentation goes over shape modeling workflow documentation, different ShapeWorks interfaces with a video illustration for Studio. Lower memory footprint and faster optimization: ShapeWorks is now using OpenVDB , a more memory-efficient data structure, for signed distance transforms. Along with other refactoring and code optimizations, ShapeWorks now uses 85% less memory (from 57.09GB to 9.67GB in one use case). Additionally, the particle optimizer is now 2X faster . Automated development builds: We now offer up-to-date development binary builds that track the master branch, available here . Please understand that these are in-progress development builds, not official releases.","title":"What's New"},{"location":"about/release-notes.html#improvements_1","text":"Improved scalability for Studio: Files now loaded on-demand as necessary. Restructured datasets portal: Better and more consistent directory structure for use cases datasets. See datasets guidelines for more details. Visit our ShapeWorks Portal to register and download datasets to run use cases . Lower memory footprint for estimating correspondences for new shapes on existing shape models: For usage, only distance transforms for the new shapes will be loaded. Improved use case documentation: Better documentation for the RunUseCase input arguments. Documentation for running existing shape models: Added instructions on how to load a pre-trained shape model without running the full pipeline. Consistent cross-platform splitting direction: Correspondence point splits take a random direction, but are now consistent and repeatable across platforms for reproducible shape models.","title":"Improvements"},{"location":"about/release-notes.html#fixes_2","text":"Mesh export in Studio: Changed exported meshes to be compatible with CloudCompare","title":"Fixes"},{"location":"about/release-notes.html#shapeworks-530-2020-02-20","text":"","title":"ShapeWorks 5.3.0 - 2020-02-20"},{"location":"about/release-notes.html#whats-new_2","text":"Use cases: Added fixed domains use case that demonstrates adding a single shape to an existing shape model Use cases: Added ellipsoid evaluation use that demonstrates the quantitative evaluation of the ellipsoid New commands: Added commands to the shapeworks executable to quantitatively evaluate shape models: read-particle-system, compactness, generalization, specificity New shapeworks executables commands: read-image, write-image, antialias, isoresample, binarize, recenter-image, pad. New build method: Separate build_dependencies script (see Documentation/Build/BUILD.md)","title":"What's New"},{"location":"about/release-notes.html#improvements_2","text":"Use cases: Updated femur use case with an interactive cutting plane selection Studio: Replaced bar chart with explained variance chart","title":"Improvements"},{"location":"about/release-notes.html#shapeworks-522-2020-01-09","text":"","title":"ShapeWorks 5.2.2 - 2020-01-09"},{"location":"about/release-notes.html#fixes_3","text":"Returned to superbuild.sh build process Fixed non-Qt build Fixed Windows conda_installs.bat Studio: Fixed import and processing of non-RAI image volumes Studio: Fix centering of groomed and reconstructed volumes","title":"Fixes"},{"location":"about/release-notes.html#shapeworks-521-2019-11-09","text":"","title":"ShapeWorks 5.2.1 - 2019-11-09"},{"location":"about/release-notes.html#fixes_4","text":"Fix examples, binaries","title":"Fixes"},{"location":"about/release-notes.html#shapeworks-520-2019-11-07","text":"","title":"ShapeWorks 5.2.0 - 2019-11-07"},{"location":"about/release-notes.html#whats-new_3","text":"Studio: Added live particle optimization updates Studio: Added \"Stop optimization\" button Use cases: Added a femur use case that demonstrates grooming surface meshes along with imaging data for shape modeling Use cases: Added a left atrium use case that demonstrates grooming images data along with shape data and build multi-scale shape models Use cases: New portal downloadable example data (ellipsoid, left atrium, and femurs) Windows installer and binary releases for Mac and Linux CMake-based superbuild with all dependencies built automatically","title":"What's New"},{"location":"about/release-notes.html#improvements_3","text":"Studio: Added support for reading View2 parameter files Studio: Added legacy (View2) surface reconstructor Studio: Replaced optimization library with same used by ShapeWorksRun","title":"Improvements"},{"location":"about/release-notes.html#fixes_5","text":"Studio: Fixes crashes on exit during optimizations (clean shutdown of threads)","title":"Fixes"},{"location":"about/team.html","text":"Meet ShapeWorkers! Principal Investigators Shireen Elhabian Ross Whitaker Software Developers Alan Morris Cameron Christensen Archanasri Subramanian Researchers Riddhish Bhalodia Atefeh Ghanaatikashani Jadie Adams Hong Xu Karthik Karanth Krithika Iyer Past Contributors Joshua Cates (now @Orthogrid Inc.) Manasi Datar Brig Bagley Praful Agrawal (now @Amazon Inc.) Oleks Korshak (now @Microsoft) Anupama Goparaju (now @ Galileo Financial Technologies)","title":"Meet ShapeWorkers!"},{"location":"about/team.html#meet-shapeworkers","text":"","title":"Meet ShapeWorkers!"},{"location":"about/team.html#principal-investigators","text":"Shireen Elhabian Ross Whitaker","title":"Principal Investigators"},{"location":"about/team.html#software-developers","text":"Alan Morris Cameron Christensen Archanasri Subramanian","title":"Software Developers"},{"location":"about/team.html#researchers","text":"Riddhish Bhalodia Atefeh Ghanaatikashani Jadie Adams Hong Xu Karthik Karanth Krithika Iyer","title":"Researchers"},{"location":"about/team.html#past-contributors","text":"Joshua Cates (now @Orthogrid Inc.) Manasi Datar Brig Bagley Praful Agrawal (now @Amazon Inc.) Oleks Korshak (now @Microsoft) Anupama Goparaju (now @ Galileo Financial Technologies)","title":"Past Contributors"},{"location":"backlog/ShapeworksCmdTools.html","text":"ShapeWorks Tools The shapeworks executable has a variety of tools for data alignment, processing of images and meshes, reading and converting various file formats, optimization, and analysis. These tasks are described below. [Image Tools] [Mesh Tools] [Alignment] [Analysis] [Optimization] [File Utilities] Image Tools Image manipulation is used to blah blah blah... [AntiAliasing] (#anti-aliasing) [ClipVolume] [CloseHoles] [Cropping Images] [ExtractGivenLabelImage] [FastMarching] [FindLargestBoundingBox] [PadVolumeWithConstant] [ReflectVolumes] [ResampleVolumesToBeIsotropic] [ThresholdImages] [TopologyPreservingSmoothing] [WriteImageInfoToText] Read This tool reads an image. Command Line Name: read-image It uses the following input arguments: --name = Name of file to read. Write This tool writes an image. Command Line Name: write-image It uses the following input arguments: --name = Name of file to write. AntiAliasing This tool antialiases binary volumes. Command Line Name: antialias It uses the following input arguments: --maxrmserror = The maximum RMS error determines how fast the solver converges. (Range [0.0, 1.0]) [default 0.01]. --numiterations = Number of iterations [default 50]. --numlayers = Number of layers around a 3d pixel to use for this computation [default image dims]. PadVolumeWithConstant This tool pads a contant value in the x-, y-, and z- directions of a given volume. Command Line Name: pad It uses the following input arguments: --padding = Number of voxels to be padded in each direction. --value = Value to be used to fill padded voxels. ResampleVolumesToBeIsotropic This tool resamples given mri/binary volumes to have isotropic voxel spacing. Command Line Name: isoresample It uses the following input arguments: --isospacing = The isotropic spacing in all dimensions. --sizez = Image size in x-direction [ize is autmatically estimated from input image]. --sizey = Image size in y-direction [size is autmatically estimated from input image]. --sizez = Image size in z-direction [size is autmatically estimated from input image]. Binarize This tool binarizes an image at some given threshold. Command Line Name: binarize It uses the following input arguments: --threshold = Resulting image has two values for pixels: > threshold set to inside value, <= threshold set to outside value [default epsilon]. --inside = Value of pixels > threshold [default 1.0]. --outside = Value of pixels <= threshold [default 0.0]. Recenter This tool recenters an image by changing its origin in image header to the physical coordinates of the center of the image. Command Line Name: recenter-image ReflectVolumes A command line tool that reflect 3d volume images with respect to image center and specific axis. It uses the following input arguments: -inFilename - Image file name which needs to be reflected. - outFilename - Output file name for the reflected image - paddingSize - Axis along which it needs to be reflected - centerFilename - The filename where the image center information will be stored. ClipVolume Slice 3D Volume Using Cutting Planes Takes set of .nrrd volumes and corresponding cutting planes, and chops the volume accordingly It uses a parameter file with the following tags - num_shapes: number of volumes to be processed (put this correctly) - inputs: paths to input files - outputs: paths to output files - cutting_planes : Set of cutting planes, expressed as 3 3D points separated by spaces example : 1 2 -3 0 34 102 -23.4 2 0.44 CloseHoles TODO Cropping Images This tool is to crop the images given the starting index and bounding box size Provide the smallest index and bounding box values from the above tool to this tool as inputs. Inputs: -inFilename - Input image file name -outFilename - Output image file name -MRIinFilename - MRI image file name -MRIoutFilename - MRI output image file name -bbX - bounding box value in X direction -bbY - bounding box value in Y direction -bbZ - bounding box value in Z direction -startingIndexX - starting index in X direction -startingIndexY - starting index in Y direction -startingIndexZ - starting index in Z direction ExtractGivenLabelImage A command line tool that extracts/isolates a specific voxel label from a given multi-label volume and outputs the corresponding binary image. It uses the following input arguments: -inFilename The filename of the input image from which label has to be extracted. -labelVal The label value which has to be extracted. -outFilename The filename of the output image. FastMarching A command line tool that computes distance transform volume from a binary (antialiased) image ... --inFilename = The filename of the input image to be processed. --outFilename = The filename of the output distance transform image. --isoValue = The level set value that defines the interface between foreground and background. FindLargestBoundingBox A command line tool that o compute largest bounding box size given a set of images. It uses the following input arguments: -inFilename - A text file with the file names for which the largest size has to be computed. - outPrefix - output prefix to be used to save the parameters for the estimated bounding box - paddingSize - number of extra voxels in each direction to pad the largest bounding box, checks agains min image size is performed to make sure that this padding won't get out of bounds for the smallest image in the file names provides ThresholdImages A command line tool that threholds a given image into a binary label based on upper and lower intensity bounds given by the user..... --inFilename = Input image file path. --outFilename = The filename of the output threshold image. --lowerThresholdLevel = The lower threshold level (optional, default = FLT_MIN) --upperThresholdLevel = The upper threshold level (optional, default = FLT_MAX) --insideValue = The inside pixel value after threshold --outsideValue = The outside pixel value after threshold TopologyPreservingSmoothing A parameter file based tool that smoothes distance transforms while preserving shape's topology. Here is an example of an input xml file that can be used. parameter file tags are as follows: -inputs The filenames of the input distance transforms to be smoothed. -dtFiles The filenames of the output smoothed distance transforms. -outputs The filenames of the output smoothed isosurface images. -verbose Show each intermediate step [default 0]. -isoValue Isovalue to be used to define the surface in the input distance transform [default 0.0]. -smoothing_iterations Number of iterations to perform smoothing [default 10]. -alpha Smoothing parameter in I' = (max-min). \\frac{1}{1+exp(-\\frac{1-\\beta}{\\alpha)} + min [default 10.5]. -beta Smoothing parameter in I' = (max-min). \\frac{1}{1+exp(-\\frac{1-\\beta}{\\alpha)} + min [default 10.0]. -propagationScale The PropagationScaling parameter can be used to switch from propagation outwards (POSITIVE) versus propagating inwards (NEGATIVE). [default 20.0]. WriteImageInfoToText A command line tool that extracts header information from a nrrd file and write it to a text file It uses the following input arguments: -inFilename The filename of the input image to extract its header information (origin, size, spacing). -outPrefix The output prefix to be used to save header info to _ .txt where info is origin, size, and spacing. Mesh Tools Mesh manipulation is used to blah blah blah... ClipClosedSurface ComputeCurvatureAndCoordFiles ComputeGeodesicDistanceToCurve ComputeGeodesicDistanceToLandmark ComputeGeodesicDistanceFromVerticesToPoints ComputeMeshGeodesics ComputeRasterizationVolumeOriginAndSize ExtractVertexFeatureFromMesh FillMeshHoles FixCuttingPlanes GenerateBinaryAndDTImagesFromMeshes GenerateFeatureGradientFiles GenerateFidsFiles GenerateFidsFilesFromMeshes GetFeatureVolume ParticleBasedSurfaceReconstruction PreviewCmd ProbeFeatureVolumesAtMeshVertices ProbeNormals ProjectPointsOntoMesh ComputeMeanNormals GenerateNormalFeaFiles Reflect Meshes RemoveFidsDTLeakage SmoothMesh ClipClosedSurface Given a set of meshes (vtk) and corresponding cutting planes, this tool clip each mesh with its cutting plane and result in a closed surface. A buffer distance can be used to move the cutting plane in the opposite direction of its normal to leave a buffer region below the cutting plane It uses a parameter file with the following tags: - input_meshes: a list of vtk file names for meshes to be processed - output_meshes: the vtk filenames of the output to be produced - cutting_planes: a list of cutting planes in three-point form ( x1 y1 z1 x2 y2 z2 x3 y3 z3 ) - buffer_distance: phyical distance of the buffer region ComputeCurvatureAndCoordFiles Compute curvature and x,y,z on vertices on given triangular meshes (ply format) It uses a parameter file with the following tags - mesh: a list of ply file names for meshes to be processed - outPath: path to save feature files ComputeGeodesicDistanceToCurve Compute geodesic distances on given triangular meshes (ply format) to a set of given curves It uses a parameter file with the following tags - with_fids: a flag to indicate whether to use fids to generate the geodesics, if 0, kdtree will be used instead (filenames will be affected), if true, we need to provide the spacing, size and origin of their distance transforms that generated those fids files - mesh: a list of ply file names for meshes to be processed - fids: a list of corresponding fids files (in case with_fids = 1) - origin_x, origin_t, origin_z: the origin in physical units of the resulting distance transform - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - size_x, size_y, size_z: the size (rows,cols,slices) of the resulting distance transform - curves_prefix: where we can find the point files of the curves, this should be the common part of the filename for all given mesh files - curves_suffix: string (optional) that will suffix all curve files - curves_ext: the extension of the curve files - curve_labels: the name of each curve according to the suffix given for each curve pts file, this will also be used in the naming of the output feature files (fea) This tool uses fids to project a given landmark onto the closest VERTEX on the mesh then use this vertex as a seed for geodesics computation. It will be more accurate to project the point onto a triangle and use its vertices as seeds to initiate geodesics to the rest of the mesh then use geodesic approximation scheme to obtain the geodesic from the projected point to all the mesh vertices. ComputeGeodesicDistanceToLandmark Compute geodesic distances on given triangular meshes (ply format) to a set of given landmarks It uses a parameter file with the following tags\" - with_fids: a flag to indicate whether to use fids to generate the geodesics, if 0, kdtree will be used instead (filenames will be affected), if true, we need to provide the spacing, size and origin of their distance transforms that generated those fids files - origin_x, origin_t, origin_z: the origin in physical units of the resulting distance transform - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - size_x, size_y, size_z: the size (rows,cols,slices) of the resulting distance transform - mesh: a list of ply file names for meshes to be processed - fids: a list of corresponding fids files (in case with_fids = 1) - landmark_files: a list of text files containing the 3d point locations of the landmarks associated with each mesh (a single file for each mesh with each row a different landmark xyz) - landmark_labels: the name of each landmark specified in landmark files, this will be used in the naming of the output feature files (fea) This tool uses fids to project a given landmark onto the closest VERTEX on the mesh then use this vertex as a seed for geodesics computation. It will be more accurate to project the point onto a triangle and use its vertices as seeds to initiate geodesics to the rest of the mesh then use geodesic approximation scheme to obtain the geodesic from the projected point to all the mesh vertices. ComputeGeodesicDistanceFromVerticesToPoints Generate a text file containing geodesic distance to vertices of given triangular mesh (ply) for given set of points with their triangle information (x y z triangleId alpha beta gamma) Usage: ComputeGeodesicDistanceFromVerticesToPoints meshFile(.ply) pointsFile outFileName ComputeMeshGeodesics Compute pairwise vertices geodesic distances on given triangular meshes (ply format) It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed - geo: a list of geo file names for meshes to be saved - stopping_distance_factor: percentage of the great circle circumference where vertices will be assigned an very large geodesic distance ComputeRasterizationVolumeOriginAndSize Given a set of meshes (vtk), compute the origin and size of a volume that would contain the rasterization of each mesh so that all meshes will be embedded within the same volume characteristics It uses a parameter file with the following tags\" - mesh: a list of vtk file names for meshes to be processed - centering: a flag to indicate whether build the to-be built rasterization is centered on the shape's center - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - pad_pixels: number of pixels to be padded in each dimension - out_origin_filename: the filename to save the origin information - out_size_filename: the filename to save the size information ExtractVertexFeatureFromMesh Extract scalar values associated to mesh vertices to txt file It uses a parameter file with the following tags: - inputs: a list of vtk file names for meshes to be processed - out_path: a directory (otional) to save output files, it not indicated, out files will be saved in the same directory as the mesh files - vtk_feature_name: feature name in the vtk file - out_feature_name: feature name to be used in the output file names, if not indicated, mesh filenames will be used as is - read_as_ply: to read ply rather than vtk files FillMeshHoles Given a set of meshes (vtk), this tool finds holes in a mesh and closes them. It uses a parameter file with the following tags: - inputs: a list of vtk file names for meshes to be processed - outputs: the vtk filenames of the output to be produced FixCuttingPlanes Given bunch of curves/landmarks for each shape, fix the orientation of the provided cutting plane to point to the side where the curves/landmarks exist It uses a parameter file with the following tags - mesh: a list of vtk/ply file names for meshes to be processed - input_planes: a list of txt file names for the planes to be processed - output_planes: a list of txt file names for the fixed planes to be saved - curves_prefix: where we can find the point files of the curves, this should be the common part of the filename for all given mesh files - curves_suffix: string (optional) that will suffix all curve files - curves_ext: the extension of the curve files - curve_labels: the name of each curve according to the suffix given for each curve pts file GenerateBinaryAndDTImagesFromMeshes Compute binary images (i.e. segmentation) and their distance transforms of a given set of meshes (ply format) It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed - origin_x, origin_y, origin_z: the origin in physical units of the resulting distance transform - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - size_x, size_y, size_z: the size (rows,cols,slices) of the resulting distance transform GenerateFeatureGradientFiles Probe gradient at vertices and generate feature gradient files It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed \".ply\" - fea_vol: feature volume(s) \".nrrd\" - fea_per_mesh: number of features \"int\" GenerateFidsFiles Compute face index map of a given set of meshes (ply format) along with its signed distance transfrom using spatial hashing It uses a parameter file with the following tags\" - dist: a list of approximate distance transforms - mesh: a list of ply file names for meshes to be processed - number_of_subvoxels: : number of subvoxels to divid each voxel (higher improve subvoxel accuarcy esp for meshes with high curvature regions) - number_of_voxels: number of voxels to construct a supervoxel - narrow_band: a narrow band defined in phyical units to limit the supvoxel-accuarate distance transform computation, distance values of voxels outside this band will be inferred using fids - ball_radius_factor: to reduce the radius(b) at each super-voxel. (At times b is too big and contains the whole mesh. Use < 1) - num_threads: number of thread to be spawned GenerateFidsFilesFromMeshes Compute face index map of a given set of meshes (ply format) along with its signed distance transfrom using spatial hashing It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed - origin_x, origin_y, origin_z: the origin in physical units of the resulting distance transform - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - size_x, size_y, size_z: the size (rows,cols,slices) of the resulting distance transform - number_of_subvoxels: : number of subvoxels to divid each voxel (higher improve subvoxel accuarcy esp for meshes with high curvature regions) - number_of_voxels: number of voxels to construct a supervoxel - narrow_band: a narrow band defined in phyical units to limit the supvoxel-accuarate distance transform computation, distance values of voxels outside this band will be inferred using fids - ball_radius_factor: to reduce the radius(b) at each super-voxel. (At times b is too big and contains the whole mesh. Use < 1) - num_threads: number of thread to be spawned GetFeatureVolume Compute a volumetric representation of fea files by propagating feature values (from fea files) from mesh surface to a narrowband surrounding the mesh It uses a parameter file with the following tags\" - with_fids: a flag to indicate whether to use fids to generate the geodesics, if 0, kdtree will be used instead - mesh: a list of ply file names for meshes to be processed - fids: a list of corresponding fids files (in case with_fids = 1) - dist: a list of corresponding distance transforms to indicate where to fill the feature volume, i.e. the narrowband - fea_per_mesh: number of fea files to be processed per mesh - fea: a list of fea files to be processed such that the first fea_per_mesh fea files correspond to the first mesh and so on. - narrow_band: distance in physical coordinates from the mesh its inside and outside where we propagate feature values. PreviewCmd This is a commandline tool that encapsulate preview-based triangular mesh processing tools including decimation, smoothing and fixing, it has the following inputs: --inFile: the input vtk filename of the mesh to be processed. --outFile: the output vtk filename. --fixWinding: do element winding fix (default: 1) --decimate: perform mesh decimation (default: 1) --doLaplacianSmoothingBeforeDecimation: perform laplacian smoothing before decimation (default: 1) --doLaplacianSmoothingAfterDecimation: perform laplacian smoothing after decimation (default: 1) --smoothingLambda: laplacian smoothing lambda (default: 0.5) --smoothingIterations: laplacian smoothing number of iterations (default: 1) --decimationPercentage: percentage of target number of clusters/vertices (default: 0.5) ProbeFeatureVolumesAtMeshVertices Given a set of meshes (vtk) and corresponding feature volumes where these meshes live, this tool probe the feature volumes at each mesh vertex and output vtk meshes with scalar field defined based on such probing process (report the feature values from the feature volumes at the mesh vertices) It uses a parameter file with the following tags\" - input_meshes: a list of vtk file names for meshes to be processed - feature_volumes: a list of image files (3D) to be probed corresponding to the given set of meshes - output_meshes: the vtk filenames of the output to be produced ProbeNormals Usage: ProbeNormals DTfilename pointsFilename outFileName ComputeMeanNormals Compute mean normals using spherical coordinates for given normals for a set of shapes It uses a parameter file with the following tags - normals: a list of files containing normals at a set of points on each shape (output files from ProbeNormals) - pointsCount: number of points in every shape file - outFileName: full filename (with path) to save resulting file (default: mean.normals.txt) GenerateNormalFeaFiles Probe normals at vertices and save as fea files It uses a parameter file with the following tags - DT: a list of DT file names to be processed - mesh: a list of mesh file names to be processed Reflect Meshes A command line tool that reflect meshes with respect to a specified center and specific axis. -inFilename - Mesh file to be reflected. -outFilename - The filename of the output reflection mesh. -reflectCenterFilename(Optional) - The filename for origin about which reflection occurs. (Default reflection happes about the center of the mesh bounding box) -inputDirection - Direction along which it needs to be reflected -meshFormat(Optional) - The IO mesh format (Default = vtk, another option is ply) RemoveFidsDTLeakage This tool can be used a postprocessing for fids distance trasnform to fix voxels that are mis-signed as in or out of the isosurface in fids computation (mainly due to irregular triangulation It uses a parameter file with the following tags\" - fids_dist: a list of distance transforms computed via fids - approx_dist: the corresponding approximate distances (from rasterization then dt computation) - out_dist: output distance transform filenames SmoothMesh Given a set of meshes (vtk), this tool laplacian smooth the mesh It uses a parameter file with the following tags: - inputs: a list of vtk file names for meshes to be processed - outputs: the vtk filenames of the output to be produced - iterations: number of smoothing iterations - relaxation_factor: amount of vertex displacement in each iteration Alignment Alignment tools are used to blah blah blah... [ICPRigid3DImageRegistration] [ICPRigid3DMeshRegistration] [ReflectMeshes] [TranslateShapeToImageOrigin] [Resize_origin_resampleShape] ICPRigid3DImageRegistration A command line tool that performs iterative closed point (ICP) 3D rigid registration on a pair of images. It uses the following input arguments: Input: -targetDistanceMap: the distance map of target image. -sourceDistanceMap: the distance map of source image. -sourceSegmentation: the segmentation of source image. -isoValue: as we need to get point set from surface for ICP, this iso value is required to get the isosurface. The default value is 0.0. -icpIterations: the number of iterations user want to run. Output: -solutionSegmentation: the filename of the aligned segmentation of source image. ICPRigid3DMeshRegistration Performs iterative closed point (ICP) rigid registration on a pair of vtk meshes. It uses a parameter file that would enable to specify the source mesh (moving) and the target mesh (fixed) to be used to estimated the rigid transformation matrix then apply the same transformation on other meshes defined in the source mesh domain to be mapped to the target domain parameter file tags are as follows: - source_mesh: vtk filename of the moving mesh - target_mesh: vtk filename of the fixed mesh - out_mesh : vtk filename of the aligned moving mesh to be save - out_transform : txt filename to save the estimated transformation - source_meshes: (optional) a list of vtk filenames for meshes defined in the source mesh domain to be mapped to the target domain using the same transformation matrix estimated. - out_meshes : a list vtk filenames to save source_meshes after applying the transformation matrix. - mode : Registration mode rigid, similarity, affine (default: similarity) - icp_iterations: number of iterations - debug: verbose debugging information - visualize: display the resulting alignment ReflectMeshes Reflect meshes to make data in whole ensemble align in same direction. It uses a parameter file with the following tags\" - inputs: a list of vtk/ply file names for meshes to be relfected - output: a list of output filenames - is_unstructured_grid: : (a scalar 1/0 for each input file) indicated whether the input meshes are in vtk unstructured grid format (Default 0) - direction: which axis to reflect with respect to, 0 for x-axis, 1 fir y-axis, and 2 for z-axis TranslateShapeToImageOrigin A command line tool that performs translational alignment of a given shape image based on either its center of mass or a given 3d point. It uses the following input arguments: -inFilename The filename of the input shape to be transformed. -outFilename The filename of the output transformed shape. -MRIinFilename The assoicated image filename to be transformed. -MRIoutFilename The filename of the output transformed image. -useCenterOfMass A flag to transform image using center of mass. -centerX x-coordinate of a user-defined center point. -centerY y-coordinate of a user-defined center point. -centerZ z-coordinate of a user-defined center point. -parameterFilename The filename to store the transform parameters Resize_origin_resampleShape TODO Analysis Analysis is used to blah blah blah... Optimization Optimization is used to blah blah blah... File Utilities The File Utilities are used to read and convert data stored in a variety of different formats. [GENERIC MESH FORMAT CONVERSION] [fea2vtk] [stl2ply] [stl2vtk] [vtk2ply] [VTKUnstructuredGridToPolyData] [MeshFromDT] GENERIC MESH FORMAT CONVERSION Converts different mesh formats (vtk, ply, stl, obj) to other formats It uses a parameter file with the following tags - input_format: integer value specifying the input mesh file format (1) .vtk (2) .ply (3) .stl (4) .obj - output_format: integer value specifying the output mesh file format (1) .vtk (2) .ply (3) .stl - input_mesh: paths of the input meshes - output_mesh: paths of the output meshes Usage: ./GenericMeshConversion paramfile fea2vtk Read in feature files (.fea) and their corresponding triangular mesh (ply) and output a vtk file containing the feature values as scalar onto the mesh (colormap) It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed - fea_per_mesh: number of fea files to be processed per mesh - fea: a list of fea files to be processed such that the first fea_per_mesh fea files correspond to the first mesh and so on. stl2ply Commandline tool to convert stl file format to ply file format ... \"Usage: stl2vtk inFilename(.stl) outFilename(.ply) stl2vtk Commandline tool to convert stl file format to vtk file format ... \"Usage: stl2vtk inFilename(.stl) outFilename(.vtk) vtk2ply Commandline tool to convert vtk file format to ply file format ... \"Usage: vtk2ply inFilename(.vtk) outFilename(.ply) VTKUnstructuredGridToPolyData Convert vtk unstructered grid data to vtk polydata. It uses a paramfile of the following tags: - inputs: list of input vtk files containing unstructured grid data - outputs: list of output vtk files which will be saved containing the vtk polydata version of the corresponding input vtkfiles MeshFromDT TODO FeaFromVTK TODO","title":"ShapeworksCmdTools"},{"location":"backlog/ShapeworksCmdTools.html#_1","text":"","title":""},{"location":"backlog/ShapeworksCmdTools.html#shapeworks-tools","text":"","title":"ShapeWorks Tools"},{"location":"backlog/ShapeworksCmdTools.html#_2","text":"The shapeworks executable has a variety of tools for data alignment, processing of images and meshes, reading and converting various file formats, optimization, and analysis. These tasks are described below.","title":""},{"location":"backlog/ShapeworksCmdTools.html#_3","text":"[Image Tools] [Mesh Tools] [Alignment] [Analysis] [Optimization] [File Utilities]","title":""},{"location":"backlog/ShapeworksCmdTools.html#image-tools","text":"Image manipulation is used to blah blah blah... [AntiAliasing] (#anti-aliasing) [ClipVolume] [CloseHoles] [Cropping Images] [ExtractGivenLabelImage] [FastMarching] [FindLargestBoundingBox] [PadVolumeWithConstant] [ReflectVolumes] [ResampleVolumesToBeIsotropic] [ThresholdImages] [TopologyPreservingSmoothing] [WriteImageInfoToText]","title":"Image Tools"},{"location":"backlog/ShapeworksCmdTools.html#read","text":"This tool reads an image. Command Line Name: read-image It uses the following input arguments: --name = Name of file to read.","title":"Read"},{"location":"backlog/ShapeworksCmdTools.html#write","text":"This tool writes an image. Command Line Name: write-image It uses the following input arguments: --name = Name of file to write.","title":"Write"},{"location":"backlog/ShapeworksCmdTools.html#antialiasing","text":"This tool antialiases binary volumes. Command Line Name: antialias It uses the following input arguments: --maxrmserror = The maximum RMS error determines how fast the solver converges. (Range [0.0, 1.0]) [default 0.01]. --numiterations = Number of iterations [default 50]. --numlayers = Number of layers around a 3d pixel to use for this computation [default image dims].","title":" AntiAliasing"},{"location":"backlog/ShapeworksCmdTools.html#padvolumewithconstant","text":"This tool pads a contant value in the x-, y-, and z- directions of a given volume. Command Line Name: pad It uses the following input arguments: --padding = Number of voxels to be padded in each direction. --value = Value to be used to fill padded voxels.","title":"PadVolumeWithConstant"},{"location":"backlog/ShapeworksCmdTools.html#resamplevolumestobeisotropic","text":"This tool resamples given mri/binary volumes to have isotropic voxel spacing. Command Line Name: isoresample It uses the following input arguments: --isospacing = The isotropic spacing in all dimensions. --sizez = Image size in x-direction [ize is autmatically estimated from input image]. --sizey = Image size in y-direction [size is autmatically estimated from input image]. --sizez = Image size in z-direction [size is autmatically estimated from input image].","title":"ResampleVolumesToBeIsotropic"},{"location":"backlog/ShapeworksCmdTools.html#binarize","text":"This tool binarizes an image at some given threshold. Command Line Name: binarize It uses the following input arguments: --threshold = Resulting image has two values for pixels: > threshold set to inside value, <= threshold set to outside value [default epsilon]. --inside = Value of pixels > threshold [default 1.0]. --outside = Value of pixels <= threshold [default 0.0].","title":"Binarize"},{"location":"backlog/ShapeworksCmdTools.html#recenter","text":"This tool recenters an image by changing its origin in image header to the physical coordinates of the center of the image. Command Line Name: recenter-image","title":"Recenter"},{"location":"backlog/ShapeworksCmdTools.html#reflectvolumes","text":"A command line tool that reflect 3d volume images with respect to image center and specific axis. It uses the following input arguments: -inFilename - Image file name which needs to be reflected. - outFilename - Output file name for the reflected image - paddingSize - Axis along which it needs to be reflected - centerFilename - The filename where the image center information will be stored.","title":"ReflectVolumes"},{"location":"backlog/ShapeworksCmdTools.html#clipvolume","text":"Slice 3D Volume Using Cutting Planes Takes set of .nrrd volumes and corresponding cutting planes, and chops the volume accordingly It uses a parameter file with the following tags - num_shapes: number of volumes to be processed (put this correctly) - inputs: paths to input files - outputs: paths to output files - cutting_planes : Set of cutting planes, expressed as 3 3D points separated by spaces example : 1 2 -3 0 34 102 -23.4 2 0.44","title":"ClipVolume"},{"location":"backlog/ShapeworksCmdTools.html#closeholes","text":"TODO","title":"CloseHoles"},{"location":"backlog/ShapeworksCmdTools.html#cropping-images","text":"This tool is to crop the images given the starting index and bounding box size Provide the smallest index and bounding box values from the above tool to this tool as inputs. Inputs: -inFilename - Input image file name -outFilename - Output image file name -MRIinFilename - MRI image file name -MRIoutFilename - MRI output image file name -bbX - bounding box value in X direction -bbY - bounding box value in Y direction -bbZ - bounding box value in Z direction -startingIndexX - starting index in X direction -startingIndexY - starting index in Y direction -startingIndexZ - starting index in Z direction","title":"Cropping Images"},{"location":"backlog/ShapeworksCmdTools.html#extractgivenlabelimage","text":"A command line tool that extracts/isolates a specific voxel label from a given multi-label volume and outputs the corresponding binary image. It uses the following input arguments: -inFilename The filename of the input image from which label has to be extracted. -labelVal The label value which has to be extracted. -outFilename The filename of the output image.","title":"ExtractGivenLabelImage"},{"location":"backlog/ShapeworksCmdTools.html#fastmarching","text":"A command line tool that computes distance transform volume from a binary (antialiased) image ... --inFilename = The filename of the input image to be processed. --outFilename = The filename of the output distance transform image. --isoValue = The level set value that defines the interface between foreground and background.","title":"FastMarching"},{"location":"backlog/ShapeworksCmdTools.html#findlargestboundingbox","text":"A command line tool that o compute largest bounding box size given a set of images. It uses the following input arguments: -inFilename - A text file with the file names for which the largest size has to be computed. - outPrefix - output prefix to be used to save the parameters for the estimated bounding box - paddingSize - number of extra voxels in each direction to pad the largest bounding box, checks agains min image size is performed to make sure that this padding won't get out of bounds for the smallest image in the file names provides","title":"FindLargestBoundingBox"},{"location":"backlog/ShapeworksCmdTools.html#thresholdimages","text":"A command line tool that threholds a given image into a binary label based on upper and lower intensity bounds given by the user..... --inFilename = Input image file path. --outFilename = The filename of the output threshold image. --lowerThresholdLevel = The lower threshold level (optional, default = FLT_MIN) --upperThresholdLevel = The upper threshold level (optional, default = FLT_MAX) --insideValue = The inside pixel value after threshold --outsideValue = The outside pixel value after threshold","title":"ThresholdImages"},{"location":"backlog/ShapeworksCmdTools.html#topologypreservingsmoothing","text":"A parameter file based tool that smoothes distance transforms while preserving shape's topology. Here is an example of an input xml file that can be used. parameter file tags are as follows: -inputs The filenames of the input distance transforms to be smoothed. -dtFiles The filenames of the output smoothed distance transforms. -outputs The filenames of the output smoothed isosurface images. -verbose Show each intermediate step [default 0]. -isoValue Isovalue to be used to define the surface in the input distance transform [default 0.0]. -smoothing_iterations Number of iterations to perform smoothing [default 10]. -alpha Smoothing parameter in I' = (max-min). \\frac{1}{1+exp(-\\frac{1-\\beta}{\\alpha)} + min [default 10.5]. -beta Smoothing parameter in I' = (max-min). \\frac{1}{1+exp(-\\frac{1-\\beta}{\\alpha)} + min [default 10.0]. -propagationScale The PropagationScaling parameter can be used to switch from propagation outwards (POSITIVE) versus propagating inwards (NEGATIVE). [default 20.0].","title":"TopologyPreservingSmoothing"},{"location":"backlog/ShapeworksCmdTools.html#writeimageinfototext","text":"A command line tool that extracts header information from a nrrd file and write it to a text file It uses the following input arguments: -inFilename The filename of the input image to extract its header information (origin, size, spacing). -outPrefix The output prefix to be used to save header info to _ .txt where info is origin, size, and spacing.","title":"WriteImageInfoToText"},{"location":"backlog/ShapeworksCmdTools.html#mesh-tools","text":"Mesh manipulation is used to blah blah blah...","title":"Mesh Tools"},{"location":"backlog/ShapeworksCmdTools.html#clipclosedsurface","text":"","title":"ClipClosedSurface"},{"location":"backlog/ShapeworksCmdTools.html#computecurvatureandcoordfiles","text":"","title":"ComputeCurvatureAndCoordFiles"},{"location":"backlog/ShapeworksCmdTools.html#computegeodesicdistancetocurve","text":"","title":"ComputeGeodesicDistanceToCurve"},{"location":"backlog/ShapeworksCmdTools.html#computegeodesicdistancetolandmark","text":"","title":"ComputeGeodesicDistanceToLandmark"},{"location":"backlog/ShapeworksCmdTools.html#computegeodesicdistancefromverticestopoints","text":"","title":"ComputeGeodesicDistanceFromVerticesToPoints"},{"location":"backlog/ShapeworksCmdTools.html#computemeshgeodesics","text":"","title":"ComputeMeshGeodesics"},{"location":"backlog/ShapeworksCmdTools.html#computerasterizationvolumeoriginandsize","text":"","title":"ComputeRasterizationVolumeOriginAndSize"},{"location":"backlog/ShapeworksCmdTools.html#extractvertexfeaturefrommesh","text":"","title":"ExtractVertexFeatureFromMesh"},{"location":"backlog/ShapeworksCmdTools.html#fillmeshholes","text":"","title":"FillMeshHoles"},{"location":"backlog/ShapeworksCmdTools.html#fixcuttingplanes","text":"","title":"FixCuttingPlanes"},{"location":"backlog/ShapeworksCmdTools.html#generatebinaryanddtimagesfrommeshes","text":"","title":"GenerateBinaryAndDTImagesFromMeshes"},{"location":"backlog/ShapeworksCmdTools.html#generatefeaturegradientfiles","text":"","title":"GenerateFeatureGradientFiles"},{"location":"backlog/ShapeworksCmdTools.html#generatefidsfiles","text":"","title":"GenerateFidsFiles"},{"location":"backlog/ShapeworksCmdTools.html#generatefidsfilesfrommeshes","text":"","title":"GenerateFidsFilesFromMeshes"},{"location":"backlog/ShapeworksCmdTools.html#getfeaturevolume","text":"","title":"GetFeatureVolume"},{"location":"backlog/ShapeworksCmdTools.html#particlebasedsurfacereconstruction","text":"","title":"ParticleBasedSurfaceReconstruction"},{"location":"backlog/ShapeworksCmdTools.html#previewcmd","text":"","title":"PreviewCmd"},{"location":"backlog/ShapeworksCmdTools.html#probefeaturevolumesatmeshvertices","text":"","title":"ProbeFeatureVolumesAtMeshVertices"},{"location":"backlog/ShapeworksCmdTools.html#probenormals","text":"","title":"ProbeNormals"},{"location":"backlog/ShapeworksCmdTools.html#projectpointsontomesh","text":"","title":"ProjectPointsOntoMesh"},{"location":"backlog/ShapeworksCmdTools.html#computemeannormals","text":"","title":"ComputeMeanNormals"},{"location":"backlog/ShapeworksCmdTools.html#generatenormalfeafiles","text":"","title":"GenerateNormalFeaFiles"},{"location":"backlog/ShapeworksCmdTools.html#reflect-meshes","text":"","title":"Reflect Meshes"},{"location":"backlog/ShapeworksCmdTools.html#removefidsdtleakage","text":"","title":"RemoveFidsDTLeakage"},{"location":"backlog/ShapeworksCmdTools.html#smoothmesh","text":"","title":"SmoothMesh"},{"location":"backlog/ShapeworksCmdTools.html#clipclosedsurface_1","text":"Given a set of meshes (vtk) and corresponding cutting planes, this tool clip each mesh with its cutting plane and result in a closed surface. A buffer distance can be used to move the cutting plane in the opposite direction of its normal to leave a buffer region below the cutting plane It uses a parameter file with the following tags: - input_meshes: a list of vtk file names for meshes to be processed - output_meshes: the vtk filenames of the output to be produced - cutting_planes: a list of cutting planes in three-point form ( x1 y1 z1 x2 y2 z2 x3 y3 z3 ) - buffer_distance: phyical distance of the buffer region","title":"ClipClosedSurface"},{"location":"backlog/ShapeworksCmdTools.html#computecurvatureandcoordfiles_1","text":"Compute curvature and x,y,z on vertices on given triangular meshes (ply format) It uses a parameter file with the following tags - mesh: a list of ply file names for meshes to be processed - outPath: path to save feature files","title":"ComputeCurvatureAndCoordFiles"},{"location":"backlog/ShapeworksCmdTools.html#computegeodesicdistancetocurve_1","text":"Compute geodesic distances on given triangular meshes (ply format) to a set of given curves It uses a parameter file with the following tags - with_fids: a flag to indicate whether to use fids to generate the geodesics, if 0, kdtree will be used instead (filenames will be affected), if true, we need to provide the spacing, size and origin of their distance transforms that generated those fids files - mesh: a list of ply file names for meshes to be processed - fids: a list of corresponding fids files (in case with_fids = 1) - origin_x, origin_t, origin_z: the origin in physical units of the resulting distance transform - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - size_x, size_y, size_z: the size (rows,cols,slices) of the resulting distance transform - curves_prefix: where we can find the point files of the curves, this should be the common part of the filename for all given mesh files - curves_suffix: string (optional) that will suffix all curve files - curves_ext: the extension of the curve files - curve_labels: the name of each curve according to the suffix given for each curve pts file, this will also be used in the naming of the output feature files (fea) This tool uses fids to project a given landmark onto the closest VERTEX on the mesh then use this vertex as a seed for geodesics computation. It will be more accurate to project the point onto a triangle and use its vertices as seeds to initiate geodesics to the rest of the mesh then use geodesic approximation scheme to obtain the geodesic from the projected point to all the mesh vertices.","title":"ComputeGeodesicDistanceToCurve"},{"location":"backlog/ShapeworksCmdTools.html#computegeodesicdistancetolandmark_1","text":"Compute geodesic distances on given triangular meshes (ply format) to a set of given landmarks It uses a parameter file with the following tags\" - with_fids: a flag to indicate whether to use fids to generate the geodesics, if 0, kdtree will be used instead (filenames will be affected), if true, we need to provide the spacing, size and origin of their distance transforms that generated those fids files - origin_x, origin_t, origin_z: the origin in physical units of the resulting distance transform - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - size_x, size_y, size_z: the size (rows,cols,slices) of the resulting distance transform - mesh: a list of ply file names for meshes to be processed - fids: a list of corresponding fids files (in case with_fids = 1) - landmark_files: a list of text files containing the 3d point locations of the landmarks associated with each mesh (a single file for each mesh with each row a different landmark xyz) - landmark_labels: the name of each landmark specified in landmark files, this will be used in the naming of the output feature files (fea) This tool uses fids to project a given landmark onto the closest VERTEX on the mesh then use this vertex as a seed for geodesics computation. It will be more accurate to project the point onto a triangle and use its vertices as seeds to initiate geodesics to the rest of the mesh then use geodesic approximation scheme to obtain the geodesic from the projected point to all the mesh vertices.","title":"ComputeGeodesicDistanceToLandmark"},{"location":"backlog/ShapeworksCmdTools.html#computegeodesicdistancefromverticestopoints_1","text":"Generate a text file containing geodesic distance to vertices of given triangular mesh (ply) for given set of points with their triangle information (x y z triangleId alpha beta gamma) Usage: ComputeGeodesicDistanceFromVerticesToPoints meshFile(.ply) pointsFile outFileName","title":"ComputeGeodesicDistanceFromVerticesToPoints"},{"location":"backlog/ShapeworksCmdTools.html#computemeshgeodesics_1","text":"Compute pairwise vertices geodesic distances on given triangular meshes (ply format) It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed - geo: a list of geo file names for meshes to be saved - stopping_distance_factor: percentage of the great circle circumference where vertices will be assigned an very large geodesic distance","title":"ComputeMeshGeodesics"},{"location":"backlog/ShapeworksCmdTools.html#computerasterizationvolumeoriginandsize_1","text":"Given a set of meshes (vtk), compute the origin and size of a volume that would contain the rasterization of each mesh so that all meshes will be embedded within the same volume characteristics It uses a parameter file with the following tags\" - mesh: a list of vtk file names for meshes to be processed - centering: a flag to indicate whether build the to-be built rasterization is centered on the shape's center - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - pad_pixels: number of pixels to be padded in each dimension - out_origin_filename: the filename to save the origin information - out_size_filename: the filename to save the size information","title":"ComputeRasterizationVolumeOriginAndSize"},{"location":"backlog/ShapeworksCmdTools.html#extractvertexfeaturefrommesh_1","text":"Extract scalar values associated to mesh vertices to txt file It uses a parameter file with the following tags: - inputs: a list of vtk file names for meshes to be processed - out_path: a directory (otional) to save output files, it not indicated, out files will be saved in the same directory as the mesh files - vtk_feature_name: feature name in the vtk file - out_feature_name: feature name to be used in the output file names, if not indicated, mesh filenames will be used as is - read_as_ply: to read ply rather than vtk files","title":"ExtractVertexFeatureFromMesh"},{"location":"backlog/ShapeworksCmdTools.html#fillmeshholes_1","text":"Given a set of meshes (vtk), this tool finds holes in a mesh and closes them. It uses a parameter file with the following tags: - inputs: a list of vtk file names for meshes to be processed - outputs: the vtk filenames of the output to be produced","title":"FillMeshHoles"},{"location":"backlog/ShapeworksCmdTools.html#fixcuttingplanes_1","text":"Given bunch of curves/landmarks for each shape, fix the orientation of the provided cutting plane to point to the side where the curves/landmarks exist It uses a parameter file with the following tags - mesh: a list of vtk/ply file names for meshes to be processed - input_planes: a list of txt file names for the planes to be processed - output_planes: a list of txt file names for the fixed planes to be saved - curves_prefix: where we can find the point files of the curves, this should be the common part of the filename for all given mesh files - curves_suffix: string (optional) that will suffix all curve files - curves_ext: the extension of the curve files - curve_labels: the name of each curve according to the suffix given for each curve pts file","title":"FixCuttingPlanes"},{"location":"backlog/ShapeworksCmdTools.html#generatebinaryanddtimagesfrommeshes_1","text":"Compute binary images (i.e. segmentation) and their distance transforms of a given set of meshes (ply format) It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed - origin_x, origin_y, origin_z: the origin in physical units of the resulting distance transform - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - size_x, size_y, size_z: the size (rows,cols,slices) of the resulting distance transform","title":"GenerateBinaryAndDTImagesFromMeshes"},{"location":"backlog/ShapeworksCmdTools.html#generatefeaturegradientfiles_1","text":"Probe gradient at vertices and generate feature gradient files It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed \".ply\" - fea_vol: feature volume(s) \".nrrd\" - fea_per_mesh: number of features \"int\"","title":"GenerateFeatureGradientFiles"},{"location":"backlog/ShapeworksCmdTools.html#generatefidsfiles_1","text":"Compute face index map of a given set of meshes (ply format) along with its signed distance transfrom using spatial hashing It uses a parameter file with the following tags\" - dist: a list of approximate distance transforms - mesh: a list of ply file names for meshes to be processed - number_of_subvoxels: : number of subvoxels to divid each voxel (higher improve subvoxel accuarcy esp for meshes with high curvature regions) - number_of_voxels: number of voxels to construct a supervoxel - narrow_band: a narrow band defined in phyical units to limit the supvoxel-accuarate distance transform computation, distance values of voxels outside this band will be inferred using fids - ball_radius_factor: to reduce the radius(b) at each super-voxel. (At times b is too big and contains the whole mesh. Use < 1) - num_threads: number of thread to be spawned","title":"GenerateFidsFiles"},{"location":"backlog/ShapeworksCmdTools.html#generatefidsfilesfrommeshes_1","text":"Compute face index map of a given set of meshes (ply format) along with its signed distance transfrom using spatial hashing It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed - origin_x, origin_y, origin_z: the origin in physical units of the resulting distance transform - spacing_x, spacing_y, spacing_z: voxel spacing of the resulting distance transform - size_x, size_y, size_z: the size (rows,cols,slices) of the resulting distance transform - number_of_subvoxels: : number of subvoxels to divid each voxel (higher improve subvoxel accuarcy esp for meshes with high curvature regions) - number_of_voxels: number of voxels to construct a supervoxel - narrow_band: a narrow band defined in phyical units to limit the supvoxel-accuarate distance transform computation, distance values of voxels outside this band will be inferred using fids - ball_radius_factor: to reduce the radius(b) at each super-voxel. (At times b is too big and contains the whole mesh. Use < 1) - num_threads: number of thread to be spawned","title":"GenerateFidsFilesFromMeshes"},{"location":"backlog/ShapeworksCmdTools.html#getfeaturevolume_1","text":"Compute a volumetric representation of fea files by propagating feature values (from fea files) from mesh surface to a narrowband surrounding the mesh It uses a parameter file with the following tags\" - with_fids: a flag to indicate whether to use fids to generate the geodesics, if 0, kdtree will be used instead - mesh: a list of ply file names for meshes to be processed - fids: a list of corresponding fids files (in case with_fids = 1) - dist: a list of corresponding distance transforms to indicate where to fill the feature volume, i.e. the narrowband - fea_per_mesh: number of fea files to be processed per mesh - fea: a list of fea files to be processed such that the first fea_per_mesh fea files correspond to the first mesh and so on. - narrow_band: distance in physical coordinates from the mesh its inside and outside where we propagate feature values.","title":"GetFeatureVolume"},{"location":"backlog/ShapeworksCmdTools.html#previewcmd_1","text":"This is a commandline tool that encapsulate preview-based triangular mesh processing tools including decimation, smoothing and fixing, it has the following inputs: --inFile: the input vtk filename of the mesh to be processed. --outFile: the output vtk filename. --fixWinding: do element winding fix (default: 1) --decimate: perform mesh decimation (default: 1) --doLaplacianSmoothingBeforeDecimation: perform laplacian smoothing before decimation (default: 1) --doLaplacianSmoothingAfterDecimation: perform laplacian smoothing after decimation (default: 1) --smoothingLambda: laplacian smoothing lambda (default: 0.5) --smoothingIterations: laplacian smoothing number of iterations (default: 1) --decimationPercentage: percentage of target number of clusters/vertices (default: 0.5)","title":"PreviewCmd"},{"location":"backlog/ShapeworksCmdTools.html#probefeaturevolumesatmeshvertices_1","text":"Given a set of meshes (vtk) and corresponding feature volumes where these meshes live, this tool probe the feature volumes at each mesh vertex and output vtk meshes with scalar field defined based on such probing process (report the feature values from the feature volumes at the mesh vertices) It uses a parameter file with the following tags\" - input_meshes: a list of vtk file names for meshes to be processed - feature_volumes: a list of image files (3D) to be probed corresponding to the given set of meshes - output_meshes: the vtk filenames of the output to be produced","title":"ProbeFeatureVolumesAtMeshVertices"},{"location":"backlog/ShapeworksCmdTools.html#probenormals_1","text":"Usage: ProbeNormals DTfilename pointsFilename outFileName","title":"ProbeNormals"},{"location":"backlog/ShapeworksCmdTools.html#computemeannormals_1","text":"Compute mean normals using spherical coordinates for given normals for a set of shapes It uses a parameter file with the following tags - normals: a list of files containing normals at a set of points on each shape (output files from ProbeNormals) - pointsCount: number of points in every shape file - outFileName: full filename (with path) to save resulting file (default: mean.normals.txt)","title":"ComputeMeanNormals"},{"location":"backlog/ShapeworksCmdTools.html#generatenormalfeafiles_1","text":"Probe normals at vertices and save as fea files It uses a parameter file with the following tags - DT: a list of DT file names to be processed - mesh: a list of mesh file names to be processed","title":"GenerateNormalFeaFiles"},{"location":"backlog/ShapeworksCmdTools.html#reflect-meshes_1","text":"A command line tool that reflect meshes with respect to a specified center and specific axis. -inFilename - Mesh file to be reflected. -outFilename - The filename of the output reflection mesh. -reflectCenterFilename(Optional) - The filename for origin about which reflection occurs. (Default reflection happes about the center of the mesh bounding box) -inputDirection - Direction along which it needs to be reflected -meshFormat(Optional) - The IO mesh format (Default = vtk, another option is ply)","title":"Reflect Meshes"},{"location":"backlog/ShapeworksCmdTools.html#removefidsdtleakage_1","text":"This tool can be used a postprocessing for fids distance trasnform to fix voxels that are mis-signed as in or out of the isosurface in fids computation (mainly due to irregular triangulation It uses a parameter file with the following tags\" - fids_dist: a list of distance transforms computed via fids - approx_dist: the corresponding approximate distances (from rasterization then dt computation) - out_dist: output distance transform filenames","title":"RemoveFidsDTLeakage"},{"location":"backlog/ShapeworksCmdTools.html#smoothmesh_1","text":"Given a set of meshes (vtk), this tool laplacian smooth the mesh It uses a parameter file with the following tags: - inputs: a list of vtk file names for meshes to be processed - outputs: the vtk filenames of the output to be produced - iterations: number of smoothing iterations - relaxation_factor: amount of vertex displacement in each iteration","title":"SmoothMesh"},{"location":"backlog/ShapeworksCmdTools.html#alignment","text":"Alignment tools are used to blah blah blah... [ICPRigid3DImageRegistration] [ICPRigid3DMeshRegistration] [ReflectMeshes] [TranslateShapeToImageOrigin] [Resize_origin_resampleShape]","title":"Alignment"},{"location":"backlog/ShapeworksCmdTools.html#icprigid3dimageregistration","text":"A command line tool that performs iterative closed point (ICP) 3D rigid registration on a pair of images. It uses the following input arguments: Input: -targetDistanceMap: the distance map of target image. -sourceDistanceMap: the distance map of source image. -sourceSegmentation: the segmentation of source image. -isoValue: as we need to get point set from surface for ICP, this iso value is required to get the isosurface. The default value is 0.0. -icpIterations: the number of iterations user want to run. Output: -solutionSegmentation: the filename of the aligned segmentation of source image.","title":"ICPRigid3DImageRegistration"},{"location":"backlog/ShapeworksCmdTools.html#icprigid3dmeshregistration","text":"Performs iterative closed point (ICP) rigid registration on a pair of vtk meshes. It uses a parameter file that would enable to specify the source mesh (moving) and the target mesh (fixed) to be used to estimated the rigid transformation matrix then apply the same transformation on other meshes defined in the source mesh domain to be mapped to the target domain parameter file tags are as follows: - source_mesh: vtk filename of the moving mesh - target_mesh: vtk filename of the fixed mesh - out_mesh : vtk filename of the aligned moving mesh to be save - out_transform : txt filename to save the estimated transformation - source_meshes: (optional) a list of vtk filenames for meshes defined in the source mesh domain to be mapped to the target domain using the same transformation matrix estimated. - out_meshes : a list vtk filenames to save source_meshes after applying the transformation matrix. - mode : Registration mode rigid, similarity, affine (default: similarity) - icp_iterations: number of iterations - debug: verbose debugging information - visualize: display the resulting alignment","title":"ICPRigid3DMeshRegistration"},{"location":"backlog/ShapeworksCmdTools.html#reflectmeshes","text":"Reflect meshes to make data in whole ensemble align in same direction. It uses a parameter file with the following tags\" - inputs: a list of vtk/ply file names for meshes to be relfected - output: a list of output filenames - is_unstructured_grid: : (a scalar 1/0 for each input file) indicated whether the input meshes are in vtk unstructured grid format (Default 0) - direction: which axis to reflect with respect to, 0 for x-axis, 1 fir y-axis, and 2 for z-axis","title":"ReflectMeshes"},{"location":"backlog/ShapeworksCmdTools.html#translateshapetoimageorigin","text":"A command line tool that performs translational alignment of a given shape image based on either its center of mass or a given 3d point. It uses the following input arguments: -inFilename The filename of the input shape to be transformed. -outFilename The filename of the output transformed shape. -MRIinFilename The assoicated image filename to be transformed. -MRIoutFilename The filename of the output transformed image. -useCenterOfMass A flag to transform image using center of mass. -centerX x-coordinate of a user-defined center point. -centerY y-coordinate of a user-defined center point. -centerZ z-coordinate of a user-defined center point. -parameterFilename The filename to store the transform parameters","title":"TranslateShapeToImageOrigin"},{"location":"backlog/ShapeworksCmdTools.html#resize_origin_resampleshape","text":"TODO","title":"Resize_origin_resampleShape"},{"location":"backlog/ShapeworksCmdTools.html#analysis","text":"Analysis is used to blah blah blah...","title":"Analysis"},{"location":"backlog/ShapeworksCmdTools.html#optimization","text":"Optimization is used to blah blah blah...","title":"Optimization"},{"location":"backlog/ShapeworksCmdTools.html#file-utilities","text":"The File Utilities are used to read and convert data stored in a variety of different formats. [GENERIC MESH FORMAT CONVERSION] [fea2vtk] [stl2ply] [stl2vtk] [vtk2ply] [VTKUnstructuredGridToPolyData] [MeshFromDT]","title":"File Utilities"},{"location":"backlog/ShapeworksCmdTools.html#generic-mesh-format-conversion","text":"Converts different mesh formats (vtk, ply, stl, obj) to other formats It uses a parameter file with the following tags - input_format: integer value specifying the input mesh file format (1) .vtk (2) .ply (3) .stl (4) .obj - output_format: integer value specifying the output mesh file format (1) .vtk (2) .ply (3) .stl - input_mesh: paths of the input meshes - output_mesh: paths of the output meshes Usage: ./GenericMeshConversion paramfile","title":"GENERIC MESH FORMAT CONVERSION"},{"location":"backlog/ShapeworksCmdTools.html#fea2vtk","text":"Read in feature files (.fea) and their corresponding triangular mesh (ply) and output a vtk file containing the feature values as scalar onto the mesh (colormap) It uses a parameter file with the following tags\" - mesh: a list of ply file names for meshes to be processed - fea_per_mesh: number of fea files to be processed per mesh - fea: a list of fea files to be processed such that the first fea_per_mesh fea files correspond to the first mesh and so on.","title":"fea2vtk"},{"location":"backlog/ShapeworksCmdTools.html#stl2ply","text":"Commandline tool to convert stl file format to ply file format ... \"Usage: stl2vtk inFilename(.stl) outFilename(.ply)","title":"stl2ply"},{"location":"backlog/ShapeworksCmdTools.html#stl2vtk","text":"Commandline tool to convert stl file format to vtk file format ... \"Usage: stl2vtk inFilename(.stl) outFilename(.vtk)","title":"stl2vtk"},{"location":"backlog/ShapeworksCmdTools.html#vtk2ply","text":"Commandline tool to convert vtk file format to ply file format ... \"Usage: vtk2ply inFilename(.vtk) outFilename(.ply)","title":"vtk2ply"},{"location":"backlog/ShapeworksCmdTools.html#vtkunstructuredgridtopolydata","text":"Convert vtk unstructered grid data to vtk polydata. It uses a paramfile of the following tags: - inputs: list of input vtk files containing unstructured grid data - outputs: list of output vtk files which will be saved containing the vtk polydata version of the corresponding input vtkfiles","title":"VTKUnstructuredGridToPolyData"},{"location":"backlog/ShapeworksCmdTools.html#meshfromdt","text":"TODO","title":"MeshFromDT"},{"location":"backlog/ShapeworksCmdTools.html#feafromvtk","text":"TODO","title":"FeaFromVTK"},{"location":"backlog/fim_v4.html","text":"FIM_V4 Library Overview TODO THIS WORK IS BASED ON FIM LIBRARY FROM FIDS AND GEODESICS REPOS JANUARY 2016 (fim_v4_10_17_2014) -- License fim: Fast Eikonal Solver for Triangular Meshes Zhisong Fu zhisong@cs.utah.edu The MIT License Copyright (c) 2009 Scientific Computing and Imaging Institute, University of Utah. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Building To build and link with ShapeWorks: Configure with CMake Make sure to specify the CMAKE_INSTALL_PREFIX variable if you don't want it to install in a standard system location. Run \"make install\" Select \"BUILD_Mesh_Support\" when configuring ShapeWorks in CMake If you have installed fim in a reasonable location, then the ShapeWorks config will automatically find it. Otherwise, you'll need to specify the FIM_LIB and FIM_INCLUDE_PATH variables. Library Export This library is deployed as part of ShapeWorks, but at some point in the future it may be desirable to make it completely independent by moving it to a separate repository. This can be achieved, maintaining the git commit history, by using the following steps from a clone of the ShapeWorks code. git filter-branch --tag-name-filter <tag> --index-filter 'git rm -r --cached --ignore-unmatch <unwanted> <dirs> <files>' --prune-empty -f -- --all git gc --aggressive Some resources if you want to better understand the above commands include: add folder from one repo to another detach part of a git repository follow renames when splitting git subtree","title":"FIM_V4 Library"},{"location":"backlog/fim_v4.html#fim_v4-library","text":"","title":"FIM_V4 Library"},{"location":"backlog/fim_v4.html#overview","text":"TODO THIS WORK IS BASED ON FIM LIBRARY FROM FIDS AND GEODESICS REPOS JANUARY 2016 (fim_v4_10_17_2014) --","title":"Overview"},{"location":"backlog/fim_v4.html#license","text":"fim: Fast Eikonal Solver for Triangular Meshes Zhisong Fu zhisong@cs.utah.edu The MIT License Copyright (c) 2009 Scientific Computing and Imaging Institute, University of Utah. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"backlog/fim_v4.html#building","text":"To build and link with ShapeWorks: Configure with CMake Make sure to specify the CMAKE_INSTALL_PREFIX variable if you don't want it to install in a standard system location. Run \"make install\" Select \"BUILD_Mesh_Support\" when configuring ShapeWorks in CMake If you have installed fim in a reasonable location, then the ShapeWorks config will automatically find it. Otherwise, you'll need to specify the FIM_LIB and FIM_INCLUDE_PATH variables.","title":"Building"},{"location":"backlog/fim_v4.html#library-export","text":"This library is deployed as part of ShapeWorks, but at some point in the future it may be desirable to make it completely independent by moving it to a separate repository. This can be achieved, maintaining the git commit history, by using the following steps from a clone of the ShapeWorks code. git filter-branch --tag-name-filter <tag> --index-filter 'git rm -r --cached --ignore-unmatch <unwanted> <dirs> <files>' --prune-empty -f -- --all git gc --aggressive Some resources if you want to better understand the above commands include: add folder from one repo to another detach part of a git repository follow renames when splitting git subtree","title":"Library Export"},{"location":"deep-learning/data-augmentation.html","text":"Data Augmentation for Deep Learning ShapeWorks includes a Python package, DataAugmentationUtils , that supports model-based data augmentation. This package is useful to increase the training sample size to train deep networks such as DeepSSM (see SSMs Directly from Images ). The DataAugmentationUtils particularly has tools to generate thousands of image-shape pairs based on the available real data. Fabricated (i.e., augmented) examples are generated to preserve the population shape statistics and exhibit realistic intensity variations. Using the Data Augmentation Package The ShapeWorks data augmentation package, DataAugmentationUtils , is installed to the ShapeWorks anaconda environment when conda_installs.sh is run. Activate shapeworks environment Each time you use ShapeWorks and/or its Python packages, you must first activate its environment using the conda activate shapeworks command on the terminal. To use the DataAugmentationUtils package, make sure you have the shapeworks conda environment is activated and add the following import to your Python code. import DataAugmentationUtils Running Data Augmentation To run the complete data augmentation process as detailed in Data Augmentation Steps , add the following to your Python code. DataAugmentationUtils . runDataAugmentation ( out_dir , img_list , point_list , num_samples , num_dim , sampler_type , mixture_num ) Input arguments: out_dir : Path to the directory where augmented data will be stored img_list : List of paths to images of the original dataset. point_list : List of paths to .particles files of the original dataset. Note, this list should be ordered in correspondence with the img_list . num_dim : The number of dimensions to reduce to in PCA embedding. If zero or not specified, num_dim will be automatically selected to preserve 95% of the population variation. sampler_type : The type of parametric distribution to fit and sample from. Options: Gaussian , mixture , or KDE . Default: KDE . mixture_num : Only necessary if sampler_type is mixture . The number of clusters (i.e., mixture components) to be used in fitting a mixture model. If zero or not specified, the optimal number of clusters will be automatically determined using the elbow method . Visualizing Data Augmentation This function creates a visualization for augmented data. It creates a matrix of scatterplots that opens automatically in the default web browser. The scatterplots show the PCA values of the real and augmented data so that they can be compared pairwise across the PCA dimensions. DataAugmentationUtils . visualizeAugmentation ( data_csv ) Input arguments: data_csv : The path to the CSV file created by running the data augmentation process. Data Augmentation Steps 0. Collect Real Data As a preliminary step, the data on which augmentation will be based on is needed. This includes the shape model .particles files and their corresponding images. This can be acquired using ShapeWorks or any other method of generating point distribution models (PDM). 1. Embed Real Data First, the real shape data (i.e., PDM) is embedded into a lower-dimensional subspace. This is done by creating a matrix that contains all of the real examples, then reducing the dimension via Principal Component Analysis (PCA). 2. Fit a Parametric Distribution Next, a parametric distribution that can be sampled from is fit to the shape examples embedded in the low-dimension subspace. 3. Sample from the Distribution Shape samples are randomly drawn from this distribution, and the closest real shape example to each sample is saved. 4. Project Samples The embedded shape samples are then projected back to shape space, providing new generated shape/correspondence examples. 5. Complete Sample Generation The closest real shape example to each generated shape sample is then used to generate the image corresponding to the fabricated shape sample, giving the full pair of generated samples. This is done by finding the warp between the closest real correspondence points and the generated correspondence points, then applying that warp to the corresponding image of the closest real example. This provides us with an image with the intensity profile of the real example but the shape of the generated sample. Sampling Distributions The sampler_type parameter determines which type of distribution to fit to the embedded data. The options are Gaussian (a single multivariate Gaussian distribution), mixture (a mixture of Gaussian distributions) or KDE (kernel density estimate). Multivariate Gaussian Distribution To fit a multivariate Gaussian distribution (which can be seen in Step 2 of Data Augmentation Steps ), the probability density function is parameterized by the mean and the covariance matrix of the embedded data matrix. New samples are then randomly sampled for this normal distribution. The closest real example to each sampled point is found by calculating the Mahalanobis distance within the embedded space. Mixture of Multivariate Gaussians Distribution A Gaussian mixture model can provide a more appropriate probability density function when the embedded data distribution is mutli-modal. To fit a mixture model, first we cluster the embedded samples and select the optimal number of clusters by minimizing Akaike information criterion (AIC) and Bayesian information criterion (BIC). This number determines how many Gaussian distributions (or components) should be used. Next, the expectation-maximization (EM) algorithm is used to fit a mixture-of-Gaussian model with this number of components. This distribution can then be randomly sampled from, and the closest real example is chosen using Mahalanobis distance within the embedded space. Kernel Density Estimate Distribution Kernel density estimate (KDE) is a non-parametric way of estimating the probability density function of the embedded data. It is fit by defining a Gaussian ball around each real data point in the embedded space, the combination of which provides the distribution. The kernel bandwidth or variance of the Gaussian balls is computed as the average distance to the nearest neighbor computed using the Mahalanobis distance within the embedded space. To sample from the KDE distribution, a real example is randomly selected, then a point is randomly sampled from its kernel. The selected real example is also returned to use its corresponding image to generate the augmented image-shape pair. Relevant papers Jadie Adams, Riddhish Bhalodia, Shireen Elhabian. Uncertain-DeepSSM: From Images to Probabilistic Shape Models. In MICCAI-ShapeMI, Springer, Cham, 2020. Riddhish Bhalodia, Shireen Elhabian, Ladislav Kavan, and Ross Whitaker. DeepSSM: a deep learning framework for statistical shape modeling from raw images. In MICCAI-ShapeMI, pp. 244-257. Springer, Cham, 2018. Riddhish Bhalodia, Anupama Goparaju, Tim Sodergren, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Joshua Cates, Ross Whitaker, Shireen Elhabian. Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation. Computing in Cardiology (CinC), 2018.","title":"Data Augmentation for Deep Learning"},{"location":"deep-learning/data-augmentation.html#data-augmentation-for-deep-learning","text":"ShapeWorks includes a Python package, DataAugmentationUtils , that supports model-based data augmentation. This package is useful to increase the training sample size to train deep networks such as DeepSSM (see SSMs Directly from Images ). The DataAugmentationUtils particularly has tools to generate thousands of image-shape pairs based on the available real data. Fabricated (i.e., augmented) examples are generated to preserve the population shape statistics and exhibit realistic intensity variations.","title":"Data Augmentation for Deep Learning"},{"location":"deep-learning/data-augmentation.html#using-the-data-augmentation-package","text":"The ShapeWorks data augmentation package, DataAugmentationUtils , is installed to the ShapeWorks anaconda environment when conda_installs.sh is run. Activate shapeworks environment Each time you use ShapeWorks and/or its Python packages, you must first activate its environment using the conda activate shapeworks command on the terminal. To use the DataAugmentationUtils package, make sure you have the shapeworks conda environment is activated and add the following import to your Python code. import DataAugmentationUtils","title":"Using the Data Augmentation Package"},{"location":"deep-learning/data-augmentation.html#running-data-augmentation","text":"To run the complete data augmentation process as detailed in Data Augmentation Steps , add the following to your Python code. DataAugmentationUtils . runDataAugmentation ( out_dir , img_list , point_list , num_samples , num_dim , sampler_type , mixture_num ) Input arguments: out_dir : Path to the directory where augmented data will be stored img_list : List of paths to images of the original dataset. point_list : List of paths to .particles files of the original dataset. Note, this list should be ordered in correspondence with the img_list . num_dim : The number of dimensions to reduce to in PCA embedding. If zero or not specified, num_dim will be automatically selected to preserve 95% of the population variation. sampler_type : The type of parametric distribution to fit and sample from. Options: Gaussian , mixture , or KDE . Default: KDE . mixture_num : Only necessary if sampler_type is mixture . The number of clusters (i.e., mixture components) to be used in fitting a mixture model. If zero or not specified, the optimal number of clusters will be automatically determined using the elbow method .","title":"Running Data Augmentation"},{"location":"deep-learning/data-augmentation.html#visualizing-data-augmentation","text":"This function creates a visualization for augmented data. It creates a matrix of scatterplots that opens automatically in the default web browser. The scatterplots show the PCA values of the real and augmented data so that they can be compared pairwise across the PCA dimensions. DataAugmentationUtils . visualizeAugmentation ( data_csv ) Input arguments: data_csv : The path to the CSV file created by running the data augmentation process.","title":"Visualizing Data Augmentation"},{"location":"deep-learning/data-augmentation.html#data-augmentation-steps","text":"","title":"Data Augmentation Steps"},{"location":"deep-learning/data-augmentation.html#0-collect-real-data","text":"As a preliminary step, the data on which augmentation will be based on is needed. This includes the shape model .particles files and their corresponding images. This can be acquired using ShapeWorks or any other method of generating point distribution models (PDM).","title":"0. Collect Real Data"},{"location":"deep-learning/data-augmentation.html#1-embed-real-data","text":"First, the real shape data (i.e., PDM) is embedded into a lower-dimensional subspace. This is done by creating a matrix that contains all of the real examples, then reducing the dimension via Principal Component Analysis (PCA).","title":"1. Embed Real Data"},{"location":"deep-learning/data-augmentation.html#2-fit-a-parametric-distribution","text":"Next, a parametric distribution that can be sampled from is fit to the shape examples embedded in the low-dimension subspace.","title":"2. Fit a Parametric Distribution"},{"location":"deep-learning/data-augmentation.html#3-sample-from-the-distribution","text":"Shape samples are randomly drawn from this distribution, and the closest real shape example to each sample is saved.","title":"3. Sample from the Distribution"},{"location":"deep-learning/data-augmentation.html#4-project-samples","text":"The embedded shape samples are then projected back to shape space, providing new generated shape/correspondence examples.","title":"4. Project Samples"},{"location":"deep-learning/data-augmentation.html#5-complete-sample-generation","text":"The closest real shape example to each generated shape sample is then used to generate the image corresponding to the fabricated shape sample, giving the full pair of generated samples. This is done by finding the warp between the closest real correspondence points and the generated correspondence points, then applying that warp to the corresponding image of the closest real example. This provides us with an image with the intensity profile of the real example but the shape of the generated sample.","title":"5. Complete Sample Generation"},{"location":"deep-learning/data-augmentation.html#sampling-distributions","text":"The sampler_type parameter determines which type of distribution to fit to the embedded data. The options are Gaussian (a single multivariate Gaussian distribution), mixture (a mixture of Gaussian distributions) or KDE (kernel density estimate).","title":"Sampling Distributions"},{"location":"deep-learning/data-augmentation.html#multivariate-gaussian-distribution","text":"To fit a multivariate Gaussian distribution (which can be seen in Step 2 of Data Augmentation Steps ), the probability density function is parameterized by the mean and the covariance matrix of the embedded data matrix. New samples are then randomly sampled for this normal distribution. The closest real example to each sampled point is found by calculating the Mahalanobis distance within the embedded space.","title":"Multivariate Gaussian Distribution"},{"location":"deep-learning/data-augmentation.html#mixture-of-multivariate-gaussians-distribution","text":"A Gaussian mixture model can provide a more appropriate probability density function when the embedded data distribution is mutli-modal. To fit a mixture model, first we cluster the embedded samples and select the optimal number of clusters by minimizing Akaike information criterion (AIC) and Bayesian information criterion (BIC). This number determines how many Gaussian distributions (or components) should be used. Next, the expectation-maximization (EM) algorithm is used to fit a mixture-of-Gaussian model with this number of components. This distribution can then be randomly sampled from, and the closest real example is chosen using Mahalanobis distance within the embedded space.","title":"Mixture of Multivariate Gaussians Distribution"},{"location":"deep-learning/data-augmentation.html#kernel-density-estimate-distribution","text":"Kernel density estimate (KDE) is a non-parametric way of estimating the probability density function of the embedded data. It is fit by defining a Gaussian ball around each real data point in the embedded space, the combination of which provides the distribution. The kernel bandwidth or variance of the Gaussian balls is computed as the average distance to the nearest neighbor computed using the Mahalanobis distance within the embedded space. To sample from the KDE distribution, a real example is randomly selected, then a point is randomly sampled from its kernel. The selected real example is also returned to use its corresponding image to generate the augmented image-shape pair. Relevant papers Jadie Adams, Riddhish Bhalodia, Shireen Elhabian. Uncertain-DeepSSM: From Images to Probabilistic Shape Models. In MICCAI-ShapeMI, Springer, Cham, 2020. Riddhish Bhalodia, Shireen Elhabian, Ladislav Kavan, and Ross Whitaker. DeepSSM: a deep learning framework for statistical shape modeling from raw images. In MICCAI-ShapeMI, pp. 244-257. Springer, Cham, 2018. Riddhish Bhalodia, Anupama Goparaju, Tim Sodergren, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Joshua Cates, Ross Whitaker, Shireen Elhabian. Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation. Computing in Cardiology (CinC), 2018.","title":"Kernel Density Estimate Distribution"},{"location":"deep-learning/deep-ssm.html","text":"SSMs Directly from Images DeepSSM is a deep learning framework that estimates statistical representations of shape directly from unsegmented images once trained. DeepSSM includes a data augmentation process and a convolutional neural network (CNN) model. This documentation provides an overview of the DeepSSM process; see relevant papers for a full explanation. Relevant papers Jadie Adams, Riddhish Bhalodia, Shireen Elhabian. Uncertain-DeepSSM: From Images to Probabilistic Shape Models. In MICCAI-ShapeMI, Springer, Cham, 2020. Riddhish Bhalodia, Shireen Elhabian, Ladislav Kavan, and Ross Whitaker. DeepSSM: a deep learning framework for statistical shape modeling from raw images. In MICCAI-ShapeMI, pp. 244-257. Springer, Cham, 2018. Riddhish Bhalodia, Anupama Goparaju, Tim Sodergren, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Joshua Cates, Ross Whitaker, Shireen Elhabian. Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation. Computing in Cardiology (CinC), 2018. What is DeepSSM? The input to the DeepSSM network is unsegmented 3D images of the anatomy of interest, and the output is the point distribution model (PDM). DeepSSM requires training examples of image/PDM pairs that are generated via the traditional Shapeworks grooming and optimization pipeline or other particle distribution models. Once the network has been trained on these examples, it can predict the PDM of unseen examples given only images of the same anatomy/object class, bypassing the need for labor-intensive segmentation, grooming, and optimization parameter tuning. Why DeepSSM? The benefits of the DeepSSM pipeline include: Less Labor : DeepSSM does not require segmentation, only a bounding box about where the anatomy of interest lies in the image. End-to-end : Does not require separate grooming and optimization steps; it is an end-to-end process. This also reduces memory requirement as images do not need to be saved after intermediate grooming steps. Faster Results : Once a DeepSSM network has been trained, it can be used to predict the shape model on a new image in seconds on a GPU. The DeepSSM network is implemented in PyTorch and requires a GPU to run efficiently. DeepSSM Steps 1. Data Augmentation The first step to creating a DeepSSM model is generating training data. Deep networks require thousands of training instances and since medical imaging data is typically limited, data augmentation is necessary. The data augmentation process is described here: Data Augmentation for Deep Learning . The data augmentation process involves reducing the PDM's to a low-dimensional space via Principal Component Analysis (PCA), preserving a chosen percentage of the variation. The PCA scores are saved and used as the target output for DeepSSM prediction. The PCA scores are deterministically mapped back to the PDM (i.e., shape space) using the eigenvalues and vectors once the DeepSSM model makes a prediction. 2. Creation of Data Loaders The next step is to reformat the data (original and augmented) into PyTorch tensors. 80% of the data is randomly selected to be training data, and the remaining 20% of the data is used as a validation set. The input images are whitened and turned into tensors. They can also be optionally downsampled to a smaller size to allow for faster training. The corresponding PCA scores are also normalized or whitened to avoid DeepSSM learning to favor the primary modes of variation and are then turned to tensors. PyTorch data loaders are then created with a batch size specified by the user. 3. Training PyTorch is used in constructing and training DeepSSM. The network architecture is defined to have five convolution layers followed by two fully connected layers, as illustrated in the figure below. Parametric ReLU activation is used, and the weights are initialized using Xavier initialization. The network is trained for the specified number of epochs using Adam optimization to minimize the L2 loss function with a learning rate of 0.0001. The average training and validation error are printed and logged each epoch to determine convergence. 4. Testing The trained model is then used to predict the PCA score from the images in the test set. These PCA scores are then un-whitened and mapped back to the particle coordinates using the eigenvalues and eigenvectors from PCA. Thus a PDM is acquired for each test image. 5. Evaluation To evaluate the accuracy of DeepSSM output, we compare a mesh created from the ground truth segmentation to a mesh created from the predicted PDM. To obtain the original mesh, we use the ShapeWorks MeshFromDistanceTransforms command to the isosurface mesh from the distance transform created from the true segmentation. To obtain the predicted mesh, we use the ShapeWorks ReconstructSurface command with the mean and predicted particles to reconstruct a surface. We then compare the original mesh to the predicted mesh via surface-to-surface distance. To find the distance from the original to the predicted, we consider each vertex in the original and find the shortest distance to the predicted mesh's surface. This process is not symmetric as it depends on the vertices of one mesh, so the distance from the predicted to the original will be slightly different. We compute the Hausdorff distance that takes the max of these vertex-wise distances to return a single value as a measure of accuracy. We also consider the vertex-wise distances as a scalar field on the mesh vertices and visualize them as a heat map on the surface. This provides us with a way of seeing where the predicted PDM was more or less accurate. Using the DeepSSM Python Package The ShapeWorks DeepSSM package, DeepSSMUtils , is installed to the ShapeWorks anaconda environment when conda_installs.sh is run. Activate shapeworks environment Each time you use ShapeWorks and/or its Python packages, you must first activate its environment using the conda activate shapeworks command on the terminal. To use the DeepSSMUtils package, make sure you have the shapeworks conda environment is activated and add the following import to your Python code: import DeepSSMUtils Get train and validation torch loaders This function turns the original and augmented data into training and validation torch loaders. The data provided is randomly split so that 80% is used in the training set and 20% is used in the validation set. DeepSSMUtils . getTrainValLoaders ( out_dir , data_aug_csv , batch_size = 1 , down_sample = False ) Input arguments: out_dir : Path to the directory to store the torch loaders. data_aug_csv : The path to the csv containing original and augmented data, which is the output when running data augmentation as detailed in Data Augmentation for Deep Learning . batch_size : The batch size for training data. The default value is 1. down_sample : If true, the images will be downsampled to a smaller size to decrease the time needed to train the network. If false, the full image will be used. The default is false. Get test torch loader This function turns the provided data into a test torch loader. DeepSSMUtils . getTestLoader ( out_dir , test_img_list , down_sample ) Input arguments: out_dir : Path to the directory to store the torch loader. test_img_list : A list of paths to the images that are in the test set. down_sample : If true, the images will be downsampled. If false, the full image will be used. This should match what is done for the training and validation loaders. The default is false. Train DeepSSM This function defines a DeepSSM model and trains it on the data provided. DeepSSMUtils . trainDeepSSM ( loader_dir , parameters , out_dir ) Input arguments: loader_dir : Path to directory where train and validation torch loaders are. parameters : A dictionary of network parameters with the following keys. epochs : The number of epochs to train for. learning_rate : The value of the learning rate. val_freq : How often to evaluate on the validation set. 1 means test on the validation set every epoch, 2 means every other epoch, and so on. out_dir : Directory to save the model and training/validation logs. Test DeepSSM This function gets predicted shape models based on the images provided using a trained DeepSSM model. DeepSSMUtils . testDeepSSM ( out_dir , model_path , loader_dir , PCA_scores_path , num_PCA ) Input arguments: out_dir : Path to directory where predictions are saved. model_path : Path to train DeepSSM model. loader_dir : Path to the directory containing test torch loader. PCA_scores_path : Path to eigenvalues and eigenvectors from data augmentation that are used to map predicted PCA scores to particles. num_PCA : The number of PCA scores the DeepSSM model is trained to predict. Analyze Results This function analyzes the shape models predicted by DeepSSM by comparing them to the true segmentation. DeepSSMUtils . analyzeResults ( out_dir , DT_dir , prediction_dir , mean_prefix ) Input arguments: out_dir : Path to the directory where meshes and analysis should be saved. DT_dir : Path to the directory containing distance transforms based on the true segmentations of the test images. prediction_dir : Path to the directory containing predicted particle files from testing DeepSSM. mean_prefix : Path to the mean particle and mesh files for the dataset.","title":"SSMs Directly from Images"},{"location":"deep-learning/deep-ssm.html#ssms-directly-from-images","text":"DeepSSM is a deep learning framework that estimates statistical representations of shape directly from unsegmented images once trained. DeepSSM includes a data augmentation process and a convolutional neural network (CNN) model. This documentation provides an overview of the DeepSSM process; see relevant papers for a full explanation. Relevant papers Jadie Adams, Riddhish Bhalodia, Shireen Elhabian. Uncertain-DeepSSM: From Images to Probabilistic Shape Models. In MICCAI-ShapeMI, Springer, Cham, 2020. Riddhish Bhalodia, Shireen Elhabian, Ladislav Kavan, and Ross Whitaker. DeepSSM: a deep learning framework for statistical shape modeling from raw images. In MICCAI-ShapeMI, pp. 244-257. Springer, Cham, 2018. Riddhish Bhalodia, Anupama Goparaju, Tim Sodergren, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Joshua Cates, Ross Whitaker, Shireen Elhabian. Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation. Computing in Cardiology (CinC), 2018.","title":"SSMs Directly from Images"},{"location":"deep-learning/deep-ssm.html#what-is-deepssm","text":"The input to the DeepSSM network is unsegmented 3D images of the anatomy of interest, and the output is the point distribution model (PDM). DeepSSM requires training examples of image/PDM pairs that are generated via the traditional Shapeworks grooming and optimization pipeline or other particle distribution models. Once the network has been trained on these examples, it can predict the PDM of unseen examples given only images of the same anatomy/object class, bypassing the need for labor-intensive segmentation, grooming, and optimization parameter tuning.","title":"What is DeepSSM?"},{"location":"deep-learning/deep-ssm.html#why-deepssm","text":"The benefits of the DeepSSM pipeline include: Less Labor : DeepSSM does not require segmentation, only a bounding box about where the anatomy of interest lies in the image. End-to-end : Does not require separate grooming and optimization steps; it is an end-to-end process. This also reduces memory requirement as images do not need to be saved after intermediate grooming steps. Faster Results : Once a DeepSSM network has been trained, it can be used to predict the shape model on a new image in seconds on a GPU. The DeepSSM network is implemented in PyTorch and requires a GPU to run efficiently.","title":"Why DeepSSM?"},{"location":"deep-learning/deep-ssm.html#deepssm-steps","text":"","title":"DeepSSM Steps"},{"location":"deep-learning/deep-ssm.html#1-data-augmentation","text":"The first step to creating a DeepSSM model is generating training data. Deep networks require thousands of training instances and since medical imaging data is typically limited, data augmentation is necessary. The data augmentation process is described here: Data Augmentation for Deep Learning . The data augmentation process involves reducing the PDM's to a low-dimensional space via Principal Component Analysis (PCA), preserving a chosen percentage of the variation. The PCA scores are saved and used as the target output for DeepSSM prediction. The PCA scores are deterministically mapped back to the PDM (i.e., shape space) using the eigenvalues and vectors once the DeepSSM model makes a prediction.","title":"1. Data Augmentation"},{"location":"deep-learning/deep-ssm.html#2-creation-of-data-loaders","text":"The next step is to reformat the data (original and augmented) into PyTorch tensors. 80% of the data is randomly selected to be training data, and the remaining 20% of the data is used as a validation set. The input images are whitened and turned into tensors. They can also be optionally downsampled to a smaller size to allow for faster training. The corresponding PCA scores are also normalized or whitened to avoid DeepSSM learning to favor the primary modes of variation and are then turned to tensors. PyTorch data loaders are then created with a batch size specified by the user.","title":"2. Creation of Data Loaders"},{"location":"deep-learning/deep-ssm.html#3-training","text":"PyTorch is used in constructing and training DeepSSM. The network architecture is defined to have five convolution layers followed by two fully connected layers, as illustrated in the figure below. Parametric ReLU activation is used, and the weights are initialized using Xavier initialization. The network is trained for the specified number of epochs using Adam optimization to minimize the L2 loss function with a learning rate of 0.0001. The average training and validation error are printed and logged each epoch to determine convergence.","title":"3. Training"},{"location":"deep-learning/deep-ssm.html#4-testing","text":"The trained model is then used to predict the PCA score from the images in the test set. These PCA scores are then un-whitened and mapped back to the particle coordinates using the eigenvalues and eigenvectors from PCA. Thus a PDM is acquired for each test image.","title":"4. Testing"},{"location":"deep-learning/deep-ssm.html#5-evaluation","text":"To evaluate the accuracy of DeepSSM output, we compare a mesh created from the ground truth segmentation to a mesh created from the predicted PDM. To obtain the original mesh, we use the ShapeWorks MeshFromDistanceTransforms command to the isosurface mesh from the distance transform created from the true segmentation. To obtain the predicted mesh, we use the ShapeWorks ReconstructSurface command with the mean and predicted particles to reconstruct a surface. We then compare the original mesh to the predicted mesh via surface-to-surface distance. To find the distance from the original to the predicted, we consider each vertex in the original and find the shortest distance to the predicted mesh's surface. This process is not symmetric as it depends on the vertices of one mesh, so the distance from the predicted to the original will be slightly different. We compute the Hausdorff distance that takes the max of these vertex-wise distances to return a single value as a measure of accuracy. We also consider the vertex-wise distances as a scalar field on the mesh vertices and visualize them as a heat map on the surface. This provides us with a way of seeing where the predicted PDM was more or less accurate.","title":"5. Evaluation"},{"location":"deep-learning/deep-ssm.html#using-the-deepssm-python-package","text":"The ShapeWorks DeepSSM package, DeepSSMUtils , is installed to the ShapeWorks anaconda environment when conda_installs.sh is run. Activate shapeworks environment Each time you use ShapeWorks and/or its Python packages, you must first activate its environment using the conda activate shapeworks command on the terminal. To use the DeepSSMUtils package, make sure you have the shapeworks conda environment is activated and add the following import to your Python code: import DeepSSMUtils","title":"Using the DeepSSM Python Package"},{"location":"deep-learning/deep-ssm.html#get-train-and-validation-torch-loaders","text":"This function turns the original and augmented data into training and validation torch loaders. The data provided is randomly split so that 80% is used in the training set and 20% is used in the validation set. DeepSSMUtils . getTrainValLoaders ( out_dir , data_aug_csv , batch_size = 1 , down_sample = False ) Input arguments: out_dir : Path to the directory to store the torch loaders. data_aug_csv : The path to the csv containing original and augmented data, which is the output when running data augmentation as detailed in Data Augmentation for Deep Learning . batch_size : The batch size for training data. The default value is 1. down_sample : If true, the images will be downsampled to a smaller size to decrease the time needed to train the network. If false, the full image will be used. The default is false.","title":"Get train and validation torch loaders"},{"location":"deep-learning/deep-ssm.html#get-test-torch-loader","text":"This function turns the provided data into a test torch loader. DeepSSMUtils . getTestLoader ( out_dir , test_img_list , down_sample ) Input arguments: out_dir : Path to the directory to store the torch loader. test_img_list : A list of paths to the images that are in the test set. down_sample : If true, the images will be downsampled. If false, the full image will be used. This should match what is done for the training and validation loaders. The default is false.","title":"Get test torch loader"},{"location":"deep-learning/deep-ssm.html#train-deepssm","text":"This function defines a DeepSSM model and trains it on the data provided. DeepSSMUtils . trainDeepSSM ( loader_dir , parameters , out_dir ) Input arguments: loader_dir : Path to directory where train and validation torch loaders are. parameters : A dictionary of network parameters with the following keys. epochs : The number of epochs to train for. learning_rate : The value of the learning rate. val_freq : How often to evaluate on the validation set. 1 means test on the validation set every epoch, 2 means every other epoch, and so on. out_dir : Directory to save the model and training/validation logs.","title":"Train DeepSSM"},{"location":"deep-learning/deep-ssm.html#test-deepssm","text":"This function gets predicted shape models based on the images provided using a trained DeepSSM model. DeepSSMUtils . testDeepSSM ( out_dir , model_path , loader_dir , PCA_scores_path , num_PCA ) Input arguments: out_dir : Path to directory where predictions are saved. model_path : Path to train DeepSSM model. loader_dir : Path to the directory containing test torch loader. PCA_scores_path : Path to eigenvalues and eigenvectors from data augmentation that are used to map predicted PCA scores to particles. num_PCA : The number of PCA scores the DeepSSM model is trained to predict.","title":"Test DeepSSM"},{"location":"deep-learning/deep-ssm.html#analyze-results","text":"This function analyzes the shape models predicted by DeepSSM by comparing them to the true segmentation. DeepSSMUtils . analyzeResults ( out_dir , DT_dir , prediction_dir , mean_prefix ) Input arguments: out_dir : Path to the directory where meshes and analysis should be saved. DT_dir : Path to the directory containing distance transforms based on the true segmentations of the test images. prediction_dir : Path to the directory containing predicted particle files from testing DeepSSM. mean_prefix : Path to the mean particle and mesh files for the dataset.","title":"Analyze Results"},{"location":"dev/build.html","text":"How to Build ShapeWorks from Source? If you encounter problems, have questions, or need help, please contact <shapeworks-dev-support@sci.utah.edu> . Minimum Requirements Linux CMake 3.11 (provided by conda below if not already installed) GCC 7.5.0 Qt 5.9.8 (optional for GUI components) Mac CMake 3.11 (provided by conda below if not already installed) Clang 10.0.0 Qt 5.9.8 (optional for GUI components) Windows CMake 3.11 (provided by conda below if not already installed) MSVC 2019 Qt 5.9.8 (optional for GUI components) Clone source To clone the ShapeWorks source: $ git clone https://github.com/SCIInstitute/ShapeWorks See How to Contribute? for more details on git commands. Install dependencies Linux and Mac Anaconda We use Anaconda (conda) to install many dependencies required for both building and running ShapeWorks. Conda [sub]environments do not affect a machine's global setup in any way, do not require sudo to install, and are only available at the user level when activated. To install conda and the dependencies it provides (currently requires either bash or zsh shell), run: $ source conda_installs.sh ShapeWorks uses git-lfs to store image data for testing. If git-lfs was not already installed before cloning ShapeWorks, please use the following commands to get this data: $ git lfs fetch $ git lfs install $ git lfs checkout Activate shapeworks environment Each time you build or use ShapeWorks from the command line , you must first activate its environment using the conda activate shapeworks command on the terminal. Qt5 Download and install the latest version of Qt5 , selecting the LGPL (free) license (at least version 5.10 required). After installing Qt5, add the directory containing qmake.exe to your PATH. See Adding to PATH Environment Variable for help with this. Example qmake directory Linux: /opt/Qt5.14.0/5.14.0/gcc_64/bin VXL, VTK, ITK, Eigen and OpenVDB These dependencies can be installed using the build_dependencies.sh script. Use $ ./build_dependencies.sh --help for more details on the available options. Note If you get an error that looks like this: which: no qmake in (...) For GUI applications, please make sure at least version $QT_MIN_VER of Qt5 is installed and that its qmake is in the path. Download Qt5 from: https://download.qt.io/archive/qt/ Make sure you added Qt to your path as explained in the Qt5 installation step. Important If you decide to build ITK yourself and you would like to use the ShapeWorks GUI applications, ITK must be built with VTK . Windows CMake Download and install [CMake] Download and install [Visual Studio 2019] Anaconda Download and install [Anaconda] . Important It is recommended not to add Anaconda to your PATH and not to register Anaconda as your default Python. Using the Anaconda Prompt , run conda_installs.bat Qt5 Download and install the latest version of Qt5 , selecting the LGPL (free) license (at least version 5.10 required). After installing Qt5, add the directory containing qmake.exe to your PATH. See Adding to PATH Environment Variable for help with this. Example qmake directory: D:\\Qt\\5.14.0\\winrt_x64_msvc2017\\bin VXL, VTK, ITK, Eigen and OpenVDB These dependencies can be installed using the build_dependencies.sh script. Use an msys shell (e.g., git bash) to do this on Windows. Use $ ./build_dependencies.sh --help for more details on the available build_dependencies options. Important If you get an error that says: which: no qmake in (...) Make sure you added Qt to your path as explained in the Qt5 installation step. Important If you decide to build ITK yourself and you would like to use the ShapeWorks GUI applications, ITK must be built with VTK . Configure and Build Linux & Mac Make a build directory and use cmake to configure your build: mkdir build cd build cmake <options> .. There is a CMake GUI to see and change any of the options: - On OSX/Linux, you can use the GUI by running ccmake instead of cmake . Options If you used the build_dependencies.sh script above, the prefix for all dependencies is the same. Otherwise, the specific paths to VTK, VXL, ITK, and Eigen3 are all required. Required (if you used build_dependencies.sh): -DCMAKE_PREFIX_PATH=<dependencies install path> Required (otherwise): -DVXL_DIR=<vxl cmake path> (contains VXLConfig.cmake) -DVTK_DIR=<vtk cmake path> (contains VTKConfig.cmake) -DITK_DIR=<itk cmake path> (contains ITKConfig.cmake) -DEigen3_DIR=<eigen3 cmake path> (contains Eigen3Config.cmake) -DOpenVDB_DIR=<openvdb cmake path> (contains FindOpenVDB.cmake) Optional: -G<generator> default: Unix Makefiles (ex: -GXCode or -G\"Visual Studio 16 2019\" -Ax64) -DBuild_Studio=[OFF|ON] default: OFF -DBuild_View2=[OFF|ON] default: OFF -DBuild_Post=[OFF|ON] default: OFF -DCMAKE_INSTALL_PREFIX=<path> default: ./install -DCMAKE_BUILD_TYPE=[Debug|Release] default: Release (only required is default generator is used) See Examples below for common values of the variables. Building Makefiles: make -j<num_procs> where num_procs is the number of parallel processes, say 8. You might need to build using cmake --build . -j 16 to pass parallel flags to dependent projects (e.g., vtk) XCode project: open ShapeWorks.xcodeproj and build from there. Before running Examples/Python scripts Add the ShapeWorks and dependency binaries to the path: $ export PATH=/path/to/shapeworks/build/bin:/path/to/dependencies/bin:$PATH Examples OSX example that builds dependencies separately, then generates an XCode project for ShapeWorks: $ ./build_dependencies.sh mkdir build cd build cmake -DCMAKE_PREFIX_PATH=\"${PWD}/../dependencies/install\" -DBuild_Post:BOOL=ON -DBuild_View2:BOOL=ON -DBuild_Studio:BOOL=ON -DUSE_OPENMP=OFF -Wno-dev -Wno-deprecated -GXcode .. open ShapeWorks.xcodeproj Windows Use the cmake from the Anaconda Prompt with shapeworks env activated to configure and generate project files for your preferred build system (e.g., Visual Studio 16 2019). Options Required: -DCMAKE_PREFIX_PATH=<qt cmake path> (This is different from qmake path in the Install Qt5 step -DVXL_DIR=<vxl cmake path> (contains VXLConfig.cmake) -DVTK_DIR=<vtk cmake path> (contains VTKConfig.cmake) -DITK_DIR=<itk cmake path> (contains ITKConfig.cmake) -DEigen3_DIR=<eigen cmake path> (contains Eigen3Config.cmake) -DOpenVDB_DIR=<openvdb cmake path> (contains FindOpenVDB.cmake) Optional: -D Build_Studio=[OFF|ON] default: OFF -D Build_View2=[OFF|ON] default: OFF -D Build_Post=[OFF|ON] default: ON -D CMAKE_INSTALL_PREFIX=<path> default: ./install -D CMAKE_BUILD_TYPE=[Debug|Release] Examples An example that builds dependencies separately then generates a Visual Studio project for ShapeWorks: > conda activate shapeworks > ./build_dependencies.sh --build-dir=../dependencies --install-dir=../dependencies > mkdir build > cd build > cmake -G\"Visual Studio 16 2019\" -Ax64 -DVXL_DIR=../dependencies/vxl/build -DCMAKE_PREFIX_PATH=../dependencies -DBuild_Post:BOOL=ON -DBuild_View2:BOOL=ON -DBuild_Studio:BOOL=ON ..","title":"How to Build ShapeWorks from Source?"},{"location":"dev/build.html#how-to-build-shapeworks-from-source","text":"If you encounter problems, have questions, or need help, please contact <shapeworks-dev-support@sci.utah.edu> .","title":"How to Build ShapeWorks from Source?"},{"location":"dev/build.html#minimum-requirements","text":"","title":"Minimum Requirements"},{"location":"dev/build.html#linux","text":"CMake 3.11 (provided by conda below if not already installed) GCC 7.5.0 Qt 5.9.8 (optional for GUI components)","title":"Linux"},{"location":"dev/build.html#mac","text":"CMake 3.11 (provided by conda below if not already installed) Clang 10.0.0 Qt 5.9.8 (optional for GUI components)","title":"Mac"},{"location":"dev/build.html#windows","text":"CMake 3.11 (provided by conda below if not already installed) MSVC 2019 Qt 5.9.8 (optional for GUI components)","title":"Windows"},{"location":"dev/build.html#clone-source","text":"To clone the ShapeWorks source: $ git clone https://github.com/SCIInstitute/ShapeWorks See How to Contribute? for more details on git commands.","title":"Clone source"},{"location":"dev/build.html#install-dependencies","text":"","title":"Install dependencies"},{"location":"dev/build.html#linux-and-mac","text":"","title":"Linux and Mac"},{"location":"dev/build.html#anaconda","text":"We use Anaconda (conda) to install many dependencies required for both building and running ShapeWorks. Conda [sub]environments do not affect a machine's global setup in any way, do not require sudo to install, and are only available at the user level when activated. To install conda and the dependencies it provides (currently requires either bash or zsh shell), run: $ source conda_installs.sh ShapeWorks uses git-lfs to store image data for testing. If git-lfs was not already installed before cloning ShapeWorks, please use the following commands to get this data: $ git lfs fetch $ git lfs install $ git lfs checkout Activate shapeworks environment Each time you build or use ShapeWorks from the command line , you must first activate its environment using the conda activate shapeworks command on the terminal.","title":"Anaconda"},{"location":"dev/build.html#qt5","text":"Download and install the latest version of Qt5 , selecting the LGPL (free) license (at least version 5.10 required). After installing Qt5, add the directory containing qmake.exe to your PATH. See Adding to PATH Environment Variable for help with this. Example qmake directory Linux: /opt/Qt5.14.0/5.14.0/gcc_64/bin","title":"Qt5"},{"location":"dev/build.html#vxl-vtk-itk-eigen-and-openvdb","text":"These dependencies can be installed using the build_dependencies.sh script. Use $ ./build_dependencies.sh --help for more details on the available options. Note If you get an error that looks like this: which: no qmake in (...) For GUI applications, please make sure at least version $QT_MIN_VER of Qt5 is installed and that its qmake is in the path. Download Qt5 from: https://download.qt.io/archive/qt/ Make sure you added Qt to your path as explained in the Qt5 installation step. Important If you decide to build ITK yourself and you would like to use the ShapeWorks GUI applications, ITK must be built with VTK .","title":"VXL, VTK, ITK, Eigen and OpenVDB"},{"location":"dev/build.html#windows_1","text":"","title":"Windows"},{"location":"dev/build.html#cmake","text":"Download and install [CMake] Download and install [Visual Studio 2019]","title":"CMake"},{"location":"dev/build.html#anaconda_1","text":"Download and install [Anaconda] . Important It is recommended not to add Anaconda to your PATH and not to register Anaconda as your default Python. Using the Anaconda Prompt , run conda_installs.bat","title":"Anaconda"},{"location":"dev/build.html#qt5_1","text":"Download and install the latest version of Qt5 , selecting the LGPL (free) license (at least version 5.10 required). After installing Qt5, add the directory containing qmake.exe to your PATH. See Adding to PATH Environment Variable for help with this. Example qmake directory: D:\\Qt\\5.14.0\\winrt_x64_msvc2017\\bin","title":"Qt5"},{"location":"dev/build.html#vxl-vtk-itk-eigen-and-openvdb_1","text":"These dependencies can be installed using the build_dependencies.sh script. Use an msys shell (e.g., git bash) to do this on Windows. Use $ ./build_dependencies.sh --help for more details on the available build_dependencies options. Important If you get an error that says: which: no qmake in (...) Make sure you added Qt to your path as explained in the Qt5 installation step. Important If you decide to build ITK yourself and you would like to use the ShapeWorks GUI applications, ITK must be built with VTK .","title":"VXL, VTK, ITK, Eigen and OpenVDB"},{"location":"dev/build.html#configure-and-build","text":"","title":"Configure and Build"},{"location":"dev/build.html#linux-mac","text":"Make a build directory and use cmake to configure your build: mkdir build cd build cmake <options> .. There is a CMake GUI to see and change any of the options: - On OSX/Linux, you can use the GUI by running ccmake instead of cmake .","title":"Linux &amp; Mac"},{"location":"dev/build.html#options","text":"If you used the build_dependencies.sh script above, the prefix for all dependencies is the same. Otherwise, the specific paths to VTK, VXL, ITK, and Eigen3 are all required. Required (if you used build_dependencies.sh): -DCMAKE_PREFIX_PATH=<dependencies install path> Required (otherwise): -DVXL_DIR=<vxl cmake path> (contains VXLConfig.cmake) -DVTK_DIR=<vtk cmake path> (contains VTKConfig.cmake) -DITK_DIR=<itk cmake path> (contains ITKConfig.cmake) -DEigen3_DIR=<eigen3 cmake path> (contains Eigen3Config.cmake) -DOpenVDB_DIR=<openvdb cmake path> (contains FindOpenVDB.cmake) Optional: -G<generator> default: Unix Makefiles (ex: -GXCode or -G\"Visual Studio 16 2019\" -Ax64) -DBuild_Studio=[OFF|ON] default: OFF -DBuild_View2=[OFF|ON] default: OFF -DBuild_Post=[OFF|ON] default: OFF -DCMAKE_INSTALL_PREFIX=<path> default: ./install -DCMAKE_BUILD_TYPE=[Debug|Release] default: Release (only required is default generator is used) See Examples below for common values of the variables.","title":"Options"},{"location":"dev/build.html#building","text":"Makefiles: make -j<num_procs> where num_procs is the number of parallel processes, say 8. You might need to build using cmake --build . -j 16 to pass parallel flags to dependent projects (e.g., vtk) XCode project: open ShapeWorks.xcodeproj and build from there.","title":"Building"},{"location":"dev/build.html#before-running-examplespython-scripts","text":"Add the ShapeWorks and dependency binaries to the path: $ export PATH=/path/to/shapeworks/build/bin:/path/to/dependencies/bin:$PATH","title":"Before running Examples/Python scripts"},{"location":"dev/build.html#examples","text":"OSX example that builds dependencies separately, then generates an XCode project for ShapeWorks: $ ./build_dependencies.sh mkdir build cd build cmake -DCMAKE_PREFIX_PATH=\"${PWD}/../dependencies/install\" -DBuild_Post:BOOL=ON -DBuild_View2:BOOL=ON -DBuild_Studio:BOOL=ON -DUSE_OPENMP=OFF -Wno-dev -Wno-deprecated -GXcode .. open ShapeWorks.xcodeproj","title":"Examples"},{"location":"dev/build.html#windows_2","text":"Use the cmake from the Anaconda Prompt with shapeworks env activated to configure and generate project files for your preferred build system (e.g., Visual Studio 16 2019).","title":"Windows"},{"location":"dev/build.html#options_1","text":"Required: -DCMAKE_PREFIX_PATH=<qt cmake path> (This is different from qmake path in the Install Qt5 step -DVXL_DIR=<vxl cmake path> (contains VXLConfig.cmake) -DVTK_DIR=<vtk cmake path> (contains VTKConfig.cmake) -DITK_DIR=<itk cmake path> (contains ITKConfig.cmake) -DEigen3_DIR=<eigen cmake path> (contains Eigen3Config.cmake) -DOpenVDB_DIR=<openvdb cmake path> (contains FindOpenVDB.cmake) Optional: -D Build_Studio=[OFF|ON] default: OFF -D Build_View2=[OFF|ON] default: OFF -D Build_Post=[OFF|ON] default: ON -D CMAKE_INSTALL_PREFIX=<path> default: ./install -D CMAKE_BUILD_TYPE=[Debug|Release]","title":"Options"},{"location":"dev/build.html#examples_1","text":"An example that builds dependencies separately then generates a Visual Studio project for ShapeWorks: > conda activate shapeworks > ./build_dependencies.sh --build-dir=../dependencies --install-dir=../dependencies > mkdir build > cd build > cmake -G\"Visual Studio 16 2019\" -Ax64 -DVXL_DIR=../dependencies/vxl/build -DCMAKE_PREFIX_PATH=../dependencies -DBuild_Post:BOOL=ON -DBuild_View2:BOOL=ON -DBuild_Studio:BOOL=ON ..","title":"Examples"},{"location":"dev/commands.html","text":"How to Add ShapeWorks Commands?","title":"How to Add ShapeWorks Commands?"},{"location":"dev/commands.html#how-to-add-shapeworks-commands","text":"","title":"How to Add ShapeWorks Commands?"},{"location":"dev/contribute.html","text":"How to Contribute to ShapeWorks? ShapeWorks uses git for managing source code so changes can be integrated from multiple people. Here are some of the basics to check out the repository, make and share modifications, and keep track of all your changes along the way. The ShapeWorks repositories are stored on GitHub . Imporant If you want to contribute, first make sure you have a developer account . Source and Branches To clone the ShapeWorks source, use one of the following commands: $ git clone https://github.com/SCIInstitute/ShapeWorks To avoid having to type your password every time you refresh your source code, you can copy your public ssh key (found in ~/.ssh/id_rsa.pub) and add it to your GitHub account . Now you can use this version to clone the code: $ git clone git@github.com:sciinstitute/ShapeWorks If you've already cloned your code, simply change the [remote \"origin\"] in .git/config to url = git@github.com:SCIInstitute/ShapeWorks . All git commands remain the same. Now that you've cloned the source you can build ShapeWorks from source if you want. View current state (branch and modifications, both staged and unstaged): $ git status View all branches: $ git branch -a Checkout a branch (or a tag or a specific commit): $ git checkout <branchname> Fetch changes for a specific branch from GitHub: $ git fetch origin <branchname> Fetch changes for all branches from GitHub: $ git fetch --all Merge the fetched changes to your current branch: $ git merge origin/<branchname> For example, use this command to merge the latest master to your current branch. After that, you can test as necessary, then push your branch and make a pull request when it's ready: git merge origin/master Shortcut to both fetch and merge upstream changes to the current branch: $ git pull Push a branch to GitHub: $ git push origin <branchname> Delete a branch (e.g., after a pull request has been completed and merged ): $ git branch -d <branchname> Force delete a branch whether or not it's been merged: $ git branch -D <branchname> Prune old branches that have been removed from GitHub (--dry-run will show you what will be pruned): $ git remote prune origin [--dry-run] Commits Check the current status of your changes scheduled to be committed using: $ git status To commit your current changes to current local branch (first, use git diff to ensure they are correct): $ git diff $ git commit -m \"description of changes\" This commits only the files specifically scheduled to be committed. If you want all changes to be committed, use the -a switch: $ git commit -am \"description of changes\" If you only want to commit scheduled files and a file has been changed after being scheduled, add it again to be updated: $ git add <filename> To push your local changes for a given branch to GitHub, first use git diff to ensure they are correct, then push to origin: $ git diff origin/<branchname> <branchname> $ git push origin <branchname> Never push directly to master After pushing, submit a pull request (click the \"New pull request\" button on the main GitHub page) for your changes to be checked and merged to the master branch. Logs and diffs View the diff of the most recent commit: $ git diff HEAD~1 View the log of a particular file (including moves/renames): $ git log --follow -- <filename> View the commit history for a file showing the detailed diffs for each commit: $ git log -p <filename> View the commit history of a file/dir when its name is the same as a branch: $ git log -- <filename> View the diff between one commit and its predecessor: $ git diff <commit_SHA>~1..<commit_SHA> View only the files that changes for a given range of commits: $ git diff --name-only <SHA1> <SHA2> Tags Specific versions and releases can be identified using tags. List current tags: $ git tag --list Show details for a specific tag ( --quiet hides the diffs): $ git show <tagname> --quiet By default, checking out a tag puts your repo into a 'detached head' state. Use the following command the first time you check out a tag or a specific commit to create a local branch with that name (this happens automatically when checking out branches): $ git checkout origin/<tag> -b <tagname> Create a new tag using the following: $ git tag <tagname> Tags must be explicitly pushed to GitHub. Push your new tag using: $ git push origin <tagname> Delete a tag using: $ git tag delete <tagname> $ git push origin --delete <tagname> The second command is only necessary if the tag has been pushed to Github. Advanced There are many other things that can be done, such as mashing together commits, temporarily \"stashing\" and retrieving stashed changes, etc. Please add anything here that seems useful. Stash your current changes (temporarily set them aside without committing): $ git stash -m \"what is being stashed\" Retrieve the stashed changes: $ git stash pop Modify the description of the most recent commit: Imporant Only use this if the commit has not yet been pushed to GitHub. $ git commit --amend","title":"How to Contribute to ShapeWorks?"},{"location":"dev/contribute.html#how-to-contribute-to-shapeworks","text":"ShapeWorks uses git for managing source code so changes can be integrated from multiple people. Here are some of the basics to check out the repository, make and share modifications, and keep track of all your changes along the way. The ShapeWorks repositories are stored on GitHub . Imporant If you want to contribute, first make sure you have a developer account .","title":"How to Contribute to ShapeWorks?"},{"location":"dev/contribute.html#source-and-branches","text":"To clone the ShapeWorks source, use one of the following commands: $ git clone https://github.com/SCIInstitute/ShapeWorks To avoid having to type your password every time you refresh your source code, you can copy your public ssh key (found in ~/.ssh/id_rsa.pub) and add it to your GitHub account . Now you can use this version to clone the code: $ git clone git@github.com:sciinstitute/ShapeWorks If you've already cloned your code, simply change the [remote \"origin\"] in .git/config to url = git@github.com:SCIInstitute/ShapeWorks . All git commands remain the same. Now that you've cloned the source you can build ShapeWorks from source if you want. View current state (branch and modifications, both staged and unstaged): $ git status View all branches: $ git branch -a Checkout a branch (or a tag or a specific commit): $ git checkout <branchname> Fetch changes for a specific branch from GitHub: $ git fetch origin <branchname> Fetch changes for all branches from GitHub: $ git fetch --all Merge the fetched changes to your current branch: $ git merge origin/<branchname> For example, use this command to merge the latest master to your current branch. After that, you can test as necessary, then push your branch and make a pull request when it's ready: git merge origin/master Shortcut to both fetch and merge upstream changes to the current branch: $ git pull Push a branch to GitHub: $ git push origin <branchname> Delete a branch (e.g., after a pull request has been completed and merged ): $ git branch -d <branchname> Force delete a branch whether or not it's been merged: $ git branch -D <branchname> Prune old branches that have been removed from GitHub (--dry-run will show you what will be pruned): $ git remote prune origin [--dry-run]","title":"Source and Branches"},{"location":"dev/contribute.html#commits","text":"Check the current status of your changes scheduled to be committed using: $ git status To commit your current changes to current local branch (first, use git diff to ensure they are correct): $ git diff $ git commit -m \"description of changes\" This commits only the files specifically scheduled to be committed. If you want all changes to be committed, use the -a switch: $ git commit -am \"description of changes\" If you only want to commit scheduled files and a file has been changed after being scheduled, add it again to be updated: $ git add <filename> To push your local changes for a given branch to GitHub, first use git diff to ensure they are correct, then push to origin: $ git diff origin/<branchname> <branchname> $ git push origin <branchname> Never push directly to master After pushing, submit a pull request (click the \"New pull request\" button on the main GitHub page) for your changes to be checked and merged to the master branch.","title":"Commits"},{"location":"dev/contribute.html#logs-and-diffs","text":"View the diff of the most recent commit: $ git diff HEAD~1 View the log of a particular file (including moves/renames): $ git log --follow -- <filename> View the commit history for a file showing the detailed diffs for each commit: $ git log -p <filename> View the commit history of a file/dir when its name is the same as a branch: $ git log -- <filename> View the diff between one commit and its predecessor: $ git diff <commit_SHA>~1..<commit_SHA> View only the files that changes for a given range of commits: $ git diff --name-only <SHA1> <SHA2>","title":"Logs and diffs"},{"location":"dev/contribute.html#tags","text":"Specific versions and releases can be identified using tags. List current tags: $ git tag --list Show details for a specific tag ( --quiet hides the diffs): $ git show <tagname> --quiet By default, checking out a tag puts your repo into a 'detached head' state. Use the following command the first time you check out a tag or a specific commit to create a local branch with that name (this happens automatically when checking out branches): $ git checkout origin/<tag> -b <tagname> Create a new tag using the following: $ git tag <tagname> Tags must be explicitly pushed to GitHub. Push your new tag using: $ git push origin <tagname> Delete a tag using: $ git tag delete <tagname> $ git push origin --delete <tagname> The second command is only necessary if the tag has been pushed to Github.","title":"Tags"},{"location":"dev/contribute.html#advanced","text":"There are many other things that can be done, such as mashing together commits, temporarily \"stashing\" and retrieving stashed changes, etc. Please add anything here that seems useful. Stash your current changes (temporarily set them aside without committing): $ git stash -m \"what is being stashed\" Retrieve the stashed changes: $ git stash pop Modify the description of the most recent commit: Imporant Only use this if the commit has not yet been pushed to GitHub. $ git commit --amend","title":"Advanced"},{"location":"dev/datasets.html","text":"How to Add New Datasets? ShapeWorks Data Portal Dataset Requirements Each dataset must have a License.txt , which describes the terms of use and citation requirements for the data. You can see an example in the Left Atrium dataset License . Data should be organized into appropriately named directories: images , segmentations , meshes , landmarks , distance_transforms , shape_models . Note that each dataset will likely only have a subset of these directories. For example, the ellipsoid dataset only has a segmentations directory. images/ contents: 3D images of the data file types: itk supported image formats segmentations/ contents: 3D segmentations of the data file types: itk supported image formats meshes/ contents: meshes file types: ply, vtk, stl landmarks/ contents: sample specific files containing anatomical landmarks noted by the user file types: csv with 4 columns: x , y , z , label distance_transforms/ contents: 3D distance transform volumes file types: itk supported image formats shape_models/ contents: example shape models and parameters used to generate them file types: xml One subfolder per model stores all of the particles files For example: - shape_models/ - example_64.xml - example_64/ - *.particles Uploading a Dataset Important Uploaded datasets are only restricted to ShapeWorks developers. Create an account on the ShapeWorks Data Portal Ask Oleks to add you to the list of developers. (oleks@sci.utah.edu) Prepare dataset directory with License.txt Use DatasetUtils.uploadNewDataset(datasetName, datasetPath) to upload. Name the dataset all lowercase with underscores separating words. For example: 'ellipsoid', 'ellipsoid_fd', 'left_atrium', 'femur' Example file structure: - TestFolder/ - dataset_name/ - License.txt - images/ - segmentations/ - meshes/ - landmarks/ - distance_transforms/ - shape_models/ Example python upload usage: import DatasetUtils DatasetUtils.uploadNewDataset('dataset_name', 'TestFolder/dataset_name/') When calling DatasetUtils.downloadDataset('dataset_name') , you will get a zip file with the following structure: - dataset_name.zip - dataset_name/ - License.txt - images/ - segmentations/ - meshes/ - landmarks/ - distance_transforms/ - shape_models/ Datasets API DatasetUtils.downloadDataset(datasetName, destinationPath='.', asZip = True, fileList = None) Parameters: datasetName is one of the names returned by DatasetUtils.getDatasetList() destinationPath is where the zip file or folder will go once it is downloaded asZip toggles whether to download as zip or download individual files. (providing a fileList disables this functionality) fileList is a list of files to download. Example for femur: ['images/m03_1x_hip.nrrd', 'distance_transforms/m03_L_femur.ply'] Returns: True on success and False on failure DatasetUtils.uploadNewDataset(datasetName, datasetPath) Parameters: datasetName is the name the dataset will have on the data portal datasetPath is the path to the root folder of the dataset on the local file system Returns: True on success and False on failure DatasetUtils.getDatasetList() Returns: a list of all existing datasets on the data portal DatasetUtils.getFileList(datasetName) Parameters: datasetName is one of the names returned by DatasetUtils.getDatasetList() Returns: a list of all files in the specified dataset on the data portal","title":"How to Add New Datasets?"},{"location":"dev/datasets.html#how-to-add-new-datasets","text":"ShapeWorks Data Portal","title":"How to Add New Datasets?"},{"location":"dev/datasets.html#dataset-requirements","text":"Each dataset must have a License.txt , which describes the terms of use and citation requirements for the data. You can see an example in the Left Atrium dataset License . Data should be organized into appropriately named directories: images , segmentations , meshes , landmarks , distance_transforms , shape_models . Note that each dataset will likely only have a subset of these directories. For example, the ellipsoid dataset only has a segmentations directory.","title":"Dataset Requirements"},{"location":"dev/datasets.html#images","text":"contents: 3D images of the data file types: itk supported image formats","title":"images/"},{"location":"dev/datasets.html#segmentations","text":"contents: 3D segmentations of the data file types: itk supported image formats","title":"segmentations/"},{"location":"dev/datasets.html#meshes","text":"contents: meshes file types: ply, vtk, stl","title":"meshes/"},{"location":"dev/datasets.html#landmarks","text":"contents: sample specific files containing anatomical landmarks noted by the user file types: csv with 4 columns: x , y , z , label","title":"landmarks/"},{"location":"dev/datasets.html#distance_transforms","text":"contents: 3D distance transform volumes file types: itk supported image formats","title":"distance_transforms/"},{"location":"dev/datasets.html#shape_models","text":"contents: example shape models and parameters used to generate them file types: xml One subfolder per model stores all of the particles files For example: - shape_models/ - example_64.xml - example_64/ - *.particles","title":"shape_models/"},{"location":"dev/datasets.html#uploading-a-dataset","text":"Important Uploaded datasets are only restricted to ShapeWorks developers. Create an account on the ShapeWorks Data Portal Ask Oleks to add you to the list of developers. (oleks@sci.utah.edu) Prepare dataset directory with License.txt Use DatasetUtils.uploadNewDataset(datasetName, datasetPath) to upload. Name the dataset all lowercase with underscores separating words. For example: 'ellipsoid', 'ellipsoid_fd', 'left_atrium', 'femur' Example file structure: - TestFolder/ - dataset_name/ - License.txt - images/ - segmentations/ - meshes/ - landmarks/ - distance_transforms/ - shape_models/ Example python upload usage: import DatasetUtils DatasetUtils.uploadNewDataset('dataset_name', 'TestFolder/dataset_name/') When calling DatasetUtils.downloadDataset('dataset_name') , you will get a zip file with the following structure: - dataset_name.zip - dataset_name/ - License.txt - images/ - segmentations/ - meshes/ - landmarks/ - distance_transforms/ - shape_models/","title":"Uploading a Dataset"},{"location":"dev/datasets.html#datasets-api","text":"","title":"Datasets API"},{"location":"dev/datasets.html#datasetutilsdownloaddatasetdatasetname-destinationpath-aszip-true-filelist-none","text":"Parameters: datasetName is one of the names returned by DatasetUtils.getDatasetList() destinationPath is where the zip file or folder will go once it is downloaded asZip toggles whether to download as zip or download individual files. (providing a fileList disables this functionality) fileList is a list of files to download. Example for femur: ['images/m03_1x_hip.nrrd', 'distance_transforms/m03_L_femur.ply'] Returns: True on success and False on failure","title":"DatasetUtils.downloadDataset(datasetName, destinationPath='.', asZip = True, fileList = None)"},{"location":"dev/datasets.html#datasetutilsuploadnewdatasetdatasetname-datasetpath","text":"Parameters: datasetName is the name the dataset will have on the data portal datasetPath is the path to the root folder of the dataset on the local file system Returns: True on success and False on failure","title":"DatasetUtils.uploadNewDataset(datasetName, datasetPath)"},{"location":"dev/datasets.html#datasetutilsgetdatasetlist","text":"Returns: a list of all existing datasets on the data portal","title":"DatasetUtils.getDatasetList()"},{"location":"dev/datasets.html#datasetutilsgetfilelistdatasetname","text":"Parameters: datasetName is one of the names returned by DatasetUtils.getDatasetList() Returns: a list of all files in the specified dataset on the data portal","title":"DatasetUtils.getFileList(datasetName)"},{"location":"dev/docs.html","text":"Getting Started with Documentation Which branch to use? Please use the mkdocs branch to add/edit your documentation according to the guidelines detailed in this page. Once you are done with your edits, please submit a Pull Request (PR) to get your changes reviewed and merged to the master branch. For PRs that include documentation changes At the beginning of the PR, please add instructions on how one could compile the documentation and what pages should be reviewed. For example, these instructions could include the following: Checkout the mkdocs branch Run mkdocs serve from the root directory (containing mkdocs.yml) Open and review path/to/edited-or-added-md-files See the below discussion: how can I look at the compiled version of these docs? (you might simply direct me to the place where this is explained in the docs) You can find this link in the main readme. Please visit ShapeWorks website for more information. I can open the individual .md files, but can't really give a final review until I've seen it in action. I think the only way you can see it in action is to checkout the branch and run mkdocs serve. These changes won't be reflected in the compiled version (i.e., gh-pages branch) till it is merged to master. @akenmorris ? I think it's completely reasonable to check out the branch and run mkdocs serve then browse to localhost:8000. I recommend explaining this right at the top of the pull request so people don't have to dig through the developer docs to remember the command every time they want to thoroughly review the updates. Naming and Organization Where are the documentation files? ShapeWorks documentation is written using Markdown , a text layout language that enables simple formatting for section headers, code samples, weblinks, and images, yet is still readable as plain text. All documentation markdown files are located in the docs/ folder. Documentation settings are configured by using the mkdocs.yml configuration file in the ShapeWorks directory. Naming convention To maintain consistency, please use lower-case letters and dashes for your markdown files, e.g., fixed-domain-ellipsoid.md . Docs organization ShapeWorks documentation is organized into main sections (e.g., \"Getting Started\", \"Use Cases\", ... etc), which map to subfolders in the docs/ folder (e.g., \"getting-started\", \"use-cases\", ... etc) and images used in their markdown files are located in the corresponding subfolders in the docs/img folder. Adding Call-outs We are now using the Admonition extension to include side content that is minimally disruptive to the document flow. It is also very useful to a reader's draw attention. Below are some examples. To add a note block with a title, use the following markdown syntax. !!! note \"Title of the note\" Here is a note to keep in mind. It will render as follows. Title of the note Here is a note to keep in mind. You can also add a note without a title. !!! note This is a note without a title. This also applies to other call-outs. It will render as follows. Note This is a note without a title To make a note (or any call-out collapsable, use ??? as follows. ??? note This is a collapsable note without a title. This also applies to other call-outs. It will render as follows. Note This is a collapsable note without a title. This also applies to other call-outs. To add a danger block with a title, use the following markdown syntax. !!! danger \"Don't try this at home\" Seriously don't try this; this is a dangerous step to take. It will render as follows. Don't try this at home Seriously don't try this; this is a dangerous step to take. To add an important block without a title, use the following markdown syntax. !!! important This is an admonition box without a title. It will render as follows. Important This is an admonition box without a title. Locally Building Documentation To see local changes to documentation in realtime, cd to where source documentation (i.e., mkdocs.yml ) is maintained and run the following command: mkdocs serve This command builds markdown files into HTML and starts a development server to browse the documentation. Open up http://127.0.0.1:8000/ in your favorite browser to see the documentation. Changes you make to the markdown files are automatically rebuilt. Deploying on GitHub Pages As ShapeWorks is hosted on GitHub , we use GitHub Pages to host the documentation for ShapeWorks. We use Project Pages sites for documentation deployment. The site files are deployed to the gh-pages branch within the ShapeWorks repository. To generate static HTML files for the markdown files, checkout the branch where source documentation ( mkdocs.yml ) is maintained and run the following command: mkdocs gh-deploy Using this command, MkDocs builds the project documentation and uses the ghp-import tool to commit them to the gh-pages branch and push the gh-pages branch to GitHub. All behind the scenes ... Danger Running mkdocs gh-deploy will update the documentation seen by everyone on GitHub . It is recommented that deployed documentation on GitHub should be synced with documentation in the master branch. Important To deploy the documentation associated with software releases, please use the following command. mkdocs gh-deploy -m \"Release Number\" Use mkdocs gh-deploy --clean to start a fresh deployment. Use mkdocs gh-deploy --help for a full list of options available for the gh-deploy command. Take care when you deploy You can not review the built site before pushing it to GitHub. To verify any changes you made locally to the documentation, please use mkdocs build or mkdocs serve . Do not edit gh-pages Never manually edit files on the gh-pages branch because you will lose your work the next time you run the gh-deploy command. Contributing to Documentation Important If you added a new markdown file to docs/ , please make sure to include it in it relevant section in mkdocs.yml to make it accessible. We use GitHub to keep track of issues pertaining to ShapeWorks documentation. For an internal list of todos, which will be turned to issues, visit Documentation ToDo List . Inserting Videos in Documentation Request on GitHub for your video to be uploaded to the SCI ShapeWorks server. Insert it in the markdown file using <p><video src=\"https://sci.utah.edu/~shapeworks/doc-resources/mp4s/video.mp4\" autoplay muted loop controls style=\"width:100%\"></p> . Where are the videos stored? On SCI servers, at /usr/sci/www/shapeworks/doc-resources/mp4s/ . Auto-generating ShapeWorks Commands Documentation The DocumentationUtils package in Python has APIs for auto-documenting command-line tools and to-come-soon python APIs. We use the docs folder to save the generated documentation. To generate documentation for the shapeworks commands, first be sure to run conda_install.sh as described in How to Build ShapeWorks from Source? to install DocumentationUtils . Then, make sure that the shapeworks command is in your path ( set PATH=/path/to/shapeworks:$PATH ), then use Python to run the following command: $ python Python/RunShapeWorksAutoDoc.py --md_filename docs/tools/ShapeWorksCommands.md Parameters : md_filename is the markdown file name for the documentation file to be generated See Also MkDocs documentation Getting started with MkDocs Setting up MkDocs Projects documentation MkDocs recipes","title":"Getting Started with Documentation"},{"location":"dev/docs.html#getting-started-with-documentation","text":"Which branch to use? Please use the mkdocs branch to add/edit your documentation according to the guidelines detailed in this page. Once you are done with your edits, please submit a Pull Request (PR) to get your changes reviewed and merged to the master branch. For PRs that include documentation changes At the beginning of the PR, please add instructions on how one could compile the documentation and what pages should be reviewed. For example, these instructions could include the following: Checkout the mkdocs branch Run mkdocs serve from the root directory (containing mkdocs.yml) Open and review path/to/edited-or-added-md-files See the below discussion: how can I look at the compiled version of these docs? (you might simply direct me to the place where this is explained in the docs) You can find this link in the main readme. Please visit ShapeWorks website for more information. I can open the individual .md files, but can't really give a final review until I've seen it in action. I think the only way you can see it in action is to checkout the branch and run mkdocs serve. These changes won't be reflected in the compiled version (i.e., gh-pages branch) till it is merged to master. @akenmorris ? I think it's completely reasonable to check out the branch and run mkdocs serve then browse to localhost:8000. I recommend explaining this right at the top of the pull request so people don't have to dig through the developer docs to remember the command every time they want to thoroughly review the updates.","title":"Getting Started with Documentation"},{"location":"dev/docs.html#naming-and-organization","text":"Where are the documentation files? ShapeWorks documentation is written using Markdown , a text layout language that enables simple formatting for section headers, code samples, weblinks, and images, yet is still readable as plain text. All documentation markdown files are located in the docs/ folder. Documentation settings are configured by using the mkdocs.yml configuration file in the ShapeWorks directory. Naming convention To maintain consistency, please use lower-case letters and dashes for your markdown files, e.g., fixed-domain-ellipsoid.md . Docs organization ShapeWorks documentation is organized into main sections (e.g., \"Getting Started\", \"Use Cases\", ... etc), which map to subfolders in the docs/ folder (e.g., \"getting-started\", \"use-cases\", ... etc) and images used in their markdown files are located in the corresponding subfolders in the docs/img folder.","title":"Naming and Organization"},{"location":"dev/docs.html#adding-call-outs","text":"We are now using the Admonition extension to include side content that is minimally disruptive to the document flow. It is also very useful to a reader's draw attention. Below are some examples. To add a note block with a title, use the following markdown syntax. !!! note \"Title of the note\" Here is a note to keep in mind. It will render as follows. Title of the note Here is a note to keep in mind. You can also add a note without a title. !!! note This is a note without a title. This also applies to other call-outs. It will render as follows. Note This is a note without a title To make a note (or any call-out collapsable, use ??? as follows. ??? note This is a collapsable note without a title. This also applies to other call-outs. It will render as follows. Note This is a collapsable note without a title. This also applies to other call-outs. To add a danger block with a title, use the following markdown syntax. !!! danger \"Don't try this at home\" Seriously don't try this; this is a dangerous step to take. It will render as follows. Don't try this at home Seriously don't try this; this is a dangerous step to take. To add an important block without a title, use the following markdown syntax. !!! important This is an admonition box without a title. It will render as follows. Important This is an admonition box without a title.","title":"Adding Call-outs"},{"location":"dev/docs.html#locally-building-documentation","text":"To see local changes to documentation in realtime, cd to where source documentation (i.e., mkdocs.yml ) is maintained and run the following command: mkdocs serve This command builds markdown files into HTML and starts a development server to browse the documentation. Open up http://127.0.0.1:8000/ in your favorite browser to see the documentation. Changes you make to the markdown files are automatically rebuilt.","title":"Locally Building Documentation"},{"location":"dev/docs.html#deploying-on-github-pages","text":"As ShapeWorks is hosted on GitHub , we use GitHub Pages to host the documentation for ShapeWorks. We use Project Pages sites for documentation deployment. The site files are deployed to the gh-pages branch within the ShapeWorks repository. To generate static HTML files for the markdown files, checkout the branch where source documentation ( mkdocs.yml ) is maintained and run the following command: mkdocs gh-deploy Using this command, MkDocs builds the project documentation and uses the ghp-import tool to commit them to the gh-pages branch and push the gh-pages branch to GitHub. All behind the scenes ... Danger Running mkdocs gh-deploy will update the documentation seen by everyone on GitHub . It is recommented that deployed documentation on GitHub should be synced with documentation in the master branch. Important To deploy the documentation associated with software releases, please use the following command. mkdocs gh-deploy -m \"Release Number\" Use mkdocs gh-deploy --clean to start a fresh deployment. Use mkdocs gh-deploy --help for a full list of options available for the gh-deploy command. Take care when you deploy You can not review the built site before pushing it to GitHub. To verify any changes you made locally to the documentation, please use mkdocs build or mkdocs serve . Do not edit gh-pages Never manually edit files on the gh-pages branch because you will lose your work the next time you run the gh-deploy command.","title":"Deploying on GitHub Pages"},{"location":"dev/docs.html#contributing-to-documentation","text":"Important If you added a new markdown file to docs/ , please make sure to include it in it relevant section in mkdocs.yml to make it accessible. We use GitHub to keep track of issues pertaining to ShapeWorks documentation. For an internal list of todos, which will be turned to issues, visit Documentation ToDo List .","title":"Contributing to Documentation"},{"location":"dev/docs.html#inserting-videos-in-documentation","text":"Request on GitHub for your video to be uploaded to the SCI ShapeWorks server. Insert it in the markdown file using <p><video src=\"https://sci.utah.edu/~shapeworks/doc-resources/mp4s/video.mp4\" autoplay muted loop controls style=\"width:100%\"></p> . Where are the videos stored? On SCI servers, at /usr/sci/www/shapeworks/doc-resources/mp4s/ .","title":"Inserting Videos in Documentation"},{"location":"dev/docs.html#auto-generating-shapeworks-commands-documentation","text":"The DocumentationUtils package in Python has APIs for auto-documenting command-line tools and to-come-soon python APIs. We use the docs folder to save the generated documentation. To generate documentation for the shapeworks commands, first be sure to run conda_install.sh as described in How to Build ShapeWorks from Source? to install DocumentationUtils . Then, make sure that the shapeworks command is in your path ( set PATH=/path/to/shapeworks:$PATH ), then use Python to run the following command: $ python Python/RunShapeWorksAutoDoc.py --md_filename docs/tools/ShapeWorksCommands.md Parameters : md_filename is the markdown file name for the documentation file to be generated","title":"Auto-generating ShapeWorks Commands Documentation"},{"location":"dev/docs.html#see-also","text":"MkDocs documentation Getting started with MkDocs Setting up MkDocs Projects documentation MkDocs recipes","title":"See Also"},{"location":"dev/gh-actions.html","text":"Getting Started with GitHub Actions ShapeWorks uses GitHub Actions for automated CI/CD (Continuous Integration / Continuous Deployment). GitHub Actions will automatically build ShapeWorks pull requests, run automated tests, and build deployable packages (artifacts). Documentation for GitHub Actions is found here The YAML files for ShapeWorks are found in the source code repository in the .github/workflows folder. build-linux.yml - GitHub Actions recipe for Linux build-windows.yml - GitHub Actions recipe for Windows build-mac.yml - GitHub Actions recipe for MacOS","title":"Getting Started with GitHub Actions"},{"location":"dev/gh-actions.html#getting-started-with-github-actions","text":"ShapeWorks uses GitHub Actions for automated CI/CD (Continuous Integration / Continuous Deployment). GitHub Actions will automatically build ShapeWorks pull requests, run automated tests, and build deployable packages (artifacts). Documentation for GitHub Actions is found here The YAML files for ShapeWorks are found in the source code repository in the .github/workflows folder. build-linux.yml - GitHub Actions recipe for Linux build-windows.yml - GitHub Actions recipe for Windows build-mac.yml - GitHub Actions recipe for MacOS","title":"Getting Started with GitHub Actions"},{"location":"dev/markdown.html","text":"Getting Started with Markdown ShapeWorks documentation is written using Markdown , a text layout language that enables simple formatting for section headers, code samples, weblinks, and images, yet is still readable as plain text. Note If you are viewing this document in GitHub, you can click on the pencil icon in the top-right corner to see its source. On GitHub, it can be used for issues and wiki documentation and edited inline. Grip To edit your Markdown, it helps to have a convenient viewer. The grip instant preview application is beneficial, and is installed by the conda_installs script (see How to Build ShapeWorks from Source? . From the ShapeWorks directory, just run grip (be sure to conda activate shapeworks beforehand), and then navigate to http://localhost:6419 in your favorite browser. It will load and display markdown files just like GitHub, showing README.md by default. You can also specify relative paths to any markdown file below the directory from which it was run. Happy editing! Markdown Basics ShapeWorks uses Markdown for much of its documentation. Here are the basics of using Markdown. The plain text is readable, and there are many editors available, such as Dillinger . One method is to use a plain text editor along with an automatic rendering tool such as grip to view the formatted results in a browser as the files are edited. Use hash symbols to create section headers. Use more hashes for subsequent subsections. # Main section ## Subsection ### Sub-subsection #### And ##### so ###### on... Links are created by enclosing the text shown for the link in brackets and the link directly adjacent to parenthesis. Links to other '#'-indicated sections of the document are formed using a '#' followed by the lowercase text of the section name separated with dashes. For icons, add some additional brackets and a '!'. external site link text [external site](http://google.com) [link text](#local-section-name) [![thumbs up](https://66.media.tumblr.com/1f45d6ab69e02479f85ac1c9f1eb4301/tumblr_inline_pkaqpvkvHH1syktzs_540.png)](http://google.com) HTML comments can be utilized within a Markdown document if you don't want something shown in the rendered output: <!-- commented stuff --> Finally, code can be shown using triple back-ticks (the backward apostrophe: '`' ), even highlighted for a particular language by following the first set of back-ticks with the language name. ```python print(\"Hello Markdown!\") ``` print ( \"Hello Markdown!\" ) Use just a single tick to keep monospaced text inline with the rest of the text . You can also add tables, quoted text like you'd see in an email, bulleted items, images and more. Here are several Markdown quick references . To quickly turn a URL or email address into a link, enclose it in angle brackets. <https://www.markdownguide.org> <fake@example.com> renders as https://www.markdownguide.org fake@example.com See Also GitHub Markdown Markdown Guide Markdown Editors","title":"Getting Started with Markdown"},{"location":"dev/markdown.html#getting-started-with-markdown","text":"ShapeWorks documentation is written using Markdown , a text layout language that enables simple formatting for section headers, code samples, weblinks, and images, yet is still readable as plain text. Note If you are viewing this document in GitHub, you can click on the pencil icon in the top-right corner to see its source. On GitHub, it can be used for issues and wiki documentation and edited inline.","title":"Getting Started with Markdown"},{"location":"dev/markdown.html#grip","text":"To edit your Markdown, it helps to have a convenient viewer. The grip instant preview application is beneficial, and is installed by the conda_installs script (see How to Build ShapeWorks from Source? . From the ShapeWorks directory, just run grip (be sure to conda activate shapeworks beforehand), and then navigate to http://localhost:6419 in your favorite browser. It will load and display markdown files just like GitHub, showing README.md by default. You can also specify relative paths to any markdown file below the directory from which it was run. Happy editing!","title":"Grip"},{"location":"dev/markdown.html#markdown-basics","text":"ShapeWorks uses Markdown for much of its documentation. Here are the basics of using Markdown. The plain text is readable, and there are many editors available, such as Dillinger . One method is to use a plain text editor along with an automatic rendering tool such as grip to view the formatted results in a browser as the files are edited. Use hash symbols to create section headers. Use more hashes for subsequent subsections. # Main section ## Subsection ### Sub-subsection #### And ##### so ###### on... Links are created by enclosing the text shown for the link in brackets and the link directly adjacent to parenthesis. Links to other '#'-indicated sections of the document are formed using a '#' followed by the lowercase text of the section name separated with dashes. For icons, add some additional brackets and a '!'. external site link text [external site](http://google.com) [link text](#local-section-name) [![thumbs up](https://66.media.tumblr.com/1f45d6ab69e02479f85ac1c9f1eb4301/tumblr_inline_pkaqpvkvHH1syktzs_540.png)](http://google.com) HTML comments can be utilized within a Markdown document if you don't want something shown in the rendered output: <!-- commented stuff --> Finally, code can be shown using triple back-ticks (the backward apostrophe: '`' ), even highlighted for a particular language by following the first set of back-ticks with the language name. ```python print(\"Hello Markdown!\") ``` print ( \"Hello Markdown!\" ) Use just a single tick to keep monospaced text inline with the rest of the text . You can also add tables, quoted text like you'd see in an email, bulleted items, images and more. Here are several Markdown quick references . To quickly turn a URL or email address into a link, enclose it in angle brackets. <https://www.markdownguide.org> <fake@example.com> renders as https://www.markdownguide.org fake@example.com","title":"Markdown Basics"},{"location":"dev/markdown.html#see-also","text":"GitHub Markdown Markdown Guide Markdown Editors","title":"See Also"},{"location":"dev/modify-datasets.html","text":"When Modifying Existing Datasets When modifying use case datasets, you must adhere to the use dataset control system to prevent everyone's use cases from breaking. If you want to change the dataset in a way that breaks the use case script, you must upload a new version of the dataset using a suffix <dataset-name>-vN , where N is the dataset version. Example: If I want to change the groomed folder in the ellipsoid use case to groomed_stuff . The current python script looks for a groomed folder so this change will break the script. Because of this, I upload a new version of the ellipsoid dataset with my change, leaving the old version untouched. Then I can proceed to change the script as needed.","title":"When Modifying Existing Datasets"},{"location":"dev/modify-datasets.html#when-modifying-existing-datasets","text":"When modifying use case datasets, you must adhere to the use dataset control system to prevent everyone's use cases from breaking. If you want to change the dataset in a way that breaks the use case script, you must upload a new version of the dataset using a suffix <dataset-name>-vN , where N is the dataset version. Example: If I want to change the groomed folder in the ellipsoid use case to groomed_stuff . The current python script looks for a groomed folder so this change will break the script. Because of this, I upload a new version of the ellipsoid dataset with my change, leaving the old version untouched. Then I can proceed to change the script as needed.","title":"When Modifying Existing Datasets"},{"location":"dev/new-use-case.html","text":"How to Add New Use Cases? Adding a new use case entails the following. Python-scripted workflow Use case dataset Use case documentation Python-scripted workflow To add a new use case to the codebase: Add <use-case-name>.py python script to Examples/Python/ , where the <use-case-name> is a descriptive name for the use case, typically associated with the class of shapes to be modeled (e.g., anatomy name) Update Examples/Python/RunUseCase.py to add the new use case to the list of cases. Look for the line parser.add_argument(\"--use_case\" ...) . Make sure to use the <use-case-name> to the use case list. Use case dataset To add the dataset associated with the new use case: Organize the dataset to adhere to the Dataset Requirements . Upload the datset to the ShapeWorks Data Portal . Visit Uploading a Dataset . Use case documentation To add the documentation of the new dataset, add a markdown file <use-case-name>.md in docs/use-cases with the following structure. Please use the Naming and Organization conventions currently deployed for ShapeWorks documentation. Then, add a corresponding entry in mkdocs.yml . Important If you use the documentation of an existing use case as a starting point, PLEASE make sure to adapt the documentation to your new use case. Important PLEASE, use a spell and grammar checker (e.g., grammarly ). What and Where is the Use Case? : The goal of this use case, i.e., what does this use case demonstrate about ShapeWorks? Details about the dataset, e.g., number of samples, class of shapes, shape representation (meshes, segmentation, or contours), dataset source ... etc. Link to the use case python script. Running the Use Case : Instructions to run the use case that highlights the specific use of RunUseCase.py (in Examples/Python/ ) tags specifically for this use case. Grooming Data : Details about the data preprocessing pipeline for this use case that highlights specific considerations for this use case. Snapshots of intermediate and final groom outputs for illustration. Optimizing Shape Model : Default optimization parameters used for this use case, including a discussion of intuitions/insights on how these parameters were chosen. Analyzing Shape Model : Videos and snapshots of the optimized shape model(s), including the mean shape, scree plot, modes of variations, and individual samples. When applicable, also include videos and snapshots of results that are specific to the use case (e.g., group differences, linear discriminant analysis ... etc.).","title":"How to Add New Use Cases?"},{"location":"dev/new-use-case.html#how-to-add-new-use-cases","text":"Adding a new use case entails the following. Python-scripted workflow Use case dataset Use case documentation","title":"How to Add New Use Cases?"},{"location":"dev/new-use-case.html#python-scripted-workflow","text":"To add a new use case to the codebase: Add <use-case-name>.py python script to Examples/Python/ , where the <use-case-name> is a descriptive name for the use case, typically associated with the class of shapes to be modeled (e.g., anatomy name) Update Examples/Python/RunUseCase.py to add the new use case to the list of cases. Look for the line parser.add_argument(\"--use_case\" ...) . Make sure to use the <use-case-name> to the use case list.","title":"Python-scripted workflow"},{"location":"dev/new-use-case.html#use-case-dataset","text":"To add the dataset associated with the new use case: Organize the dataset to adhere to the Dataset Requirements . Upload the datset to the ShapeWorks Data Portal . Visit Uploading a Dataset .","title":"Use case dataset"},{"location":"dev/new-use-case.html#use-case-documentation","text":"To add the documentation of the new dataset, add a markdown file <use-case-name>.md in docs/use-cases with the following structure. Please use the Naming and Organization conventions currently deployed for ShapeWorks documentation. Then, add a corresponding entry in mkdocs.yml . Important If you use the documentation of an existing use case as a starting point, PLEASE make sure to adapt the documentation to your new use case. Important PLEASE, use a spell and grammar checker (e.g., grammarly ). What and Where is the Use Case? : The goal of this use case, i.e., what does this use case demonstrate about ShapeWorks? Details about the dataset, e.g., number of samples, class of shapes, shape representation (meshes, segmentation, or contours), dataset source ... etc. Link to the use case python script. Running the Use Case : Instructions to run the use case that highlights the specific use of RunUseCase.py (in Examples/Python/ ) tags specifically for this use case. Grooming Data : Details about the data preprocessing pipeline for this use case that highlights specific considerations for this use case. Snapshots of intermediate and final groom outputs for illustration. Optimizing Shape Model : Default optimization parameters used for this use case, including a discussion of intuitions/insights on how these parameters were chosen. Analyzing Shape Model : Videos and snapshots of the optimized shape model(s), including the mean shape, scree plot, modes of variations, and individual samples. When applicable, also include videos and snapshots of results that are specific to the use case (e.g., group differences, linear discriminant analysis ... etc.).","title":"Use case documentation"},{"location":"dev/paths.html","text":"Adding to PATH Environment Variable Danger Be careful doing this! You are responsible for messing up your own PATH. Adding to the PATH on OSX/Linux $ export PATH=path/to/add:$PATH Verify the results with the command: $ echo $PATH Adding to the PATH on Windows $ set PATH=path/to/add;%PATH% This only modifies the path for the current command prompt. To permanently add to the path: Go to Settings/Edit the system environment variables/Environment Variables Choose the Path variable and press Edit... Add your path entry to the list Verify the results with the command: $ echo %PATH%","title":"Adding to PATH Environment Variable"},{"location":"dev/paths.html#adding-to-path-environment-variable","text":"Danger Be careful doing this! You are responsible for messing up your own PATH.","title":"Adding to PATH Environment Variable"},{"location":"dev/paths.html#adding-to-the-path-on-osxlinux","text":"$ export PATH=path/to/add:$PATH Verify the results with the command: $ echo $PATH","title":"Adding to the PATH on OSX/Linux"},{"location":"dev/paths.html#adding-to-the-path-on-windows","text":"$ set PATH=path/to/add;%PATH% This only modifies the path for the current command prompt. To permanently add to the path: Go to Settings/Edit the system environment variables/Environment Variables Choose the Path variable and press Edit... Add your path entry to the list Verify the results with the command: $ echo %PATH%","title":"Adding to the PATH on Windows"},{"location":"dev/python-apis.html","text":"How to Add Python APIs?","title":"How to Add Python APIs?"},{"location":"dev/python-apis.html#how-to-add-python-apis","text":"","title":"How to Add Python APIs?"},{"location":"dev/tests.html","text":"How to Add and Run Unit Tests? Running ShapeWorks Automated Tests To run the automated tests, after building, run: $ ctest or $ make test If using Unix Makefile, for example Adding New ShapeWorks Automated Tests","title":"How to Add and Run Unit Tests?"},{"location":"dev/tests.html#how-to-add-and-run-unit-tests","text":"","title":"How to Add and Run Unit Tests?"},{"location":"dev/tests.html#running-shapeworks-automated-tests","text":"To run the automated tests, after building, run: $ ctest or $ make test If using Unix Makefile, for example","title":"Running ShapeWorks Automated Tests"},{"location":"dev/tests.html#adding-new-shapeworks-automated-tests","text":"","title":"Adding New ShapeWorks Automated Tests"},{"location":"getting-started/examples.html","text":"Examples Visit Getting Started with Use Cases for information about downloading a use case dataset and running use cases. Ellipsoid: Basic Example This example is a stepping stone for the user to get familiar with the workflow of ShapeWorks. This use case represents the standard use version of a shape modeling workflow using ShapeWorks. Fixed Domains Ellipsoid: SSM on New Shapes This use case is designed for the fixed domains functionality of ShapeWorks. The fixed domains are used for the cases where we need to place correspondences on new shapes using a pre-existing shape model. Left Atrium: SSM from Segmentations This segmentation-based use case demonstrates using the ShapeWorks functionality to groom shapes (given as binary segmentations) and their corresponding imaging data (e.g., MRI). This use case also showcases a single-scale and multi-scale optimization for correspondence models. Femurs: SSM from Meshes This mesh-based use case demonstrates using ShapeWorks tools to convert shapes (femurs in this case) given as surface meshes to signed distance maps (the currently supported data type to optimize the particle system). It also shows grooming imaging data (CT scans) of the hip to be tied with the groomed shape data. As femur meshes in this dataset have been segmented with various shaft lengths, this use case includes an interactive tool for the user to select a cutting plane on a single mesh (e.g., representative sample) to remove this variability so that it is not captured in the shape model. Your Own Use Case You can use any of these use cases as a starting point and customize it to your own dataset. Please contact ShapeWorks team for questions and guidance to customize a use case to your data.","title":"Examples"},{"location":"getting-started/examples.html#examples","text":"Visit Getting Started with Use Cases for information about downloading a use case dataset and running use cases.","title":"Examples"},{"location":"getting-started/examples.html#ellipsoid-basic-example","text":"This example is a stepping stone for the user to get familiar with the workflow of ShapeWorks. This use case represents the standard use version of a shape modeling workflow using ShapeWorks.","title":"Ellipsoid: Basic Example"},{"location":"getting-started/examples.html#fixed-domains-ellipsoid-ssm-on-new-shapes","text":"This use case is designed for the fixed domains functionality of ShapeWorks. The fixed domains are used for the cases where we need to place correspondences on new shapes using a pre-existing shape model.","title":"Fixed Domains Ellipsoid: SSM on New Shapes"},{"location":"getting-started/examples.html#left-atrium-ssm-from-segmentations","text":"This segmentation-based use case demonstrates using the ShapeWorks functionality to groom shapes (given as binary segmentations) and their corresponding imaging data (e.g., MRI). This use case also showcases a single-scale and multi-scale optimization for correspondence models.","title":"Left Atrium: SSM from Segmentations"},{"location":"getting-started/examples.html#femurs-ssm-from-meshes","text":"This mesh-based use case demonstrates using ShapeWorks tools to convert shapes (femurs in this case) given as surface meshes to signed distance maps (the currently supported data type to optimize the particle system). It also shows grooming imaging data (CT scans) of the hip to be tied with the groomed shape data. As femur meshes in this dataset have been segmented with various shaft lengths, this use case includes an interactive tool for the user to select a cutting plane on a single mesh (e.g., representative sample) to remove this variability so that it is not captured in the shape model.","title":"Femurs: SSM from Meshes"},{"location":"getting-started/examples.html#your-own-use-case","text":"You can use any of these use cases as a starting point and customize it to your own dataset. Please contact ShapeWorks team for questions and guidance to customize a use case to your data.","title":"Your Own Use Case"},{"location":"getting-started/how-tos.html","text":"How-Tos Important When using up-to-date development builds from the master branch, please understand that these are in-progress development builds, not official releases. How to install ShapeWorks on Windows , Mac , or Linux . How to get the latest ShapeWorks binary release or up-to-date development builds from the master branch for Windows , Mac , or Linux . How to run and see ShapeWorks in action on exemplar use cases . How to preprocess or groom your dataset. How to optimize your shape model. How to visualize and analyze your optimized shape model. How to contact ShapeWorks team to help you customize a use case to your own dataset. How to build ShapeWorks from source (for developers). How to auto-generate documentation for ShapeWorks interfaces and code (for developers).","title":"How-Tos"},{"location":"getting-started/how-tos.html#how-tos","text":"Important When using up-to-date development builds from the master branch, please understand that these are in-progress development builds, not official releases. How to install ShapeWorks on Windows , Mac , or Linux . How to get the latest ShapeWorks binary release or up-to-date development builds from the master branch for Windows , Mac , or Linux . How to run and see ShapeWorks in action on exemplar use cases . How to preprocess or groom your dataset. How to optimize your shape model. How to visualize and analyze your optimized shape model. How to contact ShapeWorks team to help you customize a use case to your own dataset. How to build ShapeWorks from source (for developers). How to auto-generate documentation for ShapeWorks interfaces and code (for developers).","title":"How-Tos"},{"location":"getting-started/interfaces.html","text":"ShapeWorks Interfaces ShapeWorks tools are designed to support different usage scenarios, including execution on a local computing platform through the terminal ( command line ) and the user-friendly Studio application, APIs including Python , and remote systems such as private or public clouds . ShapeWorks Commands ShapeWorks consists of a set of independent command line tools for preprocessing binary segmentations and surface meshes ( Groom ) and computing landmark-based shape models ( Optimize ). It also includes an interactive user interface called ShapeWorksStudio to analyze and visualize the optimized shape models ( Analyze ). We are consolidating these tools into a single, reusable API that is shared across different computational libraries in ShapeWorks, and a standalone shapeworks command. The shapeworks executable is highly flexible, modular, and loosely coupled, with standardized subcommands and interactive help to perform individual operations needed for a typical shape modeling workflow that includes the Groom, Optimize, and Analyze phases. Please see shapeworks commands documentation for the list of available commands. Activate shapeworks environment Each time you use ShapeWorks from the command line , you must first activate its environment using the conda activate shapeworks command on the terminal. ShapeWorks in Python Coming soon! We are developing Python bindings to enable users to script their customized shape modeling workflows. Exemplar use cases will be updated to use these Python APIs to showcase their use and flexibility. ShapeWorksStudio ShapeWorksStudio is a cross-platform graphical user interface (GUI) to support the standard shape analysis workflow needed by most ShapeWorks users, enabling a wide variety of research scenarios. It is currently supporting image-based grooming and is actively under development to support surface meshes, more sophisticated grooming operations (including user annotations), and offline processing. ShapeWorksStudio enables real-time parameter tuning and visualization of the optimization process and statistical analyses. ShapeWorks in the Cloud Work in progress ... Stay tuned!","title":"ShapeWorks Interfaces"},{"location":"getting-started/interfaces.html#shapeworks-interfaces","text":"ShapeWorks tools are designed to support different usage scenarios, including execution on a local computing platform through the terminal ( command line ) and the user-friendly Studio application, APIs including Python , and remote systems such as private or public clouds .","title":"ShapeWorks Interfaces"},{"location":"getting-started/interfaces.html#shapeworks-commands","text":"ShapeWorks consists of a set of independent command line tools for preprocessing binary segmentations and surface meshes ( Groom ) and computing landmark-based shape models ( Optimize ). It also includes an interactive user interface called ShapeWorksStudio to analyze and visualize the optimized shape models ( Analyze ). We are consolidating these tools into a single, reusable API that is shared across different computational libraries in ShapeWorks, and a standalone shapeworks command. The shapeworks executable is highly flexible, modular, and loosely coupled, with standardized subcommands and interactive help to perform individual operations needed for a typical shape modeling workflow that includes the Groom, Optimize, and Analyze phases. Please see shapeworks commands documentation for the list of available commands. Activate shapeworks environment Each time you use ShapeWorks from the command line , you must first activate its environment using the conda activate shapeworks command on the terminal.","title":"ShapeWorks Commands"},{"location":"getting-started/interfaces.html#shapeworks-in-python","text":"Coming soon! We are developing Python bindings to enable users to script their customized shape modeling workflows. Exemplar use cases will be updated to use these Python APIs to showcase their use and flexibility.","title":"ShapeWorks in Python"},{"location":"getting-started/interfaces.html#shapeworksstudio","text":"ShapeWorksStudio is a cross-platform graphical user interface (GUI) to support the standard shape analysis workflow needed by most ShapeWorks users, enabling a wide variety of research scenarios. It is currently supporting image-based grooming and is actively under development to support surface meshes, more sophisticated grooming operations (including user annotations), and offline processing. ShapeWorksStudio enables real-time parameter tuning and visualization of the optimization process and statistical analyses.","title":"ShapeWorksStudio"},{"location":"getting-started/interfaces.html#shapeworks-in-the-cloud","text":"Work in progress ... Stay tuned!","title":"ShapeWorks in the Cloud"},{"location":"getting-started/shapes.html","text":"Shapes, What & From Where? What is Shape? The shape is the characteristic that remains after removing all global geometrical information from an object. To study shape, we would like to study the differences among these characteristics in populations of objects belonging to the same class. What is Shape Modeling? Shape modeling is about learning population-specific parameterization. Where Shapes Come From? In medical imaging, shapes can be obtained from images of anatomies (e.g., CTs and MRIs), where anatomies of interest can be manually or semi-automatically segmented/delineated. Other applications could entail modeling geometries using computer-aided design systems, or acquiring real-world objects through sampling (2D via image acquisition devices, 3D via laser scanners). Free Softwares for Segmentation Seg3D ITK-SNAP 3DSlicer","title":"Shapes, What & From Where?"},{"location":"getting-started/shapes.html#shapes-what-from-where","text":"","title":"Shapes, What &amp; From Where?"},{"location":"getting-started/shapes.html#what-is-shape","text":"The shape is the characteristic that remains after removing all global geometrical information from an object. To study shape, we would like to study the differences among these characteristics in populations of objects belonging to the same class.","title":"What is Shape?"},{"location":"getting-started/shapes.html#what-is-shape-modeling","text":"Shape modeling is about learning population-specific parameterization.","title":"What is Shape Modeling?"},{"location":"getting-started/shapes.html#where-shapes-come-from","text":"In medical imaging, shapes can be obtained from images of anatomies (e.g., CTs and MRIs), where anatomies of interest can be manually or semi-automatically segmented/delineated. Other applications could entail modeling geometries using computer-aided design systems, or acquiring real-world objects through sampling (2D via image acquisition devices, 3D via laser scanners).","title":"Where Shapes Come From?"},{"location":"getting-started/shapes.html#free-softwares-for-segmentation","text":"Seg3D ITK-SNAP 3DSlicer","title":"Free Softwares for Segmentation"},{"location":"getting-started/sw-stories.html","text":"ShapeWorks Success Stories (selected) Hip Joint FAI Pathology Cam-type femoroacetabular impingement (FAI) is a morphologic deformity of the femur that may reduce the femoral neck and acetabulum's clearance, resulting in high shear forces to the cartilage. By analyzing the femur cortical bone thickness between asymptomatic controls and cam-FAI patients, collaborators have used ShapeWorks to show that impingement likely induces bone hypertrophy. Information provided by ShapeWorks has resulted in resection guidelines that can be easily executed in the operating room. Moreover, the limitations of radiographic measurements of plain film radiographs were established, which are often used in the clinical diagnosis of cam-FAI. With a shape score that depends on group-specific mean shapes, the optimized correspondence model from ShapeWorks was used to place subject-specific anatomy on a disease spectrum that is statistically derived from the shape population, providing an objective metric to assess severity. ShapeWorks has further helped develop cost-effective patient-specific meshes (which otherwise require hundreds of man-hours) of the cartilage and labrum to develop computational models and simulations to model contact mechanics and the pathogenesis of hip osteoarthritis. Relevant Papers P. Atkins, P. Mukherjee, S. Elhabian, S. Singla, M. Harris, J. Weiss, R. Whitaker, and A. Anderson. Proximal femoral cortical bone thickness in patients with femoroacetabular impingement and normal hips analyzed using statistical shape modeling. In Summer Biomechanics, Bioengineering and Biotransport Conference, 2015. P. R. Atkins, S. Y. Elhabian, P. Agrawal, M. D. Harris, R. T. Whitaker, J. A. Weiss, C. L. Peters, and A. E. Anderson. Quantitative comparison of cortical bone thickness using correspondence-based shape modeling in patients with cam femoroacetabular impingement. Journal of Orthopaedic Research, 35(8):1743\u20131753, 2017. P. R. Atkins, S. K. Aoki, R. T. Whitaker, J. A. Weiss, C. L. Peters, and A. E. Anderson. Does removal of subchondral cortical bone provide sufficient resection depth for treatment of cam femoroacetabular impingement? Clinical Orthopaedics and Related Research\u0000R , 475(8):1977\u20131986, 2017. P. R. Atkins, S. K. Aoki, S. Y. Elhabian, P. Agrawal, R. T. Whitaker, J. A. Weiss, C. L. Peters, and A. E. Anderson. Evaluation of the Sclerotic Subchondral Bone Boundary as a Surgical Resection Guide in the Treatment of Cam-type Femoroacetabular Impingement. In Annual Meeting of Orthopaedic Research Society, 2017. P. Atkins, S. Elhabian, P. Agrawal, R. Whitaker, J. Weiss, S. Aoki, C. Peters, and A. Anderson. Can the sclerotic subchondral bone of the proximal femur cam lesion be used as a surgical resection guide? An objective analysis using 3D computed tomography and statistical shape modeling. In International Society of Hip Arthroscopy Annual Scientific Meeting, 2016. P. Atkins, S. Elhabian, P. Agrawal, R. Whitaker, J. Weiss, C. Peters, S. Aoki, and A. Anderson. Which radiographic measurements best identify anatomical variation in femoral head anatomy? Analysis using 3D computed tomography and statistical shape modeling. In International Society of Hip Arthroscopy Annual Scientific Meeting, 2016. P. Atkins, Y. Shin, P. Agrawal, S. Elhabian, R. Whitaker, J. Weiss, S. Aoki, C. Peters, and A. Anderson. Which Two-dimensional Radiographic Measurements of Cam Femoroacetabular Impingement Best Describe the Three-dimensional Shape of the Proximal Femur? Clinical Orthopaedics and Related Research\u0000R, 477(1):242\u2013253, 2019. P. Atkins, P. Mukherjee, S. Elhabian, S. Singla, R. Whitaker, J. Weiss, and A. Anderson. Warping of template meshes for efficient subject-specific FE mesh generation. In International Symposium of Computer Methods in Biomechanics and Biomedical Engineering, 2015. Scapular Morphology in Hill-Sachs Patients Surgical procedures for anterior shoulder instability reconstruct the glenoid and its soft tissue by creating an anterior buttress or overcome glenoid bone loss with an additional dynamic stabilizer (e.g., Latarjet procedure). However, the native anatomy must be sacrificed to obtain a stable shoulder. Recent findings suggest that there might be a place for more subtle changes of the periarticular structures, using, for example, a directional osteotomy of the coracoid, so that more normative anatomy is obtained. To this end, ShapeWorks has been used to define a data-driven linear discriminant between the Hill-Sachs lesions and control shapes in the shape space that demonstrates the spectrum of normal and pathologic scapulae (PDF - probability density function). Modes of variations discovered by ShapeWorks were found to relate to clinically relevant shape variations. Mode 1 (33.0% of variation) represented scaling differences. Mode 2 (32.0% of variation) demonstrated large differences around the acromion. In Mode 3 (11.8% of variation), the glenoid inclination and concavity of the glenoid surface were the most substantial. Mode 4 (9.0% of variation) captured primarily differences in orientation of the coracoid pillar, coracoid process size, and bony prominence. Variation in deviation of the coracoid process and the resulting coracoacromial relationship were captured in Mode 5 (3.1% of variation). Relevant Papers Matthijs Jacxsens, Shireen Y. Elhabian, Sarah Brady, Peter Chalmers, Andreas Mueller, Robert Tashjian, Heath Henninger. Thinking outside the glenohumeral box: Hierarchical shape variation of the periarticular anatomy of the scapula using statistical shape modeling. Journal of Orthopaedic Research, in press, 2020. Matthijs Jacxsens, Shireen Y. Elhabian, Sarah Brady, Peter Chalmers, Robert Tashjian, Heath Henninger. Coracoacromial Morphology: A Contributor to Recurrent Traumatic Anterior Glenohumeral Instability?. Journal of Shoulder and Elbow Surgery, 28(7), pp. 1316-1325, 2019. Matthijs Jacxsens, Shireen Y. Elhabian, Robert Z. Tashjian1, Heath B. Henninger. Scapular Morphology In Patients With Hill-Sachs Lesions Using Statistical Shape Modeling. Abstract for podium presentation for the 27th Congress of the European Society for Surgery of the Shoulder and the Elbow (SECEC-ESSSE) conference, 2017. Shape Changes in Atrial Fibrillation Shape changes of the left atrium (LA) and LA appendage (LAA) in AF are hypothesized to be linked to AF pathology and may play a role in thrombogenesis. Thrombus in the LA or LAA, due to stagnant blood flow in these chambers, is thought to be a significant cause of cardioembolic stroke in AF patients. However, many aspects of shape variation in the heart are poorly understood. ShapeWorks models have been used to develop predictive indices of spontaneous echocardiographic contrast (SEC) and thrombus using LAA/LA shape (an indicator for the risk of stroke). Results showed distinct patterns of shape that are statistically more likely to be observed in patients with SEC. Shape-based AF severity has indicated significant differences (p-value < 0.001) in the LA among normal controls, paroxysmal AF, and persistent AF populations. In a recent study, ShapeWorks was used to discover that LA shape was shown to be an independent predictor of AF recurrence after ablation. Hence, ShapeWorks may become a useful tool to improve patient selection for ablation. Relevant Papers J. Cates, E. Bieging, A. Morris, G. Gardner, N. Akoum, E. Kholmovski, N. Marrouche, C. McGann, and R. S. MacLeod. Computational shape models characterize shape change of the left atrium in atrial fibrillation. Clinical Medicine Insights. Cardiology, 8(Suppl 1):99, 2015. E. T. Bieging, A. Morris, B. D. Wilson, C. J. McGann, N. F. Marrouche, and J. Cates. Left atrial shape predicts recurrence after atrial fibrillation catheter ablation. Journal of Cardiovascular Electrophysiology, 2018. Benchmarking ShapeWorks in Clinical Applications A recent benchmarking study has evaluated and validated ShapeWorks, Deformetrica, and SPHARM-PDM in clinical applications that rely on morphometric quantifications, particularly anatomical landmark/measurement inference and lesion screening. Results demonstrate that SSM tools display different levels of consistency. ShapeWorks and Deformetrica models are more consistent than models from SPHARM-PDM due to the groupwise approach of estimating surface correspondences. Furthermore, ShapeWorks and Deformetrica shape models are found to capture clinically relevant population-level variability compared to SPHARM-PDM models. Relevant Papers Anupama Goparaju, Ibolya Csecs, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Ross T. Whitaker, and Shireen Y. Elhabian. On the Evaluation and Validation of Off-the-shelf Statistical Shape Modeling Tools: A Clinical Application. ShapeMI-MICCAI 2018: Workshop on Shape in Medical Imaging, 2018. Anupama Goparaju, Alexandre Bone, Nan Hu, Heath Henninger, Andrew Anderson, Stanely Durrleman, MatthijsJacxsens, Alan Morris, Ibolya Csecs, Nassir Marrouche, Shireen Elhabian, 2020. Benchmarking off-the-shelf statistical shape modeling tools in clinical applications. arXiv preprint arXiv:2009.02878. Next Story Can be Yours!","title":"ShapeWorks Success Stories"},{"location":"getting-started/sw-stories.html#shapeworks-success-stories-selected","text":"","title":"ShapeWorks Success Stories (selected)"},{"location":"getting-started/sw-stories.html#hip-joint-fai-pathology","text":"Cam-type femoroacetabular impingement (FAI) is a morphologic deformity of the femur that may reduce the femoral neck and acetabulum's clearance, resulting in high shear forces to the cartilage. By analyzing the femur cortical bone thickness between asymptomatic controls and cam-FAI patients, collaborators have used ShapeWorks to show that impingement likely induces bone hypertrophy. Information provided by ShapeWorks has resulted in resection guidelines that can be easily executed in the operating room. Moreover, the limitations of radiographic measurements of plain film radiographs were established, which are often used in the clinical diagnosis of cam-FAI. With a shape score that depends on group-specific mean shapes, the optimized correspondence model from ShapeWorks was used to place subject-specific anatomy on a disease spectrum that is statistically derived from the shape population, providing an objective metric to assess severity. ShapeWorks has further helped develop cost-effective patient-specific meshes (which otherwise require hundreds of man-hours) of the cartilage and labrum to develop computational models and simulations to model contact mechanics and the pathogenesis of hip osteoarthritis. Relevant Papers P. Atkins, P. Mukherjee, S. Elhabian, S. Singla, M. Harris, J. Weiss, R. Whitaker, and A. Anderson. Proximal femoral cortical bone thickness in patients with femoroacetabular impingement and normal hips analyzed using statistical shape modeling. In Summer Biomechanics, Bioengineering and Biotransport Conference, 2015. P. R. Atkins, S. Y. Elhabian, P. Agrawal, M. D. Harris, R. T. Whitaker, J. A. Weiss, C. L. Peters, and A. E. Anderson. Quantitative comparison of cortical bone thickness using correspondence-based shape modeling in patients with cam femoroacetabular impingement. Journal of Orthopaedic Research, 35(8):1743\u20131753, 2017. P. R. Atkins, S. K. Aoki, R. T. Whitaker, J. A. Weiss, C. L. Peters, and A. E. Anderson. Does removal of subchondral cortical bone provide sufficient resection depth for treatment of cam femoroacetabular impingement? Clinical Orthopaedics and Related Research\u0000R , 475(8):1977\u20131986, 2017. P. R. Atkins, S. K. Aoki, S. Y. Elhabian, P. Agrawal, R. T. Whitaker, J. A. Weiss, C. L. Peters, and A. E. Anderson. Evaluation of the Sclerotic Subchondral Bone Boundary as a Surgical Resection Guide in the Treatment of Cam-type Femoroacetabular Impingement. In Annual Meeting of Orthopaedic Research Society, 2017. P. Atkins, S. Elhabian, P. Agrawal, R. Whitaker, J. Weiss, S. Aoki, C. Peters, and A. Anderson. Can the sclerotic subchondral bone of the proximal femur cam lesion be used as a surgical resection guide? An objective analysis using 3D computed tomography and statistical shape modeling. In International Society of Hip Arthroscopy Annual Scientific Meeting, 2016. P. Atkins, S. Elhabian, P. Agrawal, R. Whitaker, J. Weiss, C. Peters, S. Aoki, and A. Anderson. Which radiographic measurements best identify anatomical variation in femoral head anatomy? Analysis using 3D computed tomography and statistical shape modeling. In International Society of Hip Arthroscopy Annual Scientific Meeting, 2016. P. Atkins, Y. Shin, P. Agrawal, S. Elhabian, R. Whitaker, J. Weiss, S. Aoki, C. Peters, and A. Anderson. Which Two-dimensional Radiographic Measurements of Cam Femoroacetabular Impingement Best Describe the Three-dimensional Shape of the Proximal Femur? Clinical Orthopaedics and Related Research\u0000R, 477(1):242\u2013253, 2019. P. Atkins, P. Mukherjee, S. Elhabian, S. Singla, R. Whitaker, J. Weiss, and A. Anderson. Warping of template meshes for efficient subject-specific FE mesh generation. In International Symposium of Computer Methods in Biomechanics and Biomedical Engineering, 2015.","title":"Hip Joint FAI Pathology"},{"location":"getting-started/sw-stories.html#scapular-morphology-in-hill-sachs-patients","text":"Surgical procedures for anterior shoulder instability reconstruct the glenoid and its soft tissue by creating an anterior buttress or overcome glenoid bone loss with an additional dynamic stabilizer (e.g., Latarjet procedure). However, the native anatomy must be sacrificed to obtain a stable shoulder. Recent findings suggest that there might be a place for more subtle changes of the periarticular structures, using, for example, a directional osteotomy of the coracoid, so that more normative anatomy is obtained. To this end, ShapeWorks has been used to define a data-driven linear discriminant between the Hill-Sachs lesions and control shapes in the shape space that demonstrates the spectrum of normal and pathologic scapulae (PDF - probability density function). Modes of variations discovered by ShapeWorks were found to relate to clinically relevant shape variations. Mode 1 (33.0% of variation) represented scaling differences. Mode 2 (32.0% of variation) demonstrated large differences around the acromion. In Mode 3 (11.8% of variation), the glenoid inclination and concavity of the glenoid surface were the most substantial. Mode 4 (9.0% of variation) captured primarily differences in orientation of the coracoid pillar, coracoid process size, and bony prominence. Variation in deviation of the coracoid process and the resulting coracoacromial relationship were captured in Mode 5 (3.1% of variation). Relevant Papers Matthijs Jacxsens, Shireen Y. Elhabian, Sarah Brady, Peter Chalmers, Andreas Mueller, Robert Tashjian, Heath Henninger. Thinking outside the glenohumeral box: Hierarchical shape variation of the periarticular anatomy of the scapula using statistical shape modeling. Journal of Orthopaedic Research, in press, 2020. Matthijs Jacxsens, Shireen Y. Elhabian, Sarah Brady, Peter Chalmers, Robert Tashjian, Heath Henninger. Coracoacromial Morphology: A Contributor to Recurrent Traumatic Anterior Glenohumeral Instability?. Journal of Shoulder and Elbow Surgery, 28(7), pp. 1316-1325, 2019. Matthijs Jacxsens, Shireen Y. Elhabian, Robert Z. Tashjian1, Heath B. Henninger. Scapular Morphology In Patients With Hill-Sachs Lesions Using Statistical Shape Modeling. Abstract for podium presentation for the 27th Congress of the European Society for Surgery of the Shoulder and the Elbow (SECEC-ESSSE) conference, 2017.","title":"Scapular Morphology in Hill-Sachs Patients"},{"location":"getting-started/sw-stories.html#shape-changes-in-atrial-fibrillation","text":"Shape changes of the left atrium (LA) and LA appendage (LAA) in AF are hypothesized to be linked to AF pathology and may play a role in thrombogenesis. Thrombus in the LA or LAA, due to stagnant blood flow in these chambers, is thought to be a significant cause of cardioembolic stroke in AF patients. However, many aspects of shape variation in the heart are poorly understood. ShapeWorks models have been used to develop predictive indices of spontaneous echocardiographic contrast (SEC) and thrombus using LAA/LA shape (an indicator for the risk of stroke). Results showed distinct patterns of shape that are statistically more likely to be observed in patients with SEC. Shape-based AF severity has indicated significant differences (p-value < 0.001) in the LA among normal controls, paroxysmal AF, and persistent AF populations. In a recent study, ShapeWorks was used to discover that LA shape was shown to be an independent predictor of AF recurrence after ablation. Hence, ShapeWorks may become a useful tool to improve patient selection for ablation. Relevant Papers J. Cates, E. Bieging, A. Morris, G. Gardner, N. Akoum, E. Kholmovski, N. Marrouche, C. McGann, and R. S. MacLeod. Computational shape models characterize shape change of the left atrium in atrial fibrillation. Clinical Medicine Insights. Cardiology, 8(Suppl 1):99, 2015. E. T. Bieging, A. Morris, B. D. Wilson, C. J. McGann, N. F. Marrouche, and J. Cates. Left atrial shape predicts recurrence after atrial fibrillation catheter ablation. Journal of Cardiovascular Electrophysiology, 2018.","title":"Shape Changes in Atrial Fibrillation"},{"location":"getting-started/sw-stories.html#benchmarking-shapeworks-in-clinical-applications","text":"A recent benchmarking study has evaluated and validated ShapeWorks, Deformetrica, and SPHARM-PDM in clinical applications that rely on morphometric quantifications, particularly anatomical landmark/measurement inference and lesion screening. Results demonstrate that SSM tools display different levels of consistency. ShapeWorks and Deformetrica models are more consistent than models from SPHARM-PDM due to the groupwise approach of estimating surface correspondences. Furthermore, ShapeWorks and Deformetrica shape models are found to capture clinically relevant population-level variability compared to SPHARM-PDM models. Relevant Papers Anupama Goparaju, Ibolya Csecs, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Ross T. Whitaker, and Shireen Y. Elhabian. On the Evaluation and Validation of Off-the-shelf Statistical Shape Modeling Tools: A Clinical Application. ShapeMI-MICCAI 2018: Workshop on Shape in Medical Imaging, 2018. Anupama Goparaju, Alexandre Bone, Nan Hu, Heath Henninger, Andrew Anderson, Stanely Durrleman, MatthijsJacxsens, Alan Morris, Ibolya Csecs, Nassir Marrouche, Shireen Elhabian, 2020. Benchmarking off-the-shelf statistical shape modeling tools in clinical applications. arXiv preprint arXiv:2009.02878.","title":"Benchmarking ShapeWorks in Clinical Applications"},{"location":"getting-started/sw-stories.html#next-story-can-be-yours","text":"","title":"Next Story Can be Yours!"},{"location":"getting-started/workflow.html","text":"Shape Modeling Workflow ShapeWorks supports the typical three-stage workflow for shape modeling. Groom Stage The groom stage entails data inspection and preprocessing, including conversion of the input segmentations and surface meshes into the appropriate data types that are optimization friendly so that landmarks can be optimized in a numerically stable way. It also includes visualization for preprocessed/groomed data for quality control, and we are planning to include annotations for user-defined modeling preferences such as landmarks and constraints. The grooming stage entails rigid transformations to align samples for groupwise modeling and analysis. Imaging data (e.g., CT, MRI) becomes out of alignment and cannot be tied to the resulting shape models. We have developed segmentation-based and mesh-based grooming tools and associated python scripts (i.e., use cases) to carry volumetric data through each grooming step with the shapes (meshes or segmentations) such that they can be used for subsequent analysis and visualization. These tools include image reflection (for paired anatomies), isotropic voxel resampling, image padding, applying shape-based alignment to images (center of mass and rigid alignment), and image cropping. Optimize Stage The optimize stage is an iterative cycle of correspondences (landmarks/particles) optimization, visualization/quality control of resulting correspondence model and parameter tuning. The model initialization proceeds simultaneously with the optimization in a multi-scale fashion using an iterative particle splitting strategy to generate progressively detailed correspondence models with each split. Analyze Stage The analyze stage is the model analysis phase that supports the computation and visualization of the principal components of shape variation, average shapes, and group differences.","title":"Shape Modeling Workflow"},{"location":"getting-started/workflow.html#shape-modeling-workflow","text":"ShapeWorks supports the typical three-stage workflow for shape modeling.","title":"Shape Modeling Workflow"},{"location":"getting-started/workflow.html#groom-stage","text":"The groom stage entails data inspection and preprocessing, including conversion of the input segmentations and surface meshes into the appropriate data types that are optimization friendly so that landmarks can be optimized in a numerically stable way. It also includes visualization for preprocessed/groomed data for quality control, and we are planning to include annotations for user-defined modeling preferences such as landmarks and constraints. The grooming stage entails rigid transformations to align samples for groupwise modeling and analysis. Imaging data (e.g., CT, MRI) becomes out of alignment and cannot be tied to the resulting shape models. We have developed segmentation-based and mesh-based grooming tools and associated python scripts (i.e., use cases) to carry volumetric data through each grooming step with the shapes (meshes or segmentations) such that they can be used for subsequent analysis and visualization. These tools include image reflection (for paired anatomies), isotropic voxel resampling, image padding, applying shape-based alignment to images (center of mass and rigid alignment), and image cropping.","title":"Groom Stage"},{"location":"getting-started/workflow.html#optimize-stage","text":"The optimize stage is an iterative cycle of correspondences (landmarks/particles) optimization, visualization/quality control of resulting correspondence model and parameter tuning. The model initialization proceeds simultaneously with the optimization in a multi-scale fashion using an iterative particle splitting strategy to generate progressively detailed correspondence models with each split.","title":"Optimize Stage"},{"location":"getting-started/workflow.html#analyze-stage","text":"The analyze stage is the model analysis phase that supports the computation and visualization of the principal components of shape variation, average shapes, and group differences.","title":"Analyze Stage"},{"location":"new/new-studio.html","text":"New in ShapeWorksStudio Surface Reconstruction ShapeWorksStudio provides a particle-based surface reconstruction is can reconstruct high quality surface meshes with fewer number of particles. See How to Analyze Your Shape Model? for details about the method. With particle-based surface reconstruction, there is not need to optimize denser particle systems (i.e., with more particles) to reconstruct surface meshes with subvoxel accuracy (old) VTK-based surface reconstruction (new) particle-based surface reconstruction Dynamic Loading ShapeWorksStudio support load-on-demand, which makes it more scalable by supporting loading in order of 100s of samples. For instance, an old small project that took 10+ seconds to load now starts instantly in Studio. (old) without dynamic loading (new) with dynamic loading Live Particles Movement ShapeWorksStudio allows for live introspection of the correpsondence placement optimization process and the ability to abort the optimization at any time (e.g., changing algorithmic parameters). Feature Maps ShapeWorksStudio has the ability to integrate feature maps . A feature map is a 3d image volume that contains scalar values to be associated with each shape\u2019s surface. For example, this could be raw or processed CT/MRI data. The feature map can be displayed for each surface by choosing the desired feature map in the feature map combobox at the bottom of the screen. After the correspondence is generated, the average feature map can be displayed on the mean shape in the analysis tab.","title":"New in ShapeWorksStudio"},{"location":"new/new-studio.html#new-in-shapeworksstudio","text":"","title":"New in ShapeWorksStudio"},{"location":"new/new-studio.html#surface-reconstruction","text":"ShapeWorksStudio provides a particle-based surface reconstruction is can reconstruct high quality surface meshes with fewer number of particles. See How to Analyze Your Shape Model? for details about the method. With particle-based surface reconstruction, there is not need to optimize denser particle systems (i.e., with more particles) to reconstruct surface meshes with subvoxel accuracy (old) VTK-based surface reconstruction (new) particle-based surface reconstruction","title":"Surface Reconstruction"},{"location":"new/new-studio.html#dynamic-loading","text":"ShapeWorksStudio support load-on-demand, which makes it more scalable by supporting loading in order of 100s of samples. For instance, an old small project that took 10+ seconds to load now starts instantly in Studio. (old) without dynamic loading (new) with dynamic loading","title":"Dynamic Loading"},{"location":"new/new-studio.html#live-particles-movement","text":"ShapeWorksStudio allows for live introspection of the correpsondence placement optimization process and the ability to abort the optimization at any time (e.g., changing algorithmic parameters).","title":"Live Particles Movement"},{"location":"new/new-studio.html#feature-maps","text":"ShapeWorksStudio has the ability to integrate feature maps . A feature map is a 3d image volume that contains scalar values to be associated with each shape\u2019s surface. For example, this could be raw or processed CT/MRI data. The feature map can be displayed for each surface by choosing the desired feature map in the feature map combobox at the bottom of the screen. After the correspondence is generated, the average feature map can be displayed on the mean shape in the analysis tab.","title":"Feature Maps"},{"location":"new/openvdb.html","text":"ShapeWorks Takes ~85% Less Memory ShapeWorks uses signed distance transforms to represent shape samples. This, and other quantities computed from the distance transforms consumed a lot of memory. Instead, we now store only a subset of these values that lie within a narrow band off the surface (inside and outside the surface). A default narrow band of 4 units is used. This works well for the tested use cases and is configurable using the <narrow_band> parameter, see: How to Optimize Your Shape Model . We make use of OpenVDB , a more memory-efficient data structure, for signed distance transforms. OpenVDB uses a tree-based data structure to store data in only the relevant voxels. We verified that same distance transform values are obtained and made sure optimizer loudly crashes if we sample outside the narrow band. Lower memory footprint and faster optimization Along with other refactoring and code optimizations, ShapeWorks now uses 85% less memory (from 57.09GB to 9.67GB in one use case). Additionally, the particle optimizer is now 2X faster . ShapeWorks now uses 85% less memory. The particles optimizer is now 2X faster. These benchmarks are reported on a Pelvis datasets of 40 NRRD files and a femur dataset of 57 NRRD files.","title":"ShapeWorks Takes ~85% Less Memory"},{"location":"new/openvdb.html#shapeworks-takes-85-less-memory","text":"ShapeWorks uses signed distance transforms to represent shape samples. This, and other quantities computed from the distance transforms consumed a lot of memory. Instead, we now store only a subset of these values that lie within a narrow band off the surface (inside and outside the surface). A default narrow band of 4 units is used. This works well for the tested use cases and is configurable using the <narrow_band> parameter, see: How to Optimize Your Shape Model . We make use of OpenVDB , a more memory-efficient data structure, for signed distance transforms. OpenVDB uses a tree-based data structure to store data in only the relevant voxels. We verified that same distance transform values are obtained and made sure optimizer loudly crashes if we sample outside the narrow band. Lower memory footprint and faster optimization Along with other refactoring and code optimizations, ShapeWorks now uses 85% less memory (from 57.09GB to 9.67GB in one use case). Additionally, the particle optimizer is now 2X faster . ShapeWorks now uses 85% less memory. The particles optimizer is now 2X faster. These benchmarks are reported on a Pelvis datasets of 40 NRRD files and a femur dataset of 57 NRRD files.","title":"ShapeWorks Takes ~85% Less Memory"},{"location":"new/shapeworks-api.html","text":"ShapeWorks API ShapeWorks was a conglomeration of independent executables for grooming and optimization with a GUI (ShapeWorksStudio) for analysis and visualization. This design is highly inflexible, task specific, and poorly documented, and Studio duplicated a significant portion of their functionality. We have made significant efforts in organizing the codebase based on functionalities, implementing them as libraries rather than executables to provide a common backbone to command-line and GUI-based tools, and syncing ShapeWorksStudio to use the same underlying libraries. To retain command line usage, we have created a single shapeworks command with subcommands exposing this functionality along with greater flexibility and interactive --help for each subcommand. This consolidation makes the framework more powerful and flexible. It also enables ShapeWorks functionality to be used as libraries linked to new applications. All the executables used for the segmentation-driven grooming have been consolidated, documented, tested against the original command line tools, and functionally debugged Comprehensive unit testing is implemented and executed as part of automatic validation run with each addition to the code. This also serves as independent examples of its use Example: ResampleVolumesToBeIsotropic Old command-line: ResampleVolumesToBeIsotropic ./ResampleVolumesToBeIsotropic --inFilename <input-file> --outFilename <output-file> --isoSpacing <voxel-spacing> [--isBinaryImage] [--isCenterImageOn] Disadvantages of the old command-line tool: Cannot be used by other classes or other APIs or other functions Not adaptable (need to edit script files to customize it) Each command needs to be given input and output paths Creates IO bottlenecks Fixed parameters cannot be changed (e.g., num iterations for binarization) All logic is buried behind a single command line tool Resampling images Old command-line: ResampleVolumesToBeIsotropic (for images) ./ResampleVolumesToBeIsotropic --inFilename <input-file> --outFilename <output-file> --isoSpacing <voxel-spacing> --isCenterImageOn New command-line: isoresample (for images) shapeworks readimage --name <input-file> recenter isoresample --isospacing <voxel-spacing> writeimage --name <output-file> C++ (without chaining): isoresample (for images) Image img ( < input - file > ); img . recenter (); img . isoresample ( < voxel - spacing > ); img . write ( < output - file > ); C++ (with chaining): isoresample (for images) Image img ( < input - file > ). recenter (). isoresample ( < voxel - spacing > ). write ( < output - file > ); Resampling segmentations Old command-line: ResampleVolumesToBeIsotropic (for segmentations) ./ResampleVolumesToBeIsotropic --inFilename <input-file> --outFilename <output-file> --isoSpacing <voxel-spacing> --isBinaryImage --isCenterImageOn The old executable\u2019s functionalities are broken down further to make it more modular: Antialias using shapeworks antialias Recenter using shapeworks recenter Binarize using shapeworks binarize Advantages for the new shapeworks API: Promotes user\u2019s understanding of the underlying functionality (more transparency and equivalent simplicity) Allows the user to choose the set of commands to be run User can know what parameters are considered to perform each command User can modify parameter values each step of the way User can save/visualize intermediate outputs for troubleshooting New command-line: isoresample (for segmentations) shapeworks readimage --name <input-file> recenter antialias --iterations <num-iter> isoresample --isospacing <voxel-spacing> binarize writeimage --name <output-file> C++ (without chaining): isoresample (for segmentations) Image img ( < input - file > ); img . recenter (); img . antialias ( < num - iter > ); img . isoresample ( < voxel - spacing > ); img . binarize (); img . write ( < output - file > ); C++ (with chaining): isoresample (for images) Image img ( < input - file > ). recenter (). antialias ( < num - iter > ). isoresample ( < voxel - spacing > ). binarize (). write ( < output - file > );","title":"ShapeWorks API"},{"location":"new/shapeworks-api.html#shapeworks-api","text":"ShapeWorks was a conglomeration of independent executables for grooming and optimization with a GUI (ShapeWorksStudio) for analysis and visualization. This design is highly inflexible, task specific, and poorly documented, and Studio duplicated a significant portion of their functionality. We have made significant efforts in organizing the codebase based on functionalities, implementing them as libraries rather than executables to provide a common backbone to command-line and GUI-based tools, and syncing ShapeWorksStudio to use the same underlying libraries. To retain command line usage, we have created a single shapeworks command with subcommands exposing this functionality along with greater flexibility and interactive --help for each subcommand. This consolidation makes the framework more powerful and flexible. It also enables ShapeWorks functionality to be used as libraries linked to new applications. All the executables used for the segmentation-driven grooming have been consolidated, documented, tested against the original command line tools, and functionally debugged Comprehensive unit testing is implemented and executed as part of automatic validation run with each addition to the code. This also serves as independent examples of its use","title":"ShapeWorks API"},{"location":"new/shapeworks-api.html#example-resamplevolumestobeisotropic","text":"Old command-line: ResampleVolumesToBeIsotropic ./ResampleVolumesToBeIsotropic --inFilename <input-file> --outFilename <output-file> --isoSpacing <voxel-spacing> [--isBinaryImage] [--isCenterImageOn] Disadvantages of the old command-line tool: Cannot be used by other classes or other APIs or other functions Not adaptable (need to edit script files to customize it) Each command needs to be given input and output paths Creates IO bottlenecks Fixed parameters cannot be changed (e.g., num iterations for binarization) All logic is buried behind a single command line tool","title":"Example: ResampleVolumesToBeIsotropic"},{"location":"new/shapeworks-api.html#resampling-images","text":"Old command-line: ResampleVolumesToBeIsotropic (for images) ./ResampleVolumesToBeIsotropic --inFilename <input-file> --outFilename <output-file> --isoSpacing <voxel-spacing> --isCenterImageOn New command-line: isoresample (for images) shapeworks readimage --name <input-file> recenter isoresample --isospacing <voxel-spacing> writeimage --name <output-file> C++ (without chaining): isoresample (for images) Image img ( < input - file > ); img . recenter (); img . isoresample ( < voxel - spacing > ); img . write ( < output - file > ); C++ (with chaining): isoresample (for images) Image img ( < input - file > ). recenter (). isoresample ( < voxel - spacing > ). write ( < output - file > );","title":"Resampling images"},{"location":"new/shapeworks-api.html#resampling-segmentations","text":"Old command-line: ResampleVolumesToBeIsotropic (for segmentations) ./ResampleVolumesToBeIsotropic --inFilename <input-file> --outFilename <output-file> --isoSpacing <voxel-spacing> --isBinaryImage --isCenterImageOn The old executable\u2019s functionalities are broken down further to make it more modular: Antialias using shapeworks antialias Recenter using shapeworks recenter Binarize using shapeworks binarize Advantages for the new shapeworks API: Promotes user\u2019s understanding of the underlying functionality (more transparency and equivalent simplicity) Allows the user to choose the set of commands to be run User can know what parameters are considered to perform each command User can modify parameter values each step of the way User can save/visualize intermediate outputs for troubleshooting New command-line: isoresample (for segmentations) shapeworks readimage --name <input-file> recenter antialias --iterations <num-iter> isoresample --isospacing <voxel-spacing> binarize writeimage --name <output-file> C++ (without chaining): isoresample (for segmentations) Image img ( < input - file > ); img . recenter (); img . antialias ( < num - iter > ); img . isoresample ( < voxel - spacing > ); img . binarize (); img . write ( < output - file > ); C++ (with chaining): isoresample (for images) Image img ( < input - file > ). recenter (). antialias ( < num - iter > ). isoresample ( < voxel - spacing > ). binarize (). write ( < output - file > );","title":"Resampling segmentations"},{"location":"new/ssm-eval.html","text":"Shape Model Evaluation ShapeWorks support quantitative evaluation of shape models, which can be used for algorithmic parameter tuning. Quantitative metrics are generalizability, specificity, and compactness. In Examples/Python/ , run python RunUseCase.py -use_case ellipsoid_evaluate to see an example of how to compute these metrics. Compactness For fixed training data, a compact model pdf should describe the data's distribution using the smallest possible number of parameters. Poor Model (compactness = 0.3) Compact Model (compactness = 0.99) Generalization The model should be able to generalize from the examples given in the training set, hence describing any valid instance of the class of object, not just those seen in the training set. Specificity This is the requirement that the model can only represent valid instances of the class(es) of objects presented in the training set. Hence, the model is specific for this training set. What is a good shape model? A good shape model should balance the trade-off between three requirements, namely specificity, generalization, and compactness. In particular, a shape model needs to generate samples that are plausible (i.e., respect the population statistics). It also needs to generate samples beyond the training data (i.e., generalizable) while describing the population with few parameters (i.e., compact). ShapeWorks Commands shapeworks readparticlesystem --name *.particles -- compactness --nmodes 1 shapeworks readparticlesystem --name *.particles -- generalization --nmodes 1 shapeworks readparticlesystem --name *.particles -- specificity --nmodes 1 Model Evaluation in Studio","title":"Shape Model Evaluation"},{"location":"new/ssm-eval.html#shape-model-evaluation","text":"ShapeWorks support quantitative evaluation of shape models, which can be used for algorithmic parameter tuning. Quantitative metrics are generalizability, specificity, and compactness. In Examples/Python/ , run python RunUseCase.py -use_case ellipsoid_evaluate to see an example of how to compute these metrics. Compactness For fixed training data, a compact model pdf should describe the data's distribution using the smallest possible number of parameters. Poor Model (compactness = 0.3) Compact Model (compactness = 0.99) Generalization The model should be able to generalize from the examples given in the training set, hence describing any valid instance of the class of object, not just those seen in the training set. Specificity This is the requirement that the model can only represent valid instances of the class(es) of objects presented in the training set. Hence, the model is specific for this training set. What is a good shape model? A good shape model should balance the trade-off between three requirements, namely specificity, generalization, and compactness. In particular, a shape model needs to generate samples that are plausible (i.e., respect the population statistics). It also needs to generate samples beyond the training data (i.e., generalizable) while describing the population with few parameters (i.e., compact).","title":"Shape Model Evaluation"},{"location":"new/ssm-eval.html#shapeworks-commands","text":"shapeworks readparticlesystem --name *.particles -- compactness --nmodes 1 shapeworks readparticlesystem --name *.particles -- generalization --nmodes 1 shapeworks readparticlesystem --name *.particles -- specificity --nmodes 1","title":"ShapeWorks Commands"},{"location":"new/ssm-eval.html#model-evaluation-in-studio","text":"","title":"Model Evaluation in Studio"},{"location":"new/sw-meshes.html","text":"ShapeWorks Directly on Meshes Surface meshes are capable of representing complex surfaces with thin structures while using at most several megabytes of data. By allowing ShapeWorks to optimize shape models directly on meshes, we are reducing the hardware requirements and memory load of the software, allowing it to run faster on weaker systems, including personal computers. Note Femur dataset segmentation data: 9.2GB vs. mesh data: 53.1MB Particle updates using geodesic walks so particles never leave the surface Optimizing particles on spheres with lumps of different sizes Optimizing particles on open meshes","title":"ShapeWorks Directly on Meshes"},{"location":"new/sw-meshes.html#shapeworks-directly-on-meshes","text":"Surface meshes are capable of representing complex surfaces with thin structures while using at most several megabytes of data. By allowing ShapeWorks to optimize shape models directly on meshes, we are reducing the hardware requirements and memory load of the software, allowing it to run faster on weaker systems, including personal computers. Note Femur dataset segmentation data: 9.2GB vs. mesh data: 53.1MB Particle updates using geodesic walks so particles never leave the surface Optimizing particles on spheres with lumps of different sizes Optimizing particles on open meshes","title":"ShapeWorks Directly on Meshes"},{"location":"tools/ShapeWorksCommands.html","text":"ShapeWorks Commands Activate shapeworks environment Each time you use ShapeWorks from the command line, you must first activate its environment using the conda activate shapeworks command on the terminal. Add shapeworks to your path Please make sure that shapeworks is in your path. See Adding to PATH Environment Variable . shapeworks Usage: shapeworks <command> [args]... Description: Unified ShapeWorks executable that includes command line utilities for automated construction of compact statistical landmark-based shape models of ensembles of shapes Options: -h, --help: show this help message and exit --version: show program's version number and exit -q, --quiet: don't print status messages Back to Top Image Commands add Usage: shapeworks add [args]... Description: add a value to each pixel in the given image and/or add another image in a pixelwise manner Options: -h, --help: show this help message and exit -x DOUBLE, --value=DOUBLE: Value to add to each pixel. --name=STRING: Name of image to add pixelwise. Back to Top Back to Image Commands antialias Usage: shapeworks antialias [args]... Description: antialiases binary volumes Options: -h, --help: show this help message and exit --maxrmserror=DOUBLE: Maximum RMS error determines how fast the solver converges. Range [0.0, 1.0], larger is faster [default: 0.01]. --iterations=INT: Maximum number of iterations [default: 50]. --layers=INT: Number of layers around a 3d pixel to use for this computation [default: 3]. Back to Top Back to Image Commands binarize Usage: shapeworks binarize [args]... Description: sets portion of image greater than min and less than or equal to max to the specified value Options: -h, --help: show this help message and exit --min=DOUBLE: Lower threshold level [default: 0.0]. --max=DOUBLE: Upper threshold level [default: inf ]. --value=DOUBLE: Value to set region [default: 1.0]. Back to Top Back to Image Commands blur Usage: shapeworks blur [args]... Description: applies gaussian blur Options: -h, --help: show this help message and exit --sigma=DOUBLE: Value of sigma [default: 0.0]. Back to Top Back to Image Commands bounding-box Usage: shapeworks bounding-box [args]... Description: compute largest bounding box surrounding the specified isovalue of the specified set of images Options: -h, --help: show this help message and exit --names : Paths to images (must be followed by -- ), ex: \"bounding-box --names *.nrrd -- --isovalue 1.5\") --padding=INT: Number of extra voxels in each direction to pad the largest bounding box [default: 0]. --isovalue=DOUBLE: Threshold value [default: 1.0]. Back to Top Back to Image Commands clip Usage: shapeworks clip [args]... Description: clips volume with the specified cutting planes defined by three 3D points Options: -h, --help: show this help message and exit --x1=DOUBLE: Value of x1 for cutting plane [default: 0.0]. --y1=DOUBLE: Value of y1 for cutting plane [default: 0.0]. --z1=DOUBLE: Value of z1 for cutting plane [default: 0.0]. --x2=DOUBLE: Value of x2 for cutting plane [default: 0.0]. --y2=DOUBLE: Value of y2 for cutting plane [default: 0.0]. --z2=DOUBLE: Value of z2 for cutting plane [default: 0.0]. --x3=DOUBLE: Value of x3 for cutting plane [default: 0.0]. --y3=DOUBLE: Value of y3 for cutting plane [default: 0.0]. --z3=DOUBLE: Value of z3 for cutting plane [default: 0.0]. --value=DOUBLE: Value of clipped pixels [default: 0.0]. Back to Top Back to Image Commands close-holes Usage: shapeworks close-holes [args]... Description: closes holes in a volume defined by values larger than specified value Options: -h, --help: show this help message and exit --value=DOUBLE: Largest value not in volume [default: 0.0]. Back to Top Back to Image Commands compare Usage: shapeworks compare [args]... Description: compare two images Options: -h, --help: show this help message and exit --name=STRING: Compare this image with another. --verifyall=BOOL: Also verify origin, spacing, and direction matches [default: true]. --tolerance=DOUBLE: Allowed percentage of pixel differences [default: 0.0]. --precision=DOUBLE: Allowed difference between two pixels for them to still be considered equal [default: 0.0]. Back to Top Back to Image Commands compute-dt Usage: shapeworks compute-dt [args]... Description: computes signed distance transform volume from an image at the specified isovalue Options: -h, --help: show this help message and exit --isovalue=DOUBLE: Level set value that defines the interface between foreground and background [default: 0.0]. Back to Top Back to Image Commands crop Usage: shapeworks crop [args]... Description: crop image down to the current region (e.g., from bounding-box command), or the specified min/max in each direction [default: image dimensions] Options: -h, --help: show this help message and exit --xmin=UNSIGNED: Minimum X. --xmax=UNSIGNED: Maximum X. --ymin=UNSIGNED: Minimum Y. --ymax=UNSIGNED: Maximum Y. --zmin=UNSIGNED: Minimum Z. --zmax=UNSIGNED: Maximum Z. Back to Top Back to Image Commands curvature Usage: shapeworks curvature [args]... Description: denoises an image using curvature driven flow using curvature flow image filter Options: -h, --help: show this help message and exit --iterations=INT: Number of iterations [default: 10]. Back to Top Back to Image Commands divide Usage: shapeworks divide [args]... Description: divide an image by a constant Options: -h, --help: show this help message and exit -x DOUBLE, --value=DOUBLE: Value with which to divide. Back to Top Back to Image Commands extract-label Usage: shapeworks extract-label [args]... Description: extracts/isolates a specific voxel label from a given multi-label volume and outputs the corresponding binary image Options: -h, --help: show this help message and exit --label=DOUBLE: Label value to be extracted [default: 1.0]. Back to Top Back to Image Commands gradient Usage: shapeworks gradient [args]... Description: computes gradient magnitude of an image region at each pixel using gradient magnitude filter Options: -h, --help: show this help message and exit Back to Top Back to Image Commands icp Usage: shapeworks icp [args]... Description: transform current image using iterative closest point (ICP) 3D rigid registration computed from source to target distance maps Options: -h, --help: show this help message and exit --source=STRING: Distance map of source image. --target=STRING: Distance map of target image. --isovalue=DOUBLE: Isovalue of distance maps used to create ICPtransform [default: 0.0]. --iterations=UNSIGNED: Number of iterations run ICP registration [default: 20]. Back to Top Back to Image Commands image-to-mesh Usage: shapeworks image-to-mesh [args]... Description: converts the current image to a mesh Options: -h, --help: show this help message and exit -v DOUBLE, --isovalue=DOUBLE: Isovalue to determine mesh boundary [default: 1.0]. Back to Top Back to Image Commands info Usage: shapeworks info [args]... Description: prints requested image dimensions, spacing, size, origin, direction (coordinate system), center, center of mass and bounding box [default: prints everything] Options: -h, --help: show this help message and exit --dims: Whether to display image dimensions --spacing: Whether to display physical spacing --size: Whether to display size --origin: Whether to display physical origin --direction: Whether to display direction --center: Whether to display center --centerofmass: Whether to display center of mass --boundingbox: Whether to display bounding box Back to Top Back to Image Commands multiply Usage: shapeworks multiply [args]... Description: multiply an image by a constant Options: -h, --help: show this help message and exit -x DOUBLE, --value=DOUBLE: Value with which to multiply. Back to Top Back to Image Commands negate Usage: shapeworks negate [args]... Description: negate the values in the given image Options: -h, --help: show this help message and exit Back to Top Back to Image Commands pad Usage: shapeworks pad [args]... Description: pads an image with specified value by specified number of voxels in the x-, y-, and/or z- directions; origin remains at the same location (note: negative padding to shrink an image is permitted) Options: -h, --help: show this help message and exit --padding=INT: Pad this many voxels in ALL directions (used if set) [default: 0]. -x INT, --padx=INT: Pad this many voxels in the x-direction [default: 0]. -y INT, --pady=INT: Pad this many voxels in the y-direction [default: 0]. -z INT, --padz=INT: Pad this many voxels in the z-direction [default: 0]. --value=DOUBLE: Value used to fill padded voxels [default: 0.0]. Back to Top Back to Image Commands read-image Usage: shapeworks read-image [args]... Description: reads an image Options: -h, --help: show this help message and exit --name=STRING: Name of file to read Back to Top Back to Image Commands recenter Usage: shapeworks recenter [args]... Description: recenters an image by changing its origin in the image header to the physical coordinates of the center of the image Options: -h, --help: show this help message and exit Back to Top Back to Image Commands reflect Usage: shapeworks reflect [args]... Description: reflect image with respect to logical image center and the specified axis Options: -h, --help: show this help message and exit --axis=STRING: Axis along which to reflect (X, Y, or Z). Back to Top Back to Image Commands resample Usage: shapeworks resample [args]... Description: resamples an image using new physical spacing (computes new dims) Options: -h, --help: show this help message and exit --isospacing=DOUBLE: Use this spacing in all dimensions. --spacex=DOUBLE: Pixel spacing in x-direction [default: 1]. --spacey=DOUBLE: Pixel spacing in y-direction [default: 1]. --spacez=DOUBLE: Pixel spacing in z-direction [default: 1]. --sizex=UNSIGNED: Output size in x-direction [default: current size]. --sizey=UNSIGNED: Output size in y-direction [default: current size]. --sizez=UNSIGNED: Output size in z-direction [default: current size]. --originx=DOUBLE: Output origin in x-direction [default: current origin]. --originy=DOUBLE: Output origin in y-direction [default: current origin]. --originz=DOUBLE: Output origin in z-direction [default: current origin]. --interp=CHOICE: Interpolation method to use [default: linear]. (choose from 'linear', 'nearest') Back to Top Back to Image Commands resize Usage: shapeworks resize [args]... Description: resizes an image (computes new physical spacing) Options: -h, --help: show this help message and exit -x UNSIGNED, --sizex=UNSIGNED: Output size in x-direction [default: current size]. -y UNSIGNED, --sizey=UNSIGNED: Output size in y-direction [default: current size]. -z UNSIGNED, --sizez=UNSIGNED: Output size in z-direction [default: current size]. Back to Top Back to Image Commands rotate Usage: shapeworks rotate [args]... Description: rotates image by specified value Options: -h, --help: show this help message and exit -x DOUBLE, --rx=DOUBLE: Physical axis around which to rotate [default: z-axis] -y DOUBLE, --ry=DOUBLE: Physical axis around which to rotate [default: z-axis] -z DOUBLE, --rz=DOUBLE: Physical axis around which to rotate [default: z-axis] --radians=DOUBLE: Angle in radians --degrees=DOUBLE: Angle in degrees Back to Top Back to Image Commands scale Usage: shapeworks scale [args]... Description: scales image by specified value Options: -h, --help: show this help message and exit -x DOUBLE, --sx=DOUBLE: X scale -y DOUBLE, --sy=DOUBLE: Y scale -z DOUBLE, --sz=DOUBLE: Z scale Back to Top Back to Image Commands set-origin Usage: shapeworks set-origin [args]... Description: set origin Options: -h, --help: show this help message and exit -x DOUBLE, --x=DOUBLE: x value of origin [default: 0.0]. -y DOUBLE, --y=DOUBLE: y value of origin [default: 0.0]. -z DOUBLE, --z=DOUBLE: z value of origin [default: 0.0]. Back to Top Back to Image Commands sigmoid Usage: shapeworks sigmoid [args]... Description: computes sigmoid function pixel-wise using sigmoid image filter Options: -h, --help: show this help message and exit --alpha=DOUBLE: Value of alpha [default: 10.0]. --beta=DOUBLE: Value of beta [default: 10.0]. Back to Top Back to Image Commands subtract Usage: shapeworks subtract [args]... Description: subtract a value from each pixel in this image and/or subtract another image in a pixelwise manner Options: -h, --help: show this help message and exit -x DOUBLE, --value=DOUBLE: Value to subtract from each pixel. --name=STRING: Name of image to subtract pixelwise. Back to Top Back to Image Commands topo-preserving-smooth Usage: shapeworks topo-preserving-smooth [args]... Description: Helper command that applies gradient and sigmoid filters to create a feature image for the TPLevelSet filter; note that a curvature flow filter is sometimes applied to the image before this Options: -h, --help: show this help message and exit --scaling=DOUBLE: Scale for TPLevelSet level set filter [default: 20.0]. --alpha=DOUBLE: Value of alpha for sigmoid fitler [default: 10.0]. --beta=DOUBLE: Value of beta for sigmoid fitler [default: 10.0.0]. Back to Top Back to Image Commands tp-levelset Usage: shapeworks tp-levelset [args]... Description: segments structures in image using topology preserving geodesic active contour level set filter Options: -h, --help: show this help message and exit --featureimage=STRING: Path of feature image for filter --scaling=DOUBLE: Value of scale [default: 20.0]. Back to Top Back to Image Commands translate Usage: shapeworks translate [args]... Description: translates image by specified physical (image space) distance Options: -h, --help: show this help message and exit --centerofmass: Use center of mass [default: false]. -x DOUBLE, --tx=DOUBLE: X distance -y DOUBLE, --ty=DOUBLE: Y distance -z DOUBLE, --tz=DOUBLE: Z distance Back to Top Back to Image Commands warp-image Usage: shapeworks warp-image [args]... Description: finds the warp between the source and target landmarks and transforms image by this warp Options: -h, --help: show this help message and exit --source=STRING: Path to source landmarks. --target=STRING: Path to target landmarks. --stride=INT: Every stride points will be used for warping [default: 1]. Back to Top Back to Image Commands write-image Usage: shapeworks write-image [args]... Description: writes the current image (determines type by its extension) Options: -h, --help: show this help message and exit --name=STRING: Name of file to write --compressed=BOOL: Whether to compress file [default: true] Back to Top Back to Image Commands Mesh Commands coverage Usage: shapeworks coverage [args]... Description: creates mesh of coverage between two meshes Options: -h, --help: show this help message and exit --name=STRING: Path to other mesh with which to create coverage. Back to Top Back to Mesh Commands read-mesh Usage: shapeworks read-mesh [args]... Description: reads a mesh Options: -h, --help: show this help message and exit --name=STRING: name of file to read Back to Top Back to Mesh Commands write-mesh Usage: shapeworks write-mesh [args]... Description: writes the current mesh (determines type by its extension) Options: -h, --help: show this help message and exit --name=STRING: name of file to write Back to Top Back to Mesh Commands ParticleSystem Commands compactness Usage: shapeworks compactness [args]... Description: Compute compactness of a loaded particle system Options: -h, --help: show this help message and exit --nmodes=INT: Number of modes to use --saveto=STRING: Save the scree plots for all modes to a file Back to Top Back to ParticleSystem Commands generalization Usage: shapeworks generalization [args]... Description: compute generalization of a loaded particle system Options: -h, --help: show this help message and exit --nmodes=INT: Number of modes to use --saveto=STRING: Save the reconstructions sorted by generalization along with the mapping to the original shape Back to Top Back to ParticleSystem Commands read-particle-system Usage: shapeworks read-particle-system [args]... Description: reads a particle system Options: -h, --help: show this help message and exit --names : paths to .particle files (must be followed by -- ), ex: \"--names *.particle -- next-command...\") Back to Top Back to ParticleSystem Commands specificity Usage: shapeworks specificity [args]... Description: compute specificity of a loaded particle system Options: -h, --help: show this help message and exit --nmodes=INT: Number of modes to use --saveto=STRING: Save the reconstructions sorted by specificity along with the mapping to the original shape Back to Top Back to ParticleSystem Commands","title":"ShapeWorks Commands"},{"location":"tools/ShapeWorksCommands.html#shapeworks-commands","text":"Activate shapeworks environment Each time you use ShapeWorks from the command line, you must first activate its environment using the conda activate shapeworks command on the terminal. Add shapeworks to your path Please make sure that shapeworks is in your path. See Adding to PATH Environment Variable .","title":"ShapeWorks Commands"},{"location":"tools/ShapeWorksCommands.html#shapeworks","text":"Usage: shapeworks <command> [args]... Description: Unified ShapeWorks executable that includes command line utilities for automated construction of compact statistical landmark-based shape models of ensembles of shapes Options: -h, --help: show this help message and exit --version: show program's version number and exit -q, --quiet: don't print status messages Back to Top","title":"shapeworks"},{"location":"tools/ShapeWorksCommands.html#image-commands","text":"","title":"Image Commands"},{"location":"tools/ShapeWorksCommands.html#add","text":"Usage: shapeworks add [args]... Description: add a value to each pixel in the given image and/or add another image in a pixelwise manner Options: -h, --help: show this help message and exit -x DOUBLE, --value=DOUBLE: Value to add to each pixel. --name=STRING: Name of image to add pixelwise. Back to Top Back to Image Commands","title":"add"},{"location":"tools/ShapeWorksCommands.html#antialias","text":"Usage: shapeworks antialias [args]... Description: antialiases binary volumes Options: -h, --help: show this help message and exit --maxrmserror=DOUBLE: Maximum RMS error determines how fast the solver converges. Range [0.0, 1.0], larger is faster [default: 0.01]. --iterations=INT: Maximum number of iterations [default: 50]. --layers=INT: Number of layers around a 3d pixel to use for this computation [default: 3]. Back to Top Back to Image Commands","title":"antialias"},{"location":"tools/ShapeWorksCommands.html#binarize","text":"Usage: shapeworks binarize [args]... Description: sets portion of image greater than min and less than or equal to max to the specified value Options: -h, --help: show this help message and exit --min=DOUBLE: Lower threshold level [default: 0.0]. --max=DOUBLE: Upper threshold level [default: inf ]. --value=DOUBLE: Value to set region [default: 1.0]. Back to Top Back to Image Commands","title":"binarize"},{"location":"tools/ShapeWorksCommands.html#blur","text":"Usage: shapeworks blur [args]... Description: applies gaussian blur Options: -h, --help: show this help message and exit --sigma=DOUBLE: Value of sigma [default: 0.0]. Back to Top Back to Image Commands","title":"blur"},{"location":"tools/ShapeWorksCommands.html#bounding-box","text":"Usage: shapeworks bounding-box [args]... Description: compute largest bounding box surrounding the specified isovalue of the specified set of images Options: -h, --help: show this help message and exit --names : Paths to images (must be followed by -- ), ex: \"bounding-box --names *.nrrd -- --isovalue 1.5\") --padding=INT: Number of extra voxels in each direction to pad the largest bounding box [default: 0]. --isovalue=DOUBLE: Threshold value [default: 1.0]. Back to Top Back to Image Commands","title":"bounding-box"},{"location":"tools/ShapeWorksCommands.html#clip","text":"Usage: shapeworks clip [args]... Description: clips volume with the specified cutting planes defined by three 3D points Options: -h, --help: show this help message and exit --x1=DOUBLE: Value of x1 for cutting plane [default: 0.0]. --y1=DOUBLE: Value of y1 for cutting plane [default: 0.0]. --z1=DOUBLE: Value of z1 for cutting plane [default: 0.0]. --x2=DOUBLE: Value of x2 for cutting plane [default: 0.0]. --y2=DOUBLE: Value of y2 for cutting plane [default: 0.0]. --z2=DOUBLE: Value of z2 for cutting plane [default: 0.0]. --x3=DOUBLE: Value of x3 for cutting plane [default: 0.0]. --y3=DOUBLE: Value of y3 for cutting plane [default: 0.0]. --z3=DOUBLE: Value of z3 for cutting plane [default: 0.0]. --value=DOUBLE: Value of clipped pixels [default: 0.0]. Back to Top Back to Image Commands","title":"clip"},{"location":"tools/ShapeWorksCommands.html#close-holes","text":"Usage: shapeworks close-holes [args]... Description: closes holes in a volume defined by values larger than specified value Options: -h, --help: show this help message and exit --value=DOUBLE: Largest value not in volume [default: 0.0]. Back to Top Back to Image Commands","title":"close-holes"},{"location":"tools/ShapeWorksCommands.html#compare","text":"Usage: shapeworks compare [args]... Description: compare two images Options: -h, --help: show this help message and exit --name=STRING: Compare this image with another. --verifyall=BOOL: Also verify origin, spacing, and direction matches [default: true]. --tolerance=DOUBLE: Allowed percentage of pixel differences [default: 0.0]. --precision=DOUBLE: Allowed difference between two pixels for them to still be considered equal [default: 0.0]. Back to Top Back to Image Commands","title":"compare"},{"location":"tools/ShapeWorksCommands.html#compute-dt","text":"Usage: shapeworks compute-dt [args]... Description: computes signed distance transform volume from an image at the specified isovalue Options: -h, --help: show this help message and exit --isovalue=DOUBLE: Level set value that defines the interface between foreground and background [default: 0.0]. Back to Top Back to Image Commands","title":"compute-dt"},{"location":"tools/ShapeWorksCommands.html#crop","text":"Usage: shapeworks crop [args]... Description: crop image down to the current region (e.g., from bounding-box command), or the specified min/max in each direction [default: image dimensions] Options: -h, --help: show this help message and exit --xmin=UNSIGNED: Minimum X. --xmax=UNSIGNED: Maximum X. --ymin=UNSIGNED: Minimum Y. --ymax=UNSIGNED: Maximum Y. --zmin=UNSIGNED: Minimum Z. --zmax=UNSIGNED: Maximum Z. Back to Top Back to Image Commands","title":"crop"},{"location":"tools/ShapeWorksCommands.html#curvature","text":"Usage: shapeworks curvature [args]... Description: denoises an image using curvature driven flow using curvature flow image filter Options: -h, --help: show this help message and exit --iterations=INT: Number of iterations [default: 10]. Back to Top Back to Image Commands","title":"curvature"},{"location":"tools/ShapeWorksCommands.html#divide","text":"Usage: shapeworks divide [args]... Description: divide an image by a constant Options: -h, --help: show this help message and exit -x DOUBLE, --value=DOUBLE: Value with which to divide. Back to Top Back to Image Commands","title":"divide"},{"location":"tools/ShapeWorksCommands.html#extract-label","text":"Usage: shapeworks extract-label [args]... Description: extracts/isolates a specific voxel label from a given multi-label volume and outputs the corresponding binary image Options: -h, --help: show this help message and exit --label=DOUBLE: Label value to be extracted [default: 1.0]. Back to Top Back to Image Commands","title":"extract-label"},{"location":"tools/ShapeWorksCommands.html#gradient","text":"Usage: shapeworks gradient [args]... Description: computes gradient magnitude of an image region at each pixel using gradient magnitude filter Options: -h, --help: show this help message and exit Back to Top Back to Image Commands","title":"gradient"},{"location":"tools/ShapeWorksCommands.html#icp","text":"Usage: shapeworks icp [args]... Description: transform current image using iterative closest point (ICP) 3D rigid registration computed from source to target distance maps Options: -h, --help: show this help message and exit --source=STRING: Distance map of source image. --target=STRING: Distance map of target image. --isovalue=DOUBLE: Isovalue of distance maps used to create ICPtransform [default: 0.0]. --iterations=UNSIGNED: Number of iterations run ICP registration [default: 20]. Back to Top Back to Image Commands","title":"icp"},{"location":"tools/ShapeWorksCommands.html#image-to-mesh","text":"Usage: shapeworks image-to-mesh [args]... Description: converts the current image to a mesh Options: -h, --help: show this help message and exit -v DOUBLE, --isovalue=DOUBLE: Isovalue to determine mesh boundary [default: 1.0]. Back to Top Back to Image Commands","title":"image-to-mesh"},{"location":"tools/ShapeWorksCommands.html#info","text":"Usage: shapeworks info [args]... Description: prints requested image dimensions, spacing, size, origin, direction (coordinate system), center, center of mass and bounding box [default: prints everything] Options: -h, --help: show this help message and exit --dims: Whether to display image dimensions --spacing: Whether to display physical spacing --size: Whether to display size --origin: Whether to display physical origin --direction: Whether to display direction --center: Whether to display center --centerofmass: Whether to display center of mass --boundingbox: Whether to display bounding box Back to Top Back to Image Commands","title":"info"},{"location":"tools/ShapeWorksCommands.html#multiply","text":"Usage: shapeworks multiply [args]... Description: multiply an image by a constant Options: -h, --help: show this help message and exit -x DOUBLE, --value=DOUBLE: Value with which to multiply. Back to Top Back to Image Commands","title":"multiply"},{"location":"tools/ShapeWorksCommands.html#negate","text":"Usage: shapeworks negate [args]... Description: negate the values in the given image Options: -h, --help: show this help message and exit Back to Top Back to Image Commands","title":"negate"},{"location":"tools/ShapeWorksCommands.html#pad","text":"Usage: shapeworks pad [args]... Description: pads an image with specified value by specified number of voxels in the x-, y-, and/or z- directions; origin remains at the same location (note: negative padding to shrink an image is permitted) Options: -h, --help: show this help message and exit --padding=INT: Pad this many voxels in ALL directions (used if set) [default: 0]. -x INT, --padx=INT: Pad this many voxels in the x-direction [default: 0]. -y INT, --pady=INT: Pad this many voxels in the y-direction [default: 0]. -z INT, --padz=INT: Pad this many voxels in the z-direction [default: 0]. --value=DOUBLE: Value used to fill padded voxels [default: 0.0]. Back to Top Back to Image Commands","title":"pad"},{"location":"tools/ShapeWorksCommands.html#read-image","text":"Usage: shapeworks read-image [args]... Description: reads an image Options: -h, --help: show this help message and exit --name=STRING: Name of file to read Back to Top Back to Image Commands","title":"read-image"},{"location":"tools/ShapeWorksCommands.html#recenter","text":"Usage: shapeworks recenter [args]... Description: recenters an image by changing its origin in the image header to the physical coordinates of the center of the image Options: -h, --help: show this help message and exit Back to Top Back to Image Commands","title":"recenter"},{"location":"tools/ShapeWorksCommands.html#reflect","text":"Usage: shapeworks reflect [args]... Description: reflect image with respect to logical image center and the specified axis Options: -h, --help: show this help message and exit --axis=STRING: Axis along which to reflect (X, Y, or Z). Back to Top Back to Image Commands","title":"reflect"},{"location":"tools/ShapeWorksCommands.html#resample","text":"Usage: shapeworks resample [args]... Description: resamples an image using new physical spacing (computes new dims) Options: -h, --help: show this help message and exit --isospacing=DOUBLE: Use this spacing in all dimensions. --spacex=DOUBLE: Pixel spacing in x-direction [default: 1]. --spacey=DOUBLE: Pixel spacing in y-direction [default: 1]. --spacez=DOUBLE: Pixel spacing in z-direction [default: 1]. --sizex=UNSIGNED: Output size in x-direction [default: current size]. --sizey=UNSIGNED: Output size in y-direction [default: current size]. --sizez=UNSIGNED: Output size in z-direction [default: current size]. --originx=DOUBLE: Output origin in x-direction [default: current origin]. --originy=DOUBLE: Output origin in y-direction [default: current origin]. --originz=DOUBLE: Output origin in z-direction [default: current origin]. --interp=CHOICE: Interpolation method to use [default: linear]. (choose from 'linear', 'nearest') Back to Top Back to Image Commands","title":"resample"},{"location":"tools/ShapeWorksCommands.html#resize","text":"Usage: shapeworks resize [args]... Description: resizes an image (computes new physical spacing) Options: -h, --help: show this help message and exit -x UNSIGNED, --sizex=UNSIGNED: Output size in x-direction [default: current size]. -y UNSIGNED, --sizey=UNSIGNED: Output size in y-direction [default: current size]. -z UNSIGNED, --sizez=UNSIGNED: Output size in z-direction [default: current size]. Back to Top Back to Image Commands","title":"resize"},{"location":"tools/ShapeWorksCommands.html#rotate","text":"Usage: shapeworks rotate [args]... Description: rotates image by specified value Options: -h, --help: show this help message and exit -x DOUBLE, --rx=DOUBLE: Physical axis around which to rotate [default: z-axis] -y DOUBLE, --ry=DOUBLE: Physical axis around which to rotate [default: z-axis] -z DOUBLE, --rz=DOUBLE: Physical axis around which to rotate [default: z-axis] --radians=DOUBLE: Angle in radians --degrees=DOUBLE: Angle in degrees Back to Top Back to Image Commands","title":"rotate"},{"location":"tools/ShapeWorksCommands.html#scale","text":"Usage: shapeworks scale [args]... Description: scales image by specified value Options: -h, --help: show this help message and exit -x DOUBLE, --sx=DOUBLE: X scale -y DOUBLE, --sy=DOUBLE: Y scale -z DOUBLE, --sz=DOUBLE: Z scale Back to Top Back to Image Commands","title":"scale"},{"location":"tools/ShapeWorksCommands.html#set-origin","text":"Usage: shapeworks set-origin [args]... Description: set origin Options: -h, --help: show this help message and exit -x DOUBLE, --x=DOUBLE: x value of origin [default: 0.0]. -y DOUBLE, --y=DOUBLE: y value of origin [default: 0.0]. -z DOUBLE, --z=DOUBLE: z value of origin [default: 0.0]. Back to Top Back to Image Commands","title":"set-origin"},{"location":"tools/ShapeWorksCommands.html#sigmoid","text":"Usage: shapeworks sigmoid [args]... Description: computes sigmoid function pixel-wise using sigmoid image filter Options: -h, --help: show this help message and exit --alpha=DOUBLE: Value of alpha [default: 10.0]. --beta=DOUBLE: Value of beta [default: 10.0]. Back to Top Back to Image Commands","title":"sigmoid"},{"location":"tools/ShapeWorksCommands.html#subtract","text":"Usage: shapeworks subtract [args]... Description: subtract a value from each pixel in this image and/or subtract another image in a pixelwise manner Options: -h, --help: show this help message and exit -x DOUBLE, --value=DOUBLE: Value to subtract from each pixel. --name=STRING: Name of image to subtract pixelwise. Back to Top Back to Image Commands","title":"subtract"},{"location":"tools/ShapeWorksCommands.html#topo-preserving-smooth","text":"Usage: shapeworks topo-preserving-smooth [args]... Description: Helper command that applies gradient and sigmoid filters to create a feature image for the TPLevelSet filter; note that a curvature flow filter is sometimes applied to the image before this Options: -h, --help: show this help message and exit --scaling=DOUBLE: Scale for TPLevelSet level set filter [default: 20.0]. --alpha=DOUBLE: Value of alpha for sigmoid fitler [default: 10.0]. --beta=DOUBLE: Value of beta for sigmoid fitler [default: 10.0.0]. Back to Top Back to Image Commands","title":"topo-preserving-smooth"},{"location":"tools/ShapeWorksCommands.html#tp-levelset","text":"Usage: shapeworks tp-levelset [args]... Description: segments structures in image using topology preserving geodesic active contour level set filter Options: -h, --help: show this help message and exit --featureimage=STRING: Path of feature image for filter --scaling=DOUBLE: Value of scale [default: 20.0]. Back to Top Back to Image Commands","title":"tp-levelset"},{"location":"tools/ShapeWorksCommands.html#translate","text":"Usage: shapeworks translate [args]... Description: translates image by specified physical (image space) distance Options: -h, --help: show this help message and exit --centerofmass: Use center of mass [default: false]. -x DOUBLE, --tx=DOUBLE: X distance -y DOUBLE, --ty=DOUBLE: Y distance -z DOUBLE, --tz=DOUBLE: Z distance Back to Top Back to Image Commands","title":"translate"},{"location":"tools/ShapeWorksCommands.html#warp-image","text":"Usage: shapeworks warp-image [args]... Description: finds the warp between the source and target landmarks and transforms image by this warp Options: -h, --help: show this help message and exit --source=STRING: Path to source landmarks. --target=STRING: Path to target landmarks. --stride=INT: Every stride points will be used for warping [default: 1]. Back to Top Back to Image Commands","title":"warp-image"},{"location":"tools/ShapeWorksCommands.html#write-image","text":"Usage: shapeworks write-image [args]... Description: writes the current image (determines type by its extension) Options: -h, --help: show this help message and exit --name=STRING: Name of file to write --compressed=BOOL: Whether to compress file [default: true] Back to Top Back to Image Commands","title":"write-image"},{"location":"tools/ShapeWorksCommands.html#mesh-commands","text":"","title":"Mesh Commands"},{"location":"tools/ShapeWorksCommands.html#coverage","text":"Usage: shapeworks coverage [args]... Description: creates mesh of coverage between two meshes Options: -h, --help: show this help message and exit --name=STRING: Path to other mesh with which to create coverage. Back to Top Back to Mesh Commands","title":"coverage"},{"location":"tools/ShapeWorksCommands.html#read-mesh","text":"Usage: shapeworks read-mesh [args]... Description: reads a mesh Options: -h, --help: show this help message and exit --name=STRING: name of file to read Back to Top Back to Mesh Commands","title":"read-mesh"},{"location":"tools/ShapeWorksCommands.html#write-mesh","text":"Usage: shapeworks write-mesh [args]... Description: writes the current mesh (determines type by its extension) Options: -h, --help: show this help message and exit --name=STRING: name of file to write Back to Top Back to Mesh Commands","title":"write-mesh"},{"location":"tools/ShapeWorksCommands.html#particlesystem-commands","text":"","title":"ParticleSystem Commands"},{"location":"tools/ShapeWorksCommands.html#compactness","text":"Usage: shapeworks compactness [args]... Description: Compute compactness of a loaded particle system Options: -h, --help: show this help message and exit --nmodes=INT: Number of modes to use --saveto=STRING: Save the scree plots for all modes to a file Back to Top Back to ParticleSystem Commands","title":"compactness"},{"location":"tools/ShapeWorksCommands.html#generalization","text":"Usage: shapeworks generalization [args]... Description: compute generalization of a loaded particle system Options: -h, --help: show this help message and exit --nmodes=INT: Number of modes to use --saveto=STRING: Save the reconstructions sorted by generalization along with the mapping to the original shape Back to Top Back to ParticleSystem Commands","title":"generalization"},{"location":"tools/ShapeWorksCommands.html#read-particle-system","text":"Usage: shapeworks read-particle-system [args]... Description: reads a particle system Options: -h, --help: show this help message and exit --names : paths to .particle files (must be followed by -- ), ex: \"--names *.particle -- next-command...\") Back to Top Back to ParticleSystem Commands","title":"read-particle-system"},{"location":"tools/ShapeWorksCommands.html#specificity","text":"Usage: shapeworks specificity [args]... Description: compute specificity of a loaded particle system Options: -h, --help: show this help message and exit --nmodes=INT: Number of modes to use --saveto=STRING: Save the reconstructions sorted by specificity along with the mapping to the original shape Back to Top Back to ParticleSystem Commands","title":"specificity"},{"location":"use-cases/deep-ssm-femur.html","text":"Femur SSM Directly from Images What and Where is the Use Case? This use case demonstrates how to get shape models from unsegmented images using deep learning on the femur data. This includes performing data augmentation as well as building, training and testing a DeepSSM model. For a detailed description of these processes, please see Data Augmentation for Deep Learning and SSMs Directly from Images . The image and shape data used for training and testing results from running the Femur: SSM from Meshes use case. The use case is located at: Examples/Python/deep-ssm.py Relevant papers Jadie Adams, Riddhish Bhalodia, Shireen Elhabian. Uncertain-DeepSSM: From Images to Probabilistic Shape Models. In MICCAI-ShapeMI, Springer, Cham, 2020. Riddhish Bhalodia, Shireen Elhabian, Ladislav Kavan, and Ross Whitaker. DeepSSM: a deep learning framework for statistical shape modeling from raw images. In MICCAI-ShapeMI, pp. 244-257. Springer, Cham, 2018. Riddhish Bhalodia, Anupama Goparaju, Tim Sodergren, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Joshua Cates, Ross Whitaker, Shireen Elhabian. Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation. Computing in Cardiology (CinC), 2018. Running the Use Case To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case deep_ssm See Getting Started with Use Cases for the full list of tags. Note the following tags are not applicable to this use case: --start_with_prepped_data --use_single_scale --interactive --start_with_image_and_segmentation_data This calls deep_ssm.py (in Examples/Python/ ) to perform the following. Loads the femur dataset using a local version if it exists (i.e., previously downloaded); otherwise the dataset is automatically downloaded from the ShapeWorks Data Portal . This includes the particle files in shape_models created from running the femur use case. Performs data augmentation as described in Data Augmentation for Deep Learning . Creates a DeepSSM model as described in SSMs Directly from Images and uses it to make predictions on unseen images. On CUDA This use case uses Pytorch and requires a GPU to run in a timely manner. When you run conda_installs.sh , it detects if you have a GPU and and installs the version of Pytorch compatible with your version of CUDA. Note we only support the three most recent versions of CUDA. If your GPU requires an older CUDA version, you will need to update the Pytorch install in your shapeworks conda environment to the correct CUDA version. For more information on doing so, see pytorch.org . To do a quick check to see if Pytorch is running on your GPU, you can run the use case with the --tiny-test tag. This will quickly run the use case on a few examples and print an error if it is not running on the GPU. Use Case Pipeline Step 1: Getting the original data The femur data is downloaded from the ShapeWorks Data Portal . This use case uses the original unsegmented images and the corresponding .particles files in the shape_models folder downloaded with the femur dataset. Of the 50 examples in the femur dataset, 40 are used to create training and validation sets, while the remaining 10 are saved for a test set (i.e., held out data). Step 2: Running data augmentation For a full description of the data augmentation process and how to use the ShapeWorks data augmentation Python package, please see Data Augmentation for Deep Learning . The functions relevant to this step are runDataAugmentation and visualizeAugmentation . Data augmentation is run using the images and particle files allocated for training and validation. 4960 augmented samples are created so that DeepSSM can be trained on 5000 total examples. The data is embedded to 6 dimensions using PCA, preserving 95% of the population variation. A kernel density estimate (KDE) distribution is then fit to the embedded data and used in sampling new shape samples for data augmentation. The real and augmented results are then visualized in a matrix of scatterplots. Step 3: Creating torch loaders For a full description of the DeepSSM process and how to use the ShapeWorks DeepSSM Python package, please see SSMs Directly from Images . The functions relevant to this step are getTrainValLoaders and getTestLoader . The images and particle files are reformatted into tensors for training and testing the DeepSSM network. The 5000 original and augmented image/particle pairs are turned into train (80%) and validation (20%) loaders and the images held out for the test set are turned into a test loader. A batch size of 8 is used for optimal GPU capacity. The images in the train, validation, and test sets are downsampled to 75% of their original size to decrease training time. Note If a CUDA memory error occurs when running the use case, the batch size value may need to be decreased. Step 4: Training DeepSSM This step uses function trainDeepSSM documented in SSMs Directly from Images . A DeepSSM model is created and trained for 50 epochs. A learning rate of 0.0001 is used, and the validation error is calculated and reported every epoch. Step 5: Testing DeepSSM This step uses function testDeepSSM documented in SSMs Directly from Images . The trained DeepSSM model is used to predict the PCA scores of the unseen images in the test loader. These scores are then mapped to the particle shape model using the PCA information from data augmentation, and the predicted particles are saved. Step 6: Analyze DeepSSM Results This step uses function analyzeResults documented in SSMs Directly from Images . The DeepSSM predictions are analyzed by considering the surface-to-surface distance between the mesh generated from the original segmentation and the mesh generated from the predicted particles. Heat maps of these distances on the meshes are saved from visualizing the results.","title":"Femur SSM Directly from Images"},{"location":"use-cases/deep-ssm-femur.html#femur-ssm-directly-from-images","text":"","title":"Femur SSM Directly from Images"},{"location":"use-cases/deep-ssm-femur.html#what-and-where-is-the-use-case","text":"This use case demonstrates how to get shape models from unsegmented images using deep learning on the femur data. This includes performing data augmentation as well as building, training and testing a DeepSSM model. For a detailed description of these processes, please see Data Augmentation for Deep Learning and SSMs Directly from Images . The image and shape data used for training and testing results from running the Femur: SSM from Meshes use case. The use case is located at: Examples/Python/deep-ssm.py Relevant papers Jadie Adams, Riddhish Bhalodia, Shireen Elhabian. Uncertain-DeepSSM: From Images to Probabilistic Shape Models. In MICCAI-ShapeMI, Springer, Cham, 2020. Riddhish Bhalodia, Shireen Elhabian, Ladislav Kavan, and Ross Whitaker. DeepSSM: a deep learning framework for statistical shape modeling from raw images. In MICCAI-ShapeMI, pp. 244-257. Springer, Cham, 2018. Riddhish Bhalodia, Anupama Goparaju, Tim Sodergren, Alan Morris, Evgueni Kholmovski, Nassir Marrouche, Joshua Cates, Ross Whitaker, Shireen Elhabian. Deep Learning for End-to-End Atrial Fibrillation Recurrence Estimation. Computing in Cardiology (CinC), 2018.","title":"What and Where is the Use Case?"},{"location":"use-cases/deep-ssm-femur.html#running-the-use-case","text":"To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case deep_ssm See Getting Started with Use Cases for the full list of tags. Note the following tags are not applicable to this use case: --start_with_prepped_data --use_single_scale --interactive --start_with_image_and_segmentation_data This calls deep_ssm.py (in Examples/Python/ ) to perform the following. Loads the femur dataset using a local version if it exists (i.e., previously downloaded); otherwise the dataset is automatically downloaded from the ShapeWorks Data Portal . This includes the particle files in shape_models created from running the femur use case. Performs data augmentation as described in Data Augmentation for Deep Learning . Creates a DeepSSM model as described in SSMs Directly from Images and uses it to make predictions on unseen images. On CUDA This use case uses Pytorch and requires a GPU to run in a timely manner. When you run conda_installs.sh , it detects if you have a GPU and and installs the version of Pytorch compatible with your version of CUDA. Note we only support the three most recent versions of CUDA. If your GPU requires an older CUDA version, you will need to update the Pytorch install in your shapeworks conda environment to the correct CUDA version. For more information on doing so, see pytorch.org . To do a quick check to see if Pytorch is running on your GPU, you can run the use case with the --tiny-test tag. This will quickly run the use case on a few examples and print an error if it is not running on the GPU.","title":"Running the Use Case"},{"location":"use-cases/deep-ssm-femur.html#use-case-pipeline","text":"","title":"Use Case Pipeline"},{"location":"use-cases/deep-ssm-femur.html#step-1-getting-the-original-data","text":"The femur data is downloaded from the ShapeWorks Data Portal . This use case uses the original unsegmented images and the corresponding .particles files in the shape_models folder downloaded with the femur dataset. Of the 50 examples in the femur dataset, 40 are used to create training and validation sets, while the remaining 10 are saved for a test set (i.e., held out data).","title":"Step 1: Getting the original data"},{"location":"use-cases/deep-ssm-femur.html#step-2-running-data-augmentation","text":"For a full description of the data augmentation process and how to use the ShapeWorks data augmentation Python package, please see Data Augmentation for Deep Learning . The functions relevant to this step are runDataAugmentation and visualizeAugmentation . Data augmentation is run using the images and particle files allocated for training and validation. 4960 augmented samples are created so that DeepSSM can be trained on 5000 total examples. The data is embedded to 6 dimensions using PCA, preserving 95% of the population variation. A kernel density estimate (KDE) distribution is then fit to the embedded data and used in sampling new shape samples for data augmentation. The real and augmented results are then visualized in a matrix of scatterplots.","title":"Step 2: Running data augmentation"},{"location":"use-cases/deep-ssm-femur.html#step-3-creating-torch-loaders","text":"For a full description of the DeepSSM process and how to use the ShapeWorks DeepSSM Python package, please see SSMs Directly from Images . The functions relevant to this step are getTrainValLoaders and getTestLoader . The images and particle files are reformatted into tensors for training and testing the DeepSSM network. The 5000 original and augmented image/particle pairs are turned into train (80%) and validation (20%) loaders and the images held out for the test set are turned into a test loader. A batch size of 8 is used for optimal GPU capacity. The images in the train, validation, and test sets are downsampled to 75% of their original size to decrease training time. Note If a CUDA memory error occurs when running the use case, the batch size value may need to be decreased.","title":"Step 3: Creating torch loaders"},{"location":"use-cases/deep-ssm-femur.html#step-4-training-deepssm","text":"This step uses function trainDeepSSM documented in SSMs Directly from Images . A DeepSSM model is created and trained for 50 epochs. A learning rate of 0.0001 is used, and the validation error is calculated and reported every epoch.","title":"Step 4: Training DeepSSM"},{"location":"use-cases/deep-ssm-femur.html#step-5-testing-deepssm","text":"This step uses function testDeepSSM documented in SSMs Directly from Images . The trained DeepSSM model is used to predict the PCA scores of the unseen images in the test loader. These scores are then mapped to the particle shape model using the PCA information from data augmentation, and the predicted particles are saved.","title":"Step 5: Testing DeepSSM"},{"location":"use-cases/deep-ssm-femur.html#step-6-analyze-deepssm-results","text":"This step uses function analyzeResults documented in SSMs Directly from Images . The DeepSSM predictions are analyzed by considering the surface-to-surface distance between the mesh generated from the original segmentation and the mesh generated from the predicted particles. Heat maps of these distances on the meshes are saved from visualizing the results.","title":"Step 6: Analyze DeepSSM Results"},{"location":"use-cases/ellipsoid-cutting-planes.html","text":"Ellipsoid: Cutting Planes What and Where is the Use Case? The ellipsoid dataset comprises of axis-aligned ellipsoids with varying radii along a single axis. This example demonstrates using multiple cutting planes to constrain the distribution of particles. This can be used in modeling scenarios where statistical modeling/analysis is needed for a region-of-interest on the anatomy/object-class at hand without having to affect the input data. Ellipsoids (major radius as one mode of variation) with 2 cutting planes The ellipsoid_cut.py (in Examples/Python/ ) use case represents the standard use version of a shape modeling workflow that entails one or more cutting planes using ShapeWorks. It includes the full pipeline for processed (i.e., groomed) as well as unprocessed data. The use case is located at: Examples/Python/ellipsoid_cut.py Running the Use Case To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_prepped_data : to run the optimization on previously processed/groomed data --use_single_scale : to use the single-scale optimization. Default is multi-scale optimization To run the full pipeline with multi-scale: $ cd / path / to / shapeworks / Examples / Python $ python RunUseCase . py -- use_case ellipsoid_cut This calls ellipsoid_cut.py (in Examples/Python/ ) to perform the following. Loads the ellipsoid dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the segmentations by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Defines two cutting planes to be used to constrain the particle optimization on all ellipsoid. Note that this dataset contains a set of roughly aligned ellispoids; hence a common set of cutting planes can be used for all samples. cutting_plane_points1 = [[10, 10, 0], [-10, -10, 0], [10, -10, 0]] cutting_plane_points2 = [[10, -3, 10], [-10, -3 ,10], [10, -3, -10]] cp = [cutting_plane_points1, cutting_plane_points2] Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using a previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case ellipsoid_cut --start_with_prepped_data Grooming Data The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Isotropic Resampling : Binary segmentations in ellipsoid/segmentations/ are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also be used to produce segmentations with smaller voxel spacing, and thereby reduce aliasing artifacts (i.e., staircase/jagged surface) due to binarization. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center-of-Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. Cropping : The segmentations are cropped so that all of the samples are within the same bounding box. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase. Optimizing Shape Model Below are the default optimization parameters for this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . Note that use_shape_statistics_after parameter is not used when --use_single_scale tag is given to the RunUseCase.py (in Examples/Python/ ). Also note the use of adaptivity_mode , cutting_plane_counts , and cutting_planes optimization parameters to trigger the constrained particles optimization. \"number_of_particles\": 128, \"use_normals\": 1, \"normal_weight\": 10.0, \"checkpointing_interval\": 200, \"keep_checkpoints\": 0, \"iterations_per_split\": 2000, \"optimization_iterations\": 1000, \"starting_regularization\": 100, \"ending_regularization\": 10, \"recompute_regularization_interval\": 2, \"domains_per_shape\": 1, \"domain_type\": 'image', \"relative_weighting\": 10, \"initial_relative_weighting\": 0.01, \"procrustes_interval\": 0, \"procrustes_scaling\": 0, \"save_init_splits\": 0, \"verbosity\": 2, \"adaptivity_mode\": 0, \"cutting_plane_counts\": cutting_plane_counts, \"cutting_planes\": cutting_planes Analyzing Shape Model ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? .","title":"Ellipsoid: Cutting Planes"},{"location":"use-cases/ellipsoid-cutting-planes.html#ellipsoid-cutting-planes","text":"","title":"Ellipsoid: Cutting Planes"},{"location":"use-cases/ellipsoid-cutting-planes.html#what-and-where-is-the-use-case","text":"The ellipsoid dataset comprises of axis-aligned ellipsoids with varying radii along a single axis. This example demonstrates using multiple cutting planes to constrain the distribution of particles. This can be used in modeling scenarios where statistical modeling/analysis is needed for a region-of-interest on the anatomy/object-class at hand without having to affect the input data. Ellipsoids (major radius as one mode of variation) with 2 cutting planes The ellipsoid_cut.py (in Examples/Python/ ) use case represents the standard use version of a shape modeling workflow that entails one or more cutting planes using ShapeWorks. It includes the full pipeline for processed (i.e., groomed) as well as unprocessed data. The use case is located at: Examples/Python/ellipsoid_cut.py","title":"What and Where is the Use Case?"},{"location":"use-cases/ellipsoid-cutting-planes.html#running-the-use-case","text":"To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_prepped_data : to run the optimization on previously processed/groomed data --use_single_scale : to use the single-scale optimization. Default is multi-scale optimization To run the full pipeline with multi-scale: $ cd / path / to / shapeworks / Examples / Python $ python RunUseCase . py -- use_case ellipsoid_cut This calls ellipsoid_cut.py (in Examples/Python/ ) to perform the following. Loads the ellipsoid dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the segmentations by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Defines two cutting planes to be used to constrain the particle optimization on all ellipsoid. Note that this dataset contains a set of roughly aligned ellispoids; hence a common set of cutting planes can be used for all samples. cutting_plane_points1 = [[10, 10, 0], [-10, -10, 0], [10, -10, 0]] cutting_plane_points2 = [[10, -3, 10], [-10, -3 ,10], [10, -3, -10]] cp = [cutting_plane_points1, cutting_plane_points2] Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using a previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case ellipsoid_cut --start_with_prepped_data","title":"Running the Use Case"},{"location":"use-cases/ellipsoid-cutting-planes.html#grooming-data","text":"The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Isotropic Resampling : Binary segmentations in ellipsoid/segmentations/ are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also be used to produce segmentations with smaller voxel spacing, and thereby reduce aliasing artifacts (i.e., staircase/jagged surface) due to binarization. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center-of-Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. Cropping : The segmentations are cropped so that all of the samples are within the same bounding box. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase.","title":"Grooming Data"},{"location":"use-cases/ellipsoid-cutting-planes.html#optimizing-shape-model","text":"Below are the default optimization parameters for this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . Note that use_shape_statistics_after parameter is not used when --use_single_scale tag is given to the RunUseCase.py (in Examples/Python/ ). Also note the use of adaptivity_mode , cutting_plane_counts , and cutting_planes optimization parameters to trigger the constrained particles optimization. \"number_of_particles\": 128, \"use_normals\": 1, \"normal_weight\": 10.0, \"checkpointing_interval\": 200, \"keep_checkpoints\": 0, \"iterations_per_split\": 2000, \"optimization_iterations\": 1000, \"starting_regularization\": 100, \"ending_regularization\": 10, \"recompute_regularization_interval\": 2, \"domains_per_shape\": 1, \"domain_type\": 'image', \"relative_weighting\": 10, \"initial_relative_weighting\": 0.01, \"procrustes_interval\": 0, \"procrustes_scaling\": 0, \"save_init_splits\": 0, \"verbosity\": 2, \"adaptivity_mode\": 0, \"cutting_plane_counts\": cutting_plane_counts, \"cutting_planes\": cutting_planes","title":"Optimizing Shape Model"},{"location":"use-cases/ellipsoid-cutting-planes.html#analyzing-shape-model","text":"ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? .","title":"Analyzing Shape Model"},{"location":"use-cases/ellipsoid.html","text":"Ellipsoid: Basic Example What and Where is the Use Case? The ellipsoid dataset comprises of axis-aligned ellipsoids with varying radii along a single axis. This example is a stepping stone for the user to get familiar with the workflow of ShapeWorks. The ellipsoid.py (in Examples/Python/ ) use case represents the standard use version of a shape modeling workflow using ShapeWorks. It includes the full pipeline for processed (i.e., groomed) as well as unprocessed data. The use case is located at: Examples/Python/ellipsoid.py Running the Use Case To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_prepped_data : to run the optimization on previously processed/groomed data --use_single_scale : to use the single-scale optimization. Default is multi-scale optimization To run the full pipeline with multi-scale: $ cd /path/to/shapeworks/Examples/Python $python RunUseCase.py --use_case ellipsoid This calls ellipsoid.py (in Examples/Python/ ) to perform the following. Loads the ellipsoid dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the segmentations by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using a previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case ellipsoid --start_with_prepped_data Grooming Data The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Isotropic Resampling : Binary segmentations in ellipsoid/segmentations/ are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also be used to produce segmentations with smaller voxel spacing, and thereby reduce aliasing artifacts (i.e., staircase/jagged surface) due to binarization. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center-of-Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. Cropping : The segmentations are cropped so that all of the samples are within the same bounding box. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase. Optimizing Shape Model Below are the default optimization parameters for this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . Note that use_shape_statistics_after parameter is not used when --use_single_scale tag is given to the RunUseCase.py (in Examples/Python/ ). \"number_of_particles\": 128, \"use_normals\": 1, \"normal_weight\": 10.0, \"checkpointing_interval\": 200, \"keep_checkpoints\": 0, \"iterations_per_split\": 2000, \"optimization_iterations\": 1000, \"starting_regularization\": 100, \"ending_regularization\": 10, \"recompute_regularization_interval\": 2, \"domains_per_shape\": 1, \"domain_type\": 'image', \"relative_weighting\": 10, \"initial_relative_weighting\": 0.01, \"procrustes_interval\": 0, \"procrustes_scaling\": 0, \"save_init_splits\": 0, \"verbosity\": 2 \"use_shape_statistics_after\": 32 Analyzing Shape Model ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? .","title":"Ellipsoid: Basic Example"},{"location":"use-cases/ellipsoid.html#ellipsoid-basic-example","text":"","title":"Ellipsoid: Basic Example"},{"location":"use-cases/ellipsoid.html#what-and-where-is-the-use-case","text":"The ellipsoid dataset comprises of axis-aligned ellipsoids with varying radii along a single axis. This example is a stepping stone for the user to get familiar with the workflow of ShapeWorks. The ellipsoid.py (in Examples/Python/ ) use case represents the standard use version of a shape modeling workflow using ShapeWorks. It includes the full pipeline for processed (i.e., groomed) as well as unprocessed data. The use case is located at: Examples/Python/ellipsoid.py","title":"What and Where is the Use Case?"},{"location":"use-cases/ellipsoid.html#running-the-use-case","text":"To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_prepped_data : to run the optimization on previously processed/groomed data --use_single_scale : to use the single-scale optimization. Default is multi-scale optimization To run the full pipeline with multi-scale: $ cd /path/to/shapeworks/Examples/Python $python RunUseCase.py --use_case ellipsoid This calls ellipsoid.py (in Examples/Python/ ) to perform the following. Loads the ellipsoid dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the segmentations by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using a previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case ellipsoid --start_with_prepped_data","title":"Running the Use Case"},{"location":"use-cases/ellipsoid.html#grooming-data","text":"The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Isotropic Resampling : Binary segmentations in ellipsoid/segmentations/ are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also be used to produce segmentations with smaller voxel spacing, and thereby reduce aliasing artifacts (i.e., staircase/jagged surface) due to binarization. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center-of-Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. Cropping : The segmentations are cropped so that all of the samples are within the same bounding box. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase.","title":"Grooming Data"},{"location":"use-cases/ellipsoid.html#optimizing-shape-model","text":"Below are the default optimization parameters for this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . Note that use_shape_statistics_after parameter is not used when --use_single_scale tag is given to the RunUseCase.py (in Examples/Python/ ). \"number_of_particles\": 128, \"use_normals\": 1, \"normal_weight\": 10.0, \"checkpointing_interval\": 200, \"keep_checkpoints\": 0, \"iterations_per_split\": 2000, \"optimization_iterations\": 1000, \"starting_regularization\": 100, \"ending_regularization\": 10, \"recompute_regularization_interval\": 2, \"domains_per_shape\": 1, \"domain_type\": 'image', \"relative_weighting\": 10, \"initial_relative_weighting\": 0.01, \"procrustes_interval\": 0, \"procrustes_scaling\": 0, \"save_init_splits\": 0, \"verbosity\": 2 \"use_shape_statistics_after\": 32","title":"Optimizing Shape Model"},{"location":"use-cases/ellipsoid.html#analyzing-shape-model","text":"ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? .","title":"Analyzing Shape Model"},{"location":"use-cases/femur-cutting-planes.html","text":"Femur with Cutting Planes What and Where is the Use Case? This use case is similar to Femur: SSM from Meshes , but it demonstrates the use of cutting planes to constrain the particle distribution on the femur surface. The femur meshes in this data set have been segmented with various shaft lengths, as can be seen below. To remove this variability so that it is not captured in the shape model, cutting planes can be used to limit the statistical analysis to the common anatomical regions across all samples. Note The given data (i.e., surface meshes) are not clipped using cutting planes as in Femur: SSM from Meshes , but the cutting planes are fed as an optimization parameter to shapeworks optimize command to prevent particles from moving beyond the cutting planes during optimization. The use case has a pre-defined cutting plane, but you can choose to overwrite it and define the cutting plane interactively by running the use case with the --interactive tag. There are two ways to define the cutting plane interactively, as explained in Running with Interactivity . The use case is located at: Examples/Python/femur_cut.py Running the Use Case Important Minimum of 32GB of RAM required to run the full use case. To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_prepped_data : to run the optimization on previously processed/groomed data. --interactive : This tag is used to allow you to interactively select the cutting plane to be used for constraining particle distribution. If this tag is not used, the pre-defined cutting plane is used to clip the given meshes. To run the full pipeline with multi-scale and the pre-defined cutting plane: $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case femur_cut This calls femur_cut.py (in Examples/Python/ ) to perform the following. Loads the femur dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the images and meshes by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., the optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case femur_cut --start_with_prepped_data To groom both the meshes and corresponding images, use --start_with_image_and_segmentation_data tag. The image origin, size, and spacing will be used in mesh rasterization. $ python RunUseCase.py --use_case femur_cut --start_with_image_and_segmentation_data If this tag is not used, grooming will be done on meshes only. The origin and size will be inferred from the meshes for rasterization, and isotropic spacing will be used unless the user specifies otherwise. Grooming Data We start with full unsegmented images (CT scans) of each femur's hip and segmented meshes in this use case. ShapeWorks needs a volumetric representation of shapes in the form of signed distance transforms to optimize the shape model. Hence, given surface meshes are first converted to image-based representation (i.e., binary segmentations). Additionally, the corresponding unsegmented images need to be carried through each grooming step with the meshes to be used for analysis. The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Cutting Plane : If interactive, define cutting plane either in the beginning or after rigid alignment using the interactive window. Within this window: - Zoom in and out by scrolling - Rotate viewpoint by clicking in the surrounding area - Move the cutting plane by clicking on it and dragging - Change normal vector (represented with arrow) by clicking on it and dragging Reflect Meshes : In this use case, we often have both right and left femur surface meshes. To align all the femurs, we choose one side to reflect both the image and mesh. Meshes to Volumes : Meshes must be turned into binary volumes using rasterization. The corresponding image origin, size, and spacing are used to generate the volume. Isotropic Resampling : Both the image and rasterized segmentations are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also produce segmentations with smaller voxel spacing, thereby reducing aliasing artifacts (i.e., staircase/jagged surface) due to binarization. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center of Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations and images are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. The alignment parameters are computed based on aligning segmentations and then applied to their corresponding images. The samples must be centered before they are aligned to a reference. This step can be performed with Isotropic Resampling as it is in the left atrium use case. In the Femur use case, we do so separately so that we can get the translation and apply it to the cutting plane if it has already been selected. Cropping : The images and segmentations are cropped so that all of the samples are within the same bounding box. The bounding box parameters are computed based on the biggest bounding box that encapsulates all the given dataset segmentations. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase. Optimizing Shape Model Below are the default optimization parameters when running this use case using the --use_single_scale tag. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . Note that use_shape_statistics_after parameter is not used when --use_single_scale tag is given to the RunUseCase.py (in Examples/Python/ ). Also note the use of adaptivity_mode , cutting_plane_counts , and cutting_planes optimization parameters to trigger the constrained particles optimization. $python RunUseCase.py --use_case femur_cut --use_single_scale \"number_of_particles\" : 1024, \"use_normals\": 0, \"normal_weight\": 10.0, \"checkpointing_interval\" : 200, \"keep_checkpoints\" : 1, \"iterations_per_split\" : 4000, \"optimization_iterations\" : 4000, \"starting_regularization\" : 100, \"ending_regularization\" : 0.1, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"domain_type\" : 'image', \"relative_weighting\" : 10, \"initial_relative_weighting\" : 1, \"procrustes_interval\" : 1, \"procrustes_scaling\" : 1, \"save_init_splits\" : 1, \"debug_projection\" : 0, \"verbosity\" : 2, \"use_statistics_in_init\" : 0, \"adaptivity_mode\": 0, \"cutting_plane_counts\": cutting_plane_counts, \"cutting_planes\": cutting_planes This use case can be run using the multi-scale optimization without the --use_single_scale tag as follows. $python RunUseCase.py --use_case femur_cut The use_shape_statistics_after parameter is used to trigger the multi-scale optimization mode. \"use_shape_statistics_after\": 64 Analyzing Shape Model ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? . Here is the mean shape, individual samples, and modes of shape variations of the optimized shape mode using single-scale optimization.","title":"Femur with Cutting Planes"},{"location":"use-cases/femur-cutting-planes.html#femur-with-cutting-planes","text":"","title":"Femur with Cutting Planes"},{"location":"use-cases/femur-cutting-planes.html#what-and-where-is-the-use-case","text":"This use case is similar to Femur: SSM from Meshes , but it demonstrates the use of cutting planes to constrain the particle distribution on the femur surface. The femur meshes in this data set have been segmented with various shaft lengths, as can be seen below. To remove this variability so that it is not captured in the shape model, cutting planes can be used to limit the statistical analysis to the common anatomical regions across all samples. Note The given data (i.e., surface meshes) are not clipped using cutting planes as in Femur: SSM from Meshes , but the cutting planes are fed as an optimization parameter to shapeworks optimize command to prevent particles from moving beyond the cutting planes during optimization. The use case has a pre-defined cutting plane, but you can choose to overwrite it and define the cutting plane interactively by running the use case with the --interactive tag. There are two ways to define the cutting plane interactively, as explained in Running with Interactivity . The use case is located at: Examples/Python/femur_cut.py","title":"What and Where is the Use Case?"},{"location":"use-cases/femur-cutting-planes.html#running-the-use-case","text":"Important Minimum of 32GB of RAM required to run the full use case. To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_prepped_data : to run the optimization on previously processed/groomed data. --interactive : This tag is used to allow you to interactively select the cutting plane to be used for constraining particle distribution. If this tag is not used, the pre-defined cutting plane is used to clip the given meshes. To run the full pipeline with multi-scale and the pre-defined cutting plane: $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case femur_cut This calls femur_cut.py (in Examples/Python/ ) to perform the following. Loads the femur dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the images and meshes by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., the optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case femur_cut --start_with_prepped_data To groom both the meshes and corresponding images, use --start_with_image_and_segmentation_data tag. The image origin, size, and spacing will be used in mesh rasterization. $ python RunUseCase.py --use_case femur_cut --start_with_image_and_segmentation_data If this tag is not used, grooming will be done on meshes only. The origin and size will be inferred from the meshes for rasterization, and isotropic spacing will be used unless the user specifies otherwise.","title":"Running the Use Case"},{"location":"use-cases/femur-cutting-planes.html#grooming-data","text":"We start with full unsegmented images (CT scans) of each femur's hip and segmented meshes in this use case. ShapeWorks needs a volumetric representation of shapes in the form of signed distance transforms to optimize the shape model. Hence, given surface meshes are first converted to image-based representation (i.e., binary segmentations). Additionally, the corresponding unsegmented images need to be carried through each grooming step with the meshes to be used for analysis. The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Cutting Plane : If interactive, define cutting plane either in the beginning or after rigid alignment using the interactive window. Within this window: - Zoom in and out by scrolling - Rotate viewpoint by clicking in the surrounding area - Move the cutting plane by clicking on it and dragging - Change normal vector (represented with arrow) by clicking on it and dragging Reflect Meshes : In this use case, we often have both right and left femur surface meshes. To align all the femurs, we choose one side to reflect both the image and mesh. Meshes to Volumes : Meshes must be turned into binary volumes using rasterization. The corresponding image origin, size, and spacing are used to generate the volume. Isotropic Resampling : Both the image and rasterized segmentations are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also produce segmentations with smaller voxel spacing, thereby reducing aliasing artifacts (i.e., staircase/jagged surface) due to binarization. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center of Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations and images are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. The alignment parameters are computed based on aligning segmentations and then applied to their corresponding images. The samples must be centered before they are aligned to a reference. This step can be performed with Isotropic Resampling as it is in the left atrium use case. In the Femur use case, we do so separately so that we can get the translation and apply it to the cutting plane if it has already been selected. Cropping : The images and segmentations are cropped so that all of the samples are within the same bounding box. The bounding box parameters are computed based on the biggest bounding box that encapsulates all the given dataset segmentations. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase.","title":"Grooming Data"},{"location":"use-cases/femur-cutting-planes.html#optimizing-shape-model","text":"Below are the default optimization parameters when running this use case using the --use_single_scale tag. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . Note that use_shape_statistics_after parameter is not used when --use_single_scale tag is given to the RunUseCase.py (in Examples/Python/ ). Also note the use of adaptivity_mode , cutting_plane_counts , and cutting_planes optimization parameters to trigger the constrained particles optimization. $python RunUseCase.py --use_case femur_cut --use_single_scale \"number_of_particles\" : 1024, \"use_normals\": 0, \"normal_weight\": 10.0, \"checkpointing_interval\" : 200, \"keep_checkpoints\" : 1, \"iterations_per_split\" : 4000, \"optimization_iterations\" : 4000, \"starting_regularization\" : 100, \"ending_regularization\" : 0.1, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"domain_type\" : 'image', \"relative_weighting\" : 10, \"initial_relative_weighting\" : 1, \"procrustes_interval\" : 1, \"procrustes_scaling\" : 1, \"save_init_splits\" : 1, \"debug_projection\" : 0, \"verbosity\" : 2, \"use_statistics_in_init\" : 0, \"adaptivity_mode\": 0, \"cutting_plane_counts\": cutting_plane_counts, \"cutting_planes\": cutting_planes This use case can be run using the multi-scale optimization without the --use_single_scale tag as follows. $python RunUseCase.py --use_case femur_cut The use_shape_statistics_after parameter is used to trigger the multi-scale optimization mode. \"use_shape_statistics_after\": 64","title":"Optimizing Shape Model"},{"location":"use-cases/femur-cutting-planes.html#analyzing-shape-model","text":"ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? . Here is the mean shape, individual samples, and modes of shape variations of the optimized shape mode using single-scale optimization.","title":"Analyzing Shape Model"},{"location":"use-cases/femur-mesh.html","text":"Femur-Mesh: Shape Model directly from Mesh What and Where is the Use Case? This use case demonstrates using ShapeWorks tools to perform the following. Build a shape model where shapes are given as triangular surface meshes Groom a dataset from meshes, and obtain a SSM without converting the meshes to a different representation. This requires significantly lower RAM. The use case is located at: Examples/Python/femur_mesh.py Running the Use Case To run the use case, run RunUseCase.py (in Examples/Python/ ). $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case femur_mesh This calls femur_mesh.py (in Examples/Python/ ) to perform the following. Loads the femur dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., the optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). Grooming Data In this use case, we download pre-groomed data. The grooming process used for this data can be explored in Libs/Mesh/GroomFemur.cpp . Clip : Because the femur meshes vary in shaft lengths, we clip the lengths of all the femurs to that of the shortest one. Reflect Meshes : In this use case, we often have both right and left femur surface meshes. To align all the femurs, we choose one side to reflect both the image and mesh. Scale : ShapeWorks is not size invariant and requires a sufficient scale. Centering : This factors out translations to reduce the risk of misalignment. Alignment + Cropping : This process is to further ensure that all the shafts are cropped at the same cutting plane. Optimizing Shape Model Below are the default optimization parameters when running this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . $ python RunUseCase.py --use_case femur_mesh { \"number_of_particles\" : 1024 , \"use_shape_statistics_after\" : 0 , \"use_normals\" : 0 , \"normal_weight\" : 0.0 , \"checkpointing_interval\" : 10000 , \"keep_checkpoints\" : 0 , \"iterations_per_split\" : 4000 , \"optimization_iterations\" : 500 , \"starting_regularization\" : 10 , \"ending_regularization\" : 1 , \"recompute_regularization_interval\" : 1 , \"domains_per_shape\" : 1 , \"domain_type\" : 'mesh' , \"relative_weighting\" : 10 , \"initial_relative_weighting\" : 1 , \"procrustes_interval\" : 1 , \"procrustes_scaling\" : 1 , \"save_init_splits\" : 0 , \"verbosity\" : 2 , } Analyzing Shape Model ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? . Here is the mean shape of the optimized shape mode using single-scale optimization. Here are femur samples with their optimized correspondences. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the femur dataset using single-scale optimization.","title":"Femur Mesh: SSM directly from meshes"},{"location":"use-cases/femur-mesh.html#femur-mesh-shape-model-directly-from-mesh","text":"","title":"Femur-Mesh: Shape Model directly from Mesh"},{"location":"use-cases/femur-mesh.html#what-and-where-is-the-use-case","text":"This use case demonstrates using ShapeWorks tools to perform the following. Build a shape model where shapes are given as triangular surface meshes Groom a dataset from meshes, and obtain a SSM without converting the meshes to a different representation. This requires significantly lower RAM. The use case is located at: Examples/Python/femur_mesh.py","title":"What and Where is the Use Case?"},{"location":"use-cases/femur-mesh.html#running-the-use-case","text":"To run the use case, run RunUseCase.py (in Examples/Python/ ). $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case femur_mesh This calls femur_mesh.py (in Examples/Python/ ) to perform the following. Loads the femur dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., the optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ).","title":"Running the Use Case"},{"location":"use-cases/femur-mesh.html#grooming-data","text":"In this use case, we download pre-groomed data. The grooming process used for this data can be explored in Libs/Mesh/GroomFemur.cpp . Clip : Because the femur meshes vary in shaft lengths, we clip the lengths of all the femurs to that of the shortest one. Reflect Meshes : In this use case, we often have both right and left femur surface meshes. To align all the femurs, we choose one side to reflect both the image and mesh. Scale : ShapeWorks is not size invariant and requires a sufficient scale. Centering : This factors out translations to reduce the risk of misalignment. Alignment + Cropping : This process is to further ensure that all the shafts are cropped at the same cutting plane.","title":"Grooming Data"},{"location":"use-cases/femur-mesh.html#optimizing-shape-model","text":"Below are the default optimization parameters when running this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . $ python RunUseCase.py --use_case femur_mesh { \"number_of_particles\" : 1024 , \"use_shape_statistics_after\" : 0 , \"use_normals\" : 0 , \"normal_weight\" : 0.0 , \"checkpointing_interval\" : 10000 , \"keep_checkpoints\" : 0 , \"iterations_per_split\" : 4000 , \"optimization_iterations\" : 500 , \"starting_regularization\" : 10 , \"ending_regularization\" : 1 , \"recompute_regularization_interval\" : 1 , \"domains_per_shape\" : 1 , \"domain_type\" : 'mesh' , \"relative_weighting\" : 10 , \"initial_relative_weighting\" : 1 , \"procrustes_interval\" : 1 , \"procrustes_scaling\" : 1 , \"save_init_splits\" : 0 , \"verbosity\" : 2 , }","title":"Optimizing Shape Model"},{"location":"use-cases/femur-mesh.html#analyzing-shape-model","text":"ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? . Here is the mean shape of the optimized shape mode using single-scale optimization. Here are femur samples with their optimized correspondences. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the femur dataset using single-scale optimization.","title":"Analyzing Shape Model"},{"location":"use-cases/femur.html","text":"Femur: Shape Model from Meshes What and Where is the Use Case? This use case demonstrates using ShapeWorks tools to perform the following. Build a shape model where shapes are given as triangular surface meshes Groom a dataset that contains both shapes (meshes) and their corresponding imaging data (e.g., CT) Interactive cutting plane selection for data grooming The femur meshes in this data set have been segmented with various shaft lengths, as can be seen below. To remove this variability so that it is not captured in the shape model, the femurs are clipped using a cutting plane. The use case has a pre-defined cutting plane, but you can choose to overwrite it and define the cutting plane interactively by running the use case with the --interactive tag. There are two ways to define the cutting plane interactively, as explained below. The use case is located at: Examples/Python/femur.py Running the Use Case Important Minimum of 32GB of RAM required to run the full use case. To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_prepped_data : to run the optimization on previously processed/groomed data. --interactive : This tag is used to allow you to interactively select the cutting plane to be used for clipping all meshes. If this tag is not used, the pre-defined cutting plane is used to clip the given meshes. To run the full pipeline with multi-scale and the pre-defined cutting plane: $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case femur This calls femur.py (in Examples/Python/ ) to perform the following. Loads the femur dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the images and meshes by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., the optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case femur --start_with_prepped_data To groom both the meshes and corresponding images, use -start_with_image_and_segmentation_data tag. The image origin, size, and spacing will be used in mesh rasterization. $ python RunUseCase.py --use_case femur --start_with_image_and_segmentation_data If this tag is not used, grooming will be done on meshes only. The origin and size will be inferred from the meshes, and isotropic spacing will be used unless the user specifies otherwise for rasterization. Running with Interactivity To run the use case interactively, run: $ python RunUseCase.py --use_case femur --interactive In interactive mode, you are presented with two options to choose between for defining the cutting plane interactively. Option 1 : The first option is to select the cutting plane at the beginning of the grooming steps on a sample of your choice. You will be prompted with: Type the prefix of the sample you wish to use to select the cutting plane from listed options and press enter. Then the options are listed. After you have typed in the chosen sample prefix, an interactive window will pop up in which you can select the cutting plane. When you are content with your selection, you simply close this window, and the grooming process will continue. This process can be seen below. Note that internally, whatever transformations are applied to the sample, you have defined the cutting plane on will be done to the cutting plane as well, so that when it is time to clip the samples, the cutting plane is still well-defined. Option 2 : If option 2 is selected, you will be asked to select a cutting plane for the femur shaft in the middle of the grooming process. Once the reference sample for alignment has been selected, an interactive window will pop up with the reference sample to define the cutting plane. Closing the window will continue the grooming process. Grooming Data We start with full unsegmented images (CT scans) of each femur's hip and segmented meshes in this use case. ShapeWorks needs a volumetric representation of shapes in the form of signed distance transforms to optimize the shape model. Hence, given surface meshes are first converted to image-based representation (i.e., binary segmentations). Additionally, the corresponding unsegmented images need to be carried through each grooming step with the meshes to be used for analysis. The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. Each step's results are shown for the meshes (note every step is performed on both the meshes the images, although the resulting images are not shown here). For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Cutting Plane : If interactive, define cutting plane either in the beginning or after rigid alignment using the interactive window. Within this window: - Zoom in and out by scrolling - Rotate viewpoint by clicking in the surrounding area - Move the cutting plane by clicking on it and dragging - Change normal vector (represented with arrow) by clicking on it and dragging Reflect Meshes : In this use case, we often have both right and left femur surface meshes. To align all the femurs, we choose one side to reflect both the image and mesh. Meshes to Volumes : Meshes must be turned into binary volumes using rasterization. The corresponding image origin, size, and spacing are used to generate the volume. Isotropic Resampling : Both the image and rasterized segmentations are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also produce segmentations with smaller voxel spacing, thereby reducing aliasing artifacts (i.e., staircase/jagged surface) due to binarization. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center of Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations and images are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. The alignment parameters are computed based on aligning segmentations and then applied to their corresponding images. The samples must be centered before they are aligned to a reference. This step can be performed with Isotropic Resampling as it is in the left atrium use case. In the Femur use case, we do so separately so that we can get the translation and apply it to the cutting plane if it has already been selected. Clip Segmentations : Because the femur segmentations vary in shaft lengths, we use the defined cutting plane to clip them, so only the region of interest remains. Cropping : The images and segmentations are cropped so that all of the samples are within the same bounding box. The bounding box parameters are computed based on the biggest bounding box that encapsulates all the given dataset segmentations. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase. Optimizing Shape Model Below are the default optimization parameters when running this use case using the --use_single_scale tag. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . Note that use_shape_statistics_after parameter is not used when --use_single_scale tag is given to the RunUseCase.py (in Examples/Python/ ). $python RunUseCase.py --use_case femur --use_single_scale \"number_of_particles\" : 1024, \"use_normals\": 0, \"normal_weight\": 10.0, \"checkpointing_interval\" : 200, \"keep_checkpoints\" : 1, \"iterations_per_split\" : 4000, \"optimization_iterations\" : 4000, \"starting_regularization\" : 100, \"ending_regularization\" : 0.1, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"domain_type\" : 'image', \"relative_weighting\" : 10, \"initial_relative_weighting\" : 1, \"procrustes_interval\" : 1, \"procrustes_scaling\" : 1, \"save_init_splits\" : 1, \"verbosity\" : 2, \"use_statistics_in_init\" : 0 This use case can be run using the multi-scale optimization without the --use_single_scale tag as follows. $python RunUseCase.py --use_case femur The use_shape_statistics_after parameter is used to trigger the multi-scale optimization mode. \"use_shape_statistics_after\": 64 Analyzing Shape Model ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? . Here is the mean shape of the optimized shape mode using single-scale optimization. Here are femur samples with their optimized correspondences. Zooming in some femur samples. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the femur dataset using single-scale optimization.","title":"Femur: SSM from Meshes"},{"location":"use-cases/femur.html#femur-shape-model-from-meshes","text":"","title":"Femur: Shape Model from Meshes"},{"location":"use-cases/femur.html#what-and-where-is-the-use-case","text":"This use case demonstrates using ShapeWorks tools to perform the following. Build a shape model where shapes are given as triangular surface meshes Groom a dataset that contains both shapes (meshes) and their corresponding imaging data (e.g., CT) Interactive cutting plane selection for data grooming The femur meshes in this data set have been segmented with various shaft lengths, as can be seen below. To remove this variability so that it is not captured in the shape model, the femurs are clipped using a cutting plane. The use case has a pre-defined cutting plane, but you can choose to overwrite it and define the cutting plane interactively by running the use case with the --interactive tag. There are two ways to define the cutting plane interactively, as explained below. The use case is located at: Examples/Python/femur.py","title":"What and Where is the Use Case?"},{"location":"use-cases/femur.html#running-the-use-case","text":"Important Minimum of 32GB of RAM required to run the full use case. To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_prepped_data : to run the optimization on previously processed/groomed data. --interactive : This tag is used to allow you to interactively select the cutting plane to be used for clipping all meshes. If this tag is not used, the pre-defined cutting plane is used to clip the given meshes. To run the full pipeline with multi-scale and the pre-defined cutting plane: $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case femur This calls femur.py (in Examples/Python/ ) to perform the following. Loads the femur dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the images and meshes by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., the optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case femur --start_with_prepped_data To groom both the meshes and corresponding images, use -start_with_image_and_segmentation_data tag. The image origin, size, and spacing will be used in mesh rasterization. $ python RunUseCase.py --use_case femur --start_with_image_and_segmentation_data If this tag is not used, grooming will be done on meshes only. The origin and size will be inferred from the meshes, and isotropic spacing will be used unless the user specifies otherwise for rasterization.","title":"Running the Use Case"},{"location":"use-cases/femur.html#running-with-interactivity","text":"To run the use case interactively, run: $ python RunUseCase.py --use_case femur --interactive In interactive mode, you are presented with two options to choose between for defining the cutting plane interactively. Option 1 : The first option is to select the cutting plane at the beginning of the grooming steps on a sample of your choice. You will be prompted with: Type the prefix of the sample you wish to use to select the cutting plane from listed options and press enter. Then the options are listed. After you have typed in the chosen sample prefix, an interactive window will pop up in which you can select the cutting plane. When you are content with your selection, you simply close this window, and the grooming process will continue. This process can be seen below. Note that internally, whatever transformations are applied to the sample, you have defined the cutting plane on will be done to the cutting plane as well, so that when it is time to clip the samples, the cutting plane is still well-defined. Option 2 : If option 2 is selected, you will be asked to select a cutting plane for the femur shaft in the middle of the grooming process. Once the reference sample for alignment has been selected, an interactive window will pop up with the reference sample to define the cutting plane. Closing the window will continue the grooming process.","title":"Running with Interactivity"},{"location":"use-cases/femur.html#grooming-data","text":"We start with full unsegmented images (CT scans) of each femur's hip and segmented meshes in this use case. ShapeWorks needs a volumetric representation of shapes in the form of signed distance transforms to optimize the shape model. Hence, given surface meshes are first converted to image-based representation (i.e., binary segmentations). Additionally, the corresponding unsegmented images need to be carried through each grooming step with the meshes to be used for analysis. The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. Each step's results are shown for the meshes (note every step is performed on both the meshes the images, although the resulting images are not shown here). For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Cutting Plane : If interactive, define cutting plane either in the beginning or after rigid alignment using the interactive window. Within this window: - Zoom in and out by scrolling - Rotate viewpoint by clicking in the surrounding area - Move the cutting plane by clicking on it and dragging - Change normal vector (represented with arrow) by clicking on it and dragging Reflect Meshes : In this use case, we often have both right and left femur surface meshes. To align all the femurs, we choose one side to reflect both the image and mesh. Meshes to Volumes : Meshes must be turned into binary volumes using rasterization. The corresponding image origin, size, and spacing are used to generate the volume. Isotropic Resampling : Both the image and rasterized segmentations are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also produce segmentations with smaller voxel spacing, thereby reducing aliasing artifacts (i.e., staircase/jagged surface) due to binarization. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center of Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations and images are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. The alignment parameters are computed based on aligning segmentations and then applied to their corresponding images. The samples must be centered before they are aligned to a reference. This step can be performed with Isotropic Resampling as it is in the left atrium use case. In the Femur use case, we do so separately so that we can get the translation and apply it to the cutting plane if it has already been selected. Clip Segmentations : Because the femur segmentations vary in shaft lengths, we use the defined cutting plane to clip them, so only the region of interest remains. Cropping : The images and segmentations are cropped so that all of the samples are within the same bounding box. The bounding box parameters are computed based on the biggest bounding box that encapsulates all the given dataset segmentations. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase.","title":"Grooming Data"},{"location":"use-cases/femur.html#optimizing-shape-model","text":"Below are the default optimization parameters when running this use case using the --use_single_scale tag. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . Note that use_shape_statistics_after parameter is not used when --use_single_scale tag is given to the RunUseCase.py (in Examples/Python/ ). $python RunUseCase.py --use_case femur --use_single_scale \"number_of_particles\" : 1024, \"use_normals\": 0, \"normal_weight\": 10.0, \"checkpointing_interval\" : 200, \"keep_checkpoints\" : 1, \"iterations_per_split\" : 4000, \"optimization_iterations\" : 4000, \"starting_regularization\" : 100, \"ending_regularization\" : 0.1, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"domain_type\" : 'image', \"relative_weighting\" : 10, \"initial_relative_weighting\" : 1, \"procrustes_interval\" : 1, \"procrustes_scaling\" : 1, \"save_init_splits\" : 1, \"verbosity\" : 2, \"use_statistics_in_init\" : 0 This use case can be run using the multi-scale optimization without the --use_single_scale tag as follows. $python RunUseCase.py --use_case femur The use_shape_statistics_after parameter is used to trigger the multi-scale optimization mode. \"use_shape_statistics_after\": 64","title":"Optimizing Shape Model"},{"location":"use-cases/femur.html#analyzing-shape-model","text":"ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? . Here is the mean shape of the optimized shape mode using single-scale optimization. Here are femur samples with their optimized correspondences. Zooming in some femur samples. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the femur dataset using single-scale optimization.","title":"Analyzing Shape Model"},{"location":"use-cases/fixed-domain-ellipsoid.html","text":"Fixed Domains for Ellipsoid: Correspondences on New Shape What and Where is the Use Case? This use case is designed to demonstrate the functionality of the fixed domain of ShapeWorks. The fixed domains are used for the cases where we need to place correspondences on new shapes using a pre-existing shape model. In this example, we use the example dataset in ellipsoid_fd.zip , which contains a previously generated shape model on ellipsoids and prepped segmentations of two new ellipsoids. The use case is located at: Examples/Python/ellipsoid_fd.py Running the Use Case To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. Run the following command to run this use case. This optimization is done only with single-scale support since the new shapes will have the same number of particles as the existing shape model. Hence, there is no need to use the --use_single_scale tag. $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case ellipsoid_fd This calls ellipsoid_fd.py (in Examples/Python/ ) to perform the following. Loads the ellipsoid_fd dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Loads the existing (pre-trained) shape model and converts the new segmentations into signed distance transforms. Optimizes particle distribution (i.e., the shape/correspondence model) on the new shape samples by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., optimized shape model on the new shapes and the pre-trained shape model) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using a previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case ellipsoid_fd --start_with_prepped_data Grooming Data This use case assumes that the new samples are already aligned with the existing shape model. The only grooming step is computing the signed distance transform for each new segmentation. Optimizing Shape Model Below are the default optimization parameters for this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . \"number_of_particles\" : 128, \"use_normals\": 0, \"normal_weight\": 10.0, \"checkpointing_interval\" : 200, \"keep_checkpoints\" : 0, \"iterations_per_split\" : 100, \"optimization_iterations\" : 2000, \"starting_regularization\" : 100, \"ending_regularization\" : 0.1, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"domain_type\" : 'image', \"relative_weighting\" : 10, \"initial_relative_weighting\" : 0.01, \"procrustes_interval\" : 0, \"procrustes_scaling\" : 0, \"save_init_splits\" : 0, \"verbosity\" : 3, \"number_fixed_domains\": len(fileListDT), \"fixed_domain_model_dir\": shapemodelDir, \"mean_shape_path\": meanShapePath,` In ellipsoid_fd.py (in Examples/Python/ ), the following is defined. fileListDT is the list of distance transforms for the existing shape model shapemodelDir is the directory path to the new shape model meanShapePath is the path to the mean (average) shape particles to be used for initializing correspondences on the new shape Analyzing Shape Model ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? .","title":"Fixed Domains: SSM on New Shapes"},{"location":"use-cases/fixed-domain-ellipsoid.html#fixed-domains-for-ellipsoid-correspondences-on-new-shape","text":"","title":"Fixed Domains for Ellipsoid: Correspondences on New Shape"},{"location":"use-cases/fixed-domain-ellipsoid.html#what-and-where-is-the-use-case","text":"This use case is designed to demonstrate the functionality of the fixed domain of ShapeWorks. The fixed domains are used for the cases where we need to place correspondences on new shapes using a pre-existing shape model. In this example, we use the example dataset in ellipsoid_fd.zip , which contains a previously generated shape model on ellipsoids and prepped segmentations of two new ellipsoids. The use case is located at: Examples/Python/ellipsoid_fd.py","title":"What and Where is the Use Case?"},{"location":"use-cases/fixed-domain-ellipsoid.html#running-the-use-case","text":"To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. Run the following command to run this use case. This optimization is done only with single-scale support since the new shapes will have the same number of particles as the existing shape model. Hence, there is no need to use the --use_single_scale tag. $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case ellipsoid_fd This calls ellipsoid_fd.py (in Examples/Python/ ) to perform the following. Loads the ellipsoid_fd dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Loads the existing (pre-trained) shape model and converts the new segmentations into signed distance transforms. Optimizes particle distribution (i.e., the shape/correspondence model) on the new shape samples by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., optimized shape model on the new shapes and the pre-trained shape model) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using a previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case ellipsoid_fd --start_with_prepped_data","title":"Running the Use Case"},{"location":"use-cases/fixed-domain-ellipsoid.html#grooming-data","text":"This use case assumes that the new samples are already aligned with the existing shape model. The only grooming step is computing the signed distance transform for each new segmentation.","title":"Grooming Data"},{"location":"use-cases/fixed-domain-ellipsoid.html#optimizing-shape-model","text":"Below are the default optimization parameters for this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . \"number_of_particles\" : 128, \"use_normals\": 0, \"normal_weight\": 10.0, \"checkpointing_interval\" : 200, \"keep_checkpoints\" : 0, \"iterations_per_split\" : 100, \"optimization_iterations\" : 2000, \"starting_regularization\" : 100, \"ending_regularization\" : 0.1, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"domain_type\" : 'image', \"relative_weighting\" : 10, \"initial_relative_weighting\" : 0.01, \"procrustes_interval\" : 0, \"procrustes_scaling\" : 0, \"save_init_splits\" : 0, \"verbosity\" : 3, \"number_fixed_domains\": len(fileListDT), \"fixed_domain_model_dir\": shapemodelDir, \"mean_shape_path\": meanShapePath,` In ellipsoid_fd.py (in Examples/Python/ ), the following is defined. fileListDT is the list of distance transforms for the existing shape model shapemodelDir is the directory path to the new shape model meanShapePath is the path to the mean (average) shape particles to be used for initializing correspondences on the new shape","title":"Optimizing Shape Model"},{"location":"use-cases/fixed-domain-ellipsoid.html#analyzing-shape-model","text":"ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? .","title":"Analyzing Shape Model"},{"location":"use-cases/left-atrium.html","text":"Left Atrium: Shape Model from Segmentations What and Where is the Use Case? This use case demonstrates using ShapeWorks tools to perform the following. Build a shape model where shapes are given as binary segmentations Groom a dataset that contains both shapes (segmentations) and their corresponding imaging data (e.g., MRI) Showcase both single-scale and multi-scale optimization for particles optimization For this use case, we have 58 MRI images and their corresponding binary segmentations of the left atrium (visit MIDAS-NAMIC for more details of the left atrium dataset). The use case is located at: Examples/Python/left_atrium.py Running the Use Case Important Minimum of 32GB of RAM required to run the full use case. To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_image_and_segmentation_data : to groom both raw images and segmentations using the segmentations to find the grooming parameters (e.g., bounding box, reference shape for alignment). The default is grooming only segmentations --use_single_scale : to use the single-scale optimization. Default is multi-scale optimization --start_with_prepped_data : to run the optimization on previously processed/groomed data To run the full pipeline with multi-scale: $ cd /path/to/shapeworks/Examples/Python $python RunUseCase.py --use_case left_atrium This calls left_atrium.py (in Examples/Python/ ) to perform the following. Loads the left atrium dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the images and segmentations by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using a previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case left_atrium --start_with_prepped_data Grooming Data The grooming stage entails rigid transformations to align samples for groupwise modeling and analysis. It also changes the origin, voxel size, image size, orientation, etc. of the shape data. Imaging data (CT/MRI) soon becomes out of alignment and cannot be tied to the resulting shape models. Although we do not need the raw images for the shape model optimization, carrying over images. However, the grooming stage enables streamlined construction of a correspondence model of surface geometry that simultaneously map imaging data and physical properties (e.g., functional measurements, cortical thickness, cartilage thickness, etc.) to each correspondence point. ShapeWorks image-based grooming tools and associated python scripts are developed to carry volumetric data through each grooming step with the shapes (meshes or segmentations) to be used for subsequent analysis visualization. In each step of grooming, we use the segmentation files to find the grooming parameters such as finding the reference shape for alignment or the bounding box for cropping. We save them in a TXT file and use the same set of parameters to groom the raw images. The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Isotropic Resampling : Both binary segmentations in left_atrium/segmentations/ and their corresponding images in left_atrium/images/ are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also be used to produce images and segmentations with smaller voxel spacing, and thereby reduce aliasing artifacts (i.e., staircase/jagged surface) due to binarization for segmentations. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations and images are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center-of-Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations and images are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. The alignment parameters are computed based on aligning segmentations and then applied to their corresponding images. Cropping : The images and segmentations are cropped so that all of the samples are within the same bounding box. The bounding box parameters are computed based on the biggest bounding box that encapsulate all the segmentations of the given dataset. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase. At the end of the grooming stage, both segmentations, which are turned into distance transforms, and corresponding images data, are groomed and ready for the optimize stage. Optimizing Shape Model For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . The shape model can be optimized using either a single-scale or a multi-scale optimization. In both cases, particles on each shape sample are initialized using the particle splitting strategy starting from a single particle (or a given set of landmarks) until reaching the required number of particles (or the largest power of two number of particles that is closest to the required number of particles). The optimized particles at each scale are used to initialize the next scale. At each scale, particles undergo initialization and optimization stages. The multi-scale triggers both the initialization and optimization stages. The single-scale mode, on the other hand, uses the initialization stage at each scale and runs the optimization stage when the required number of particles is reached (i.e., at the last scale). The default mode is multi-scale; to run the single-scale optimization, use the --use_single_scale tag. The differences between initialization and optimization stages are: How important the correspondence (inter-surface) objective is compared to the surface sampling (intra-surface) term using a relative weighting parameter. Hence, the initial_relative_weighting parameter is the weight used in the initialization stage, and the relative_weighting is the weight used for the optimization stage. How the notion of correspondence (inter-surface) is quantified. In initialization, especially when we do not have enough particles to describe the geometry of each surface, we use mean energy (i.e., pushing all shapes in the shape space to the mean shape or, in other words, the covariance matrix is assumed to be identity). In optimization, we use the entropy of the distribution of the shapes (assuming Gaussian-distributed shapes), which is quantified based on the covariance matrix. In the multi-scale setting, we have the option to use shape statistics at later scales using the use_shape_statistics_after parameter, where we have more particles that can reveal the covariance structure. Single-Scale Optimization Below are the default optimization parameters when running this use case using the --use_single_scale tag. $python RunUseCase.py --use_case left_atrium --use_single_scale \"number_of_particles\" : 512, \"use_normals\": 0, \"normal_weight\": 10.0, \"checkpointing_interval\" : 200, \"keep_checkpoints\" : 0, \"iterations_per_split\" : 4000, \"optimization_iterations\" : 4000, \"starting_regularization\" : 1000, \"ending_regularization\" : 10, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"relative_weighting\" : 10, \"initial_relative_weighting\" : 0.1, \"procrustes_interval\" : 1, \"procrustes_scaling\" : 1, \"save_init_splits\" : 0, \"verbosity\" : 3 Multi-Scale Optimization This use case can be run using the multi-scale optimization without the --use_single_scale tag as follows. $python RunUseCase.py --use_case left_atrium The use_shape_statistics_after parameter is used to trigger the multi-scale optimization mode. \"use_shape_statistics_after\": 128 Analyzing Shape Model ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? . Single-Scale Optimization Here is the mean shape of the optimized shape mode using single-scale optimization. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the left atrium dataset using single-scale optimization. Multi-Scale Optimization Here is the mean shape of the optimized shape mode using multi-scale optimization. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the left atrium dataset using multi-scale optimization.","title":"Left Atrium: SSM from Segmentations"},{"location":"use-cases/left-atrium.html#left-atrium-shape-model-from-segmentations","text":"","title":"Left Atrium: Shape Model from Segmentations"},{"location":"use-cases/left-atrium.html#what-and-where-is-the-use-case","text":"This use case demonstrates using ShapeWorks tools to perform the following. Build a shape model where shapes are given as binary segmentations Groom a dataset that contains both shapes (segmentations) and their corresponding imaging data (e.g., MRI) Showcase both single-scale and multi-scale optimization for particles optimization For this use case, we have 58 MRI images and their corresponding binary segmentations of the left atrium (visit MIDAS-NAMIC for more details of the left atrium dataset). The use case is located at: Examples/Python/left_atrium.py","title":"What and Where is the Use Case?"},{"location":"use-cases/left-atrium.html#running-the-use-case","text":"Important Minimum of 32GB of RAM required to run the full use case. To run the use case, run RunUseCase.py (in Examples/Python/ ) with proper tags. The tags control the type of input data and the optimization method. See Getting Started with Use Cases for the full list of tags. --start_with_image_and_segmentation_data : to groom both raw images and segmentations using the segmentations to find the grooming parameters (e.g., bounding box, reference shape for alignment). The default is grooming only segmentations --use_single_scale : to use the single-scale optimization. Default is multi-scale optimization --start_with_prepped_data : to run the optimization on previously processed/groomed data To run the full pipeline with multi-scale: $ cd /path/to/shapeworks/Examples/Python $python RunUseCase.py --use_case left_atrium This calls left_atrium.py (in Examples/Python/ ) to perform the following. Loads the left atrium dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Grooms the images and segmentations by calling data preprocessing functions in GroomUtils.py (in Examples/Python/ ). See Grooming Data for details about these preprocessing steps. Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). If you wish to start with the optimization step using a previously groomed data, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case left_atrium --start_with_prepped_data","title":"Running the Use Case"},{"location":"use-cases/left-atrium.html#grooming-data","text":"The grooming stage entails rigid transformations to align samples for groupwise modeling and analysis. It also changes the origin, voxel size, image size, orientation, etc. of the shape data. Imaging data (CT/MRI) soon becomes out of alignment and cannot be tied to the resulting shape models. Although we do not need the raw images for the shape model optimization, carrying over images. However, the grooming stage enables streamlined construction of a correspondence model of surface geometry that simultaneously map imaging data and physical properties (e.g., functional measurements, cortical thickness, cartilage thickness, etc.) to each correspondence point. ShapeWorks image-based grooming tools and associated python scripts are developed to carry volumetric data through each grooming step with the shapes (meshes or segmentations) to be used for subsequent analysis visualization. In each step of grooming, we use the segmentation files to find the grooming parameters such as finding the reference shape for alignment or the bounding box for cropping. We save them in a TXT file and use the same set of parameters to groom the raw images. The following preprocessing steps are only performed when you start with unprepped data, i.e., the tag --start_with_prepped_data is not used. For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Isotropic Resampling : Both binary segmentations in left_atrium/segmentations/ and their corresponding images in left_atrium/images/ are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also be used to produce images and segmentations with smaller voxel spacing, and thereby reduce aliasing artifacts (i.e., staircase/jagged surface) due to binarization for segmentations. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations and images are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center-of-Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations and images are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. The alignment parameters are computed based on aligning segmentations and then applied to their corresponding images. Cropping : The images and segmentations are cropped so that all of the samples are within the same bounding box. The bounding box parameters are computed based on the biggest bounding box that encapsulate all the segmentations of the given dataset. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase. At the end of the grooming stage, both segmentations, which are turned into distance transforms, and corresponding images data, are groomed and ready for the optimize stage.","title":"Grooming Data"},{"location":"use-cases/left-atrium.html#optimizing-shape-model","text":"For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . The shape model can be optimized using either a single-scale or a multi-scale optimization. In both cases, particles on each shape sample are initialized using the particle splitting strategy starting from a single particle (or a given set of landmarks) until reaching the required number of particles (or the largest power of two number of particles that is closest to the required number of particles). The optimized particles at each scale are used to initialize the next scale. At each scale, particles undergo initialization and optimization stages. The multi-scale triggers both the initialization and optimization stages. The single-scale mode, on the other hand, uses the initialization stage at each scale and runs the optimization stage when the required number of particles is reached (i.e., at the last scale). The default mode is multi-scale; to run the single-scale optimization, use the --use_single_scale tag. The differences between initialization and optimization stages are: How important the correspondence (inter-surface) objective is compared to the surface sampling (intra-surface) term using a relative weighting parameter. Hence, the initial_relative_weighting parameter is the weight used in the initialization stage, and the relative_weighting is the weight used for the optimization stage. How the notion of correspondence (inter-surface) is quantified. In initialization, especially when we do not have enough particles to describe the geometry of each surface, we use mean energy (i.e., pushing all shapes in the shape space to the mean shape or, in other words, the covariance matrix is assumed to be identity). In optimization, we use the entropy of the distribution of the shapes (assuming Gaussian-distributed shapes), which is quantified based on the covariance matrix. In the multi-scale setting, we have the option to use shape statistics at later scales using the use_shape_statistics_after parameter, where we have more particles that can reveal the covariance structure.","title":"Optimizing Shape Model"},{"location":"use-cases/left-atrium.html#single-scale-optimization","text":"Below are the default optimization parameters when running this use case using the --use_single_scale tag. $python RunUseCase.py --use_case left_atrium --use_single_scale \"number_of_particles\" : 512, \"use_normals\": 0, \"normal_weight\": 10.0, \"checkpointing_interval\" : 200, \"keep_checkpoints\" : 0, \"iterations_per_split\" : 4000, \"optimization_iterations\" : 4000, \"starting_regularization\" : 1000, \"ending_regularization\" : 10, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"relative_weighting\" : 10, \"initial_relative_weighting\" : 0.1, \"procrustes_interval\" : 1, \"procrustes_scaling\" : 1, \"save_init_splits\" : 0, \"verbosity\" : 3","title":"Single-Scale Optimization"},{"location":"use-cases/left-atrium.html#multi-scale-optimization","text":"This use case can be run using the multi-scale optimization without the --use_single_scale tag as follows. $python RunUseCase.py --use_case left_atrium The use_shape_statistics_after parameter is used to trigger the multi-scale optimization mode. \"use_shape_statistics_after\": 128","title":"Multi-Scale Optimization"},{"location":"use-cases/left-atrium.html#analyzing-shape-model","text":"ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? .","title":"Analyzing Shape Model"},{"location":"use-cases/left-atrium.html#single-scale-optimization_1","text":"Here is the mean shape of the optimized shape mode using single-scale optimization. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the left atrium dataset using single-scale optimization.","title":"Single-Scale Optimization"},{"location":"use-cases/left-atrium.html#multi-scale-optimization_1","text":"Here is the mean shape of the optimized shape mode using multi-scale optimization. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the left atrium dataset using multi-scale optimization.","title":"Multi-Scale Optimization"},{"location":"use-cases/lumps.html","text":"Lumps: Shape Model directly from Mesh What and Where is the Use Case? This use case demonstrates a minimal example to run ShapeWorks directly on a mesh using a synthetic dataset. The use case is located at: Examples/Python/lumps.py Running the Use Case To run the use case, run RunUseCase.py (in Examples/Python/ ). $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case lumps This calls lumps.py (in Examples/Python/ ) to perform the following. Loads the lumps dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., the optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ). Grooming Data This is a synthetic dataset that does not require grooming. Optimizing Shape Model Below are the default optimization parameters when running this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . $ python RunUseCase.py --use_case lumps { \"number_of_particles\" : 512 , \"use_normals\" : 0 , \"normal_weight\" : 10.0 , \"checkpointing_interval\" : 100 , \"keep_checkpoints\" : 0 , \"iterations_per_split\" : 2000 , \"optimization_iterations\" : 500 , \"starting_regularization\" : 10 , \"ending_regularization\" : 1 , \"recompute_regularization_interval\" : 1 , \"domains_per_shape\" : 1 , \"domain_type\" : \"mesh\" , \"relative_weighting\" : 10 , \"initial_relative_weighting\" : 1 , \"procrustes_interval\" : 0 , \"procrustes_scaling\" : 0 , \"save_init_splits\" : 0 , \"verbosity\" : 1 } Analyzing Shape Model ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? . Here is the mean shape of the optimized shape mode using single-scale optimization. Note the two tiny lumps at the top, and towards the right. Here are lumps samples with their optimized correspondences. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the lumps dataset using single-scale optimization.","title":"Lumps: SSM directly from meshes"},{"location":"use-cases/lumps.html#lumps-shape-model-directly-from-mesh","text":"","title":"Lumps: Shape Model directly from Mesh"},{"location":"use-cases/lumps.html#what-and-where-is-the-use-case","text":"This use case demonstrates a minimal example to run ShapeWorks directly on a mesh using a synthetic dataset. The use case is located at: Examples/Python/lumps.py","title":"What and Where is the Use Case?"},{"location":"use-cases/lumps.html#running-the-use-case","text":"To run the use case, run RunUseCase.py (in Examples/Python/ ). $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case lumps This calls lumps.py (in Examples/Python/ ) to perform the following. Loads the lumps dataset using a local version if it exists (i.e., previously downloaded); otherwise, the dataset is automatically downloaded from the ShapeWorks Data Portal . Optimizes particle distribution (i.e., the shape/correspondence model) by calling optimization functions in OptimizeUtils.py (in Examples/Python/ ). See Optimizing Shape Model for details about algorithmic parameters for optimizing the shape model. Launches ShapeWorksStudio to visualize the use case results (i.e., the optimized shape model and the groomed data) by calling functions in AnalyzeUtils.py (in Examples/Python/ ).","title":"Running the Use Case"},{"location":"use-cases/lumps.html#grooming-data","text":"This is a synthetic dataset that does not require grooming.","title":"Grooming Data"},{"location":"use-cases/lumps.html#optimizing-shape-model","text":"Below are the default optimization parameters when running this use case. For a description of the optimize tool and its algorithmic parameters, see: How to Optimize Your Shape Model . $ python RunUseCase.py --use_case lumps { \"number_of_particles\" : 512 , \"use_normals\" : 0 , \"normal_weight\" : 10.0 , \"checkpointing_interval\" : 100 , \"keep_checkpoints\" : 0 , \"iterations_per_split\" : 2000 , \"optimization_iterations\" : 500 , \"starting_regularization\" : 10 , \"ending_regularization\" : 1 , \"recompute_regularization_interval\" : 1 , \"domains_per_shape\" : 1 , \"domain_type\" : \"mesh\" , \"relative_weighting\" : 10 , \"initial_relative_weighting\" : 1 , \"procrustes_interval\" : 0 , \"procrustes_scaling\" : 0 , \"save_init_splits\" : 0 , \"verbosity\" : 1 }","title":"Optimizing Shape Model"},{"location":"use-cases/lumps.html#analyzing-shape-model","text":"ShapeWorksStudio visualizes/analyzes the optimized particle-based shape model by visualizing the mean shape, individual shape samples, and the shape modes of variations. For more information, see: How to Analyze Your Shape Model? . Here is the mean shape of the optimized shape mode using single-scale optimization. Note the two tiny lumps at the top, and towards the right. Here are lumps samples with their optimized correspondences. Here is a video showing the shape modes of variation (computed using principal component analysis - PCA) of the lumps dataset using single-scale optimization.","title":"Analyzing Shape Model"},{"location":"use-cases/right-ventricle.html","text":"Right Ventricle: Highly Variable Shapes What and Where is the Use Case? This use case demonstrates using ShapeWorks tools to perform the following. Model the highly variable shapes of the right ventricle Study the group difference of right ventricle shape between control and patients Build a shape model where shapes are given as binary segmentations Groom a dataset that only contains shapes In this study, we have two sets of data, the control group with 6 subjects, and the patient group consists of 26 subjects. For each group, we have diastole and systole segmentations. The goal is to study the variation of the systole and diastole in the two groups. We pre-process/groom diastole and systole data and then optimize the particle system for them, independently. Then using the group difference analysis tool of ShapeWorks, we can quantify and study the difference of variation in control and patient groups from diastole to systole stage of heart. Important This use case is not yet released! Running the Use Case Important This use case is not yet added to the use case list! Grooming Data For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Isotropic Resampling : Binary segmentations are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also be used to produce images and segmentations with smaller voxel spacing, and thereby reduce aliasing artifacts (i.e., staircase/jagged surface) due to binarization for segmentations. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center-of-Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. Cropping : The segmentations are cropped so that all of the samples are within the same bounding box. The bounding box parameters are computed based on the biggest bounding box that encapsulates all the segmentations of the given dataset. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase. Optimizing Shape Model Analyzing Shape Model","title":"Right Ventricle: Highly Variable Shapes"},{"location":"use-cases/right-ventricle.html#right-ventricle-highly-variable-shapes","text":"","title":"Right Ventricle: Highly Variable Shapes"},{"location":"use-cases/right-ventricle.html#what-and-where-is-the-use-case","text":"This use case demonstrates using ShapeWorks tools to perform the following. Model the highly variable shapes of the right ventricle Study the group difference of right ventricle shape between control and patients Build a shape model where shapes are given as binary segmentations Groom a dataset that only contains shapes In this study, we have two sets of data, the control group with 6 subjects, and the patient group consists of 26 subjects. For each group, we have diastole and systole segmentations. The goal is to study the variation of the systole and diastole in the two groups. We pre-process/groom diastole and systole data and then optimize the particle system for them, independently. Then using the group difference analysis tool of ShapeWorks, we can quantify and study the difference of variation in control and patient groups from diastole to systole stage of heart. Important This use case is not yet released!","title":"What and Where is the Use Case?"},{"location":"use-cases/right-ventricle.html#running-the-use-case","text":"Important This use case is not yet added to the use case list!","title":"Running the Use Case"},{"location":"use-cases/right-ventricle.html#grooming-data","text":"For a description of the grooming tools and parameters, see: How to Groom Your Dataset? . Isotropic Resampling : Binary segmentations are resampled to have an isotropic voxel spacing using a user-defined spacing. This step could also be used to produce images and segmentations with smaller voxel spacing, and thereby reduce aliasing artifacts (i.e., staircase/jagged surface) due to binarization for segmentations. Apply Padding : Segmentations that touch the image boundary will have an artificial hole at that intersection. Segmentations are padded by adding a user-defined number of voxels along each image direction (rows, cols, and slices) to avoid introducing artificial holes. Center-of-Mass Alignment : This translational alignment step is performed before rigidly aligning the samples to a shape reference. This factors out translations to reduce the risk of misalignment and allow for a medoid sample to be automatically selected as the reference for rigid alignment. Reference Selection : The reference is selected by first computing the mean (average) distance transform of the segmentations, then selecting the sample closest to that mean (i.e., medoid). Rigid Alignment : All of the segmentations are then aligned to the selected reference using rigid alignment, which factors out the rotation and remaining translation. Cropping : The segmentations are cropped so that all of the samples are within the same bounding box. The bounding box parameters are computed based on the biggest bounding box that encapsulates all the segmentations of the given dataset. Distance Transform : Finally, the signed distance transform is computed, and the dataset is now ready for the optimize phase.","title":"Grooming Data"},{"location":"use-cases/right-ventricle.html#optimizing-shape-model","text":"","title":"Optimizing Shape Model"},{"location":"use-cases/right-ventricle.html#analyzing-shape-model","text":"","title":"Analyzing Shape Model"},{"location":"use-cases/use-cases.html","text":"Use Cases Downloading Use Case Dataset Use Case datasets such as the ellipsoid and left atrium datasets will be downloaded automatically from the ShapeWorks Data Portal . When running one of the use case example scripts, you will see something like this: Can't find ellipsoid.zip in the current directory. .___________________________. | | | ShapeWorks Portal | |___________________________| Downloading the ellipsoid dataset from the ShapeWorks Portal Login info is not found in the current directory. New ShapeWorks Portal users: Register an account at http://cibc1.sci.utah.edu:8080/#?dialog=register Returning ShapeWorks Portal users: Enter your username and password Username: Danger Do not use the same password as for your bank account or email! After registering a free account, you can log in from within the script. Username: joeshmoe Password: [1/1 MB] joeshmoe downloaded the ellipsoid dataset from the ShapeWorks Portal. Downloaded Data When a use case is run, it downloads the appropriate zipped data to Examples/Python/Data/ . The data is then extracted to Examples/Python/output/use_case_name/ where all of the output from running the use case is also saved. The downloaded data includes the raw data the use case starts with (segmentations, meshes, and/or images) as well as the output from the final grooming step of the use case (i.e. distance transforms) and the shape model generated from running the use case (particle files and xmls). To visualize a shape model, open the included \"analyze.xml\" file can be opened in Studio. For example, to view the shape model downloaded for the ellipsoid use case run: $ cd Examples/Python/Output/ellipsoid/ $ ShapeWorksStudio ellipsoid-v0/shape_models/ellipsoid/analyze_128.xml Running Use Case The use cases are located at: Examples/Python/ . When a use case is run, the dataset is automatically downloaded. Important You must first register of a free ShapeWorks account by visiting the ShapeWorks Data Portal . To run a use case, run the following command from the Examples/Python/ directory: $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case [insert name of use case here] For names for the use cases that are currently released and the full list of optional tags, run: $ python RunUseCase.py --help --use_subsample : To run on a subset of the data in the use case, add the --use_subsample tag followed by the --num_subsample tag with the number of samples you wish to use. For example: $ python RunUseCase.py --use_case [insert name of use case here] --use_subsample --num_subsample 10 This will select a representative subset of the specified sample size to run through the pipeline so that the use case runs faster and uses less memory. The subset is selected by running clustering, then picking one sample from each cluster so that the resulting subset is representative of all the data. If --use_subsample is used without --num_subsample it will use the default number of subsamples which is 3. --interactive : When the interactive tag is used, the user must press enter after each step. This allows the user to inspect the intermediate output between steps of the pipeline. $ python RunUseCase.py --use_case [insert name of use case here] --interactive --start_with_prepped_data : When this tag is used, the grooming steps are skipped. Instead of generating the distance transforms from segmentations via grooming, the distance transforms from the data .zip folder are used in optimization. If a user wishes to start with the optimization step, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case [insert name of use case here] --start_with_prepped_data --start_with_image_and_segmentation_data : Some use cases can be run on just segmentations or on segmentations plus the corresponding imaging data. To carry the image through the grooming process with the segmentation, the --start_with_image_and_segmentation_data tag must be used. $ python RunUseCase.py --use_case [insert name of use case here] --start_with_image_and_segmentation_data --use_single_scale : Use cases can be run with multi-scale or single-scale optimization. In both cases, particles on each shape sample are initialized using the particle splitting strategy starting from a single particle (or a given set of landmarks) until reaching the required number of particles. The optimized particles at each scale are used to initialize the next scale. At each scale, particles undergo initialization and optimization stages. The multi-scale triggers both the initialization and optimization stages, while the single-scale mode uses the initialization stage at each scale and runs the optimization stage when the required number of particles is reached (i.e., at the last scale). The default mode is mutli-scale. To run single-scale, use the --use_single_scale tag. $ python RunUseCase.py --use_case [insert name of use case here] --use_single_scale --tiny_test : Users can run a fast version of the use case by using the --tiny_test tag . This runs on a subset of the data for fewer optimization iterations to verify ShapeWorks has been properly installed. It is meant to test that use cases can run, not to create a good correspondence model. $ python RunUseCase.py --use_case [insert name of use case here] --tiny_test --shapeworks_path : This can be used to automatically add the ShapeWorks executable to the user's PATH variable if it has not already been set. To use this tag, add the --shapeworks_path tag followed by the path to the bin folder that contains ShapeWorks executables. $ python RunUseCase.py --use_case [insert name of use case here] shapeworks_path [path to shapeworks bin folder here] Running Subsequent Analysis To run ShapeWorksStudio again without running the full pipeline, you must first navigate to the Examples/Python/ directory, and then run ShapeWorksStudio on the appropriate analysis XML file. For example: $ cd /path/to/shapeworks/Examples/Python $ ShapeWorksStudio TestEllipsoids/PointFiles/analyze.xml Dataset Guidelines Check out How to Add New Datasets? for dataset upload instructions and guidelines.","title":"Getting Started with Use Cases"},{"location":"use-cases/use-cases.html#use-cases","text":"","title":"Use Cases"},{"location":"use-cases/use-cases.html#downloading-use-case-dataset","text":"Use Case datasets such as the ellipsoid and left atrium datasets will be downloaded automatically from the ShapeWorks Data Portal . When running one of the use case example scripts, you will see something like this: Can't find ellipsoid.zip in the current directory. .___________________________. | | | ShapeWorks Portal | |___________________________| Downloading the ellipsoid dataset from the ShapeWorks Portal Login info is not found in the current directory. New ShapeWorks Portal users: Register an account at http://cibc1.sci.utah.edu:8080/#?dialog=register Returning ShapeWorks Portal users: Enter your username and password Username: Danger Do not use the same password as for your bank account or email! After registering a free account, you can log in from within the script. Username: joeshmoe Password: [1/1 MB] joeshmoe downloaded the ellipsoid dataset from the ShapeWorks Portal.","title":"Downloading Use Case Dataset"},{"location":"use-cases/use-cases.html#downloaded-data","text":"When a use case is run, it downloads the appropriate zipped data to Examples/Python/Data/ . The data is then extracted to Examples/Python/output/use_case_name/ where all of the output from running the use case is also saved. The downloaded data includes the raw data the use case starts with (segmentations, meshes, and/or images) as well as the output from the final grooming step of the use case (i.e. distance transforms) and the shape model generated from running the use case (particle files and xmls). To visualize a shape model, open the included \"analyze.xml\" file can be opened in Studio. For example, to view the shape model downloaded for the ellipsoid use case run: $ cd Examples/Python/Output/ellipsoid/ $ ShapeWorksStudio ellipsoid-v0/shape_models/ellipsoid/analyze_128.xml","title":"Downloaded Data"},{"location":"use-cases/use-cases.html#running-use-case","text":"The use cases are located at: Examples/Python/ . When a use case is run, the dataset is automatically downloaded. Important You must first register of a free ShapeWorks account by visiting the ShapeWorks Data Portal . To run a use case, run the following command from the Examples/Python/ directory: $ cd /path/to/shapeworks/Examples/Python $ python RunUseCase.py --use_case [insert name of use case here] For names for the use cases that are currently released and the full list of optional tags, run: $ python RunUseCase.py --help --use_subsample : To run on a subset of the data in the use case, add the --use_subsample tag followed by the --num_subsample tag with the number of samples you wish to use. For example: $ python RunUseCase.py --use_case [insert name of use case here] --use_subsample --num_subsample 10 This will select a representative subset of the specified sample size to run through the pipeline so that the use case runs faster and uses less memory. The subset is selected by running clustering, then picking one sample from each cluster so that the resulting subset is representative of all the data. If --use_subsample is used without --num_subsample it will use the default number of subsamples which is 3. --interactive : When the interactive tag is used, the user must press enter after each step. This allows the user to inspect the intermediate output between steps of the pipeline. $ python RunUseCase.py --use_case [insert name of use case here] --interactive --start_with_prepped_data : When this tag is used, the grooming steps are skipped. Instead of generating the distance transforms from segmentations via grooming, the distance transforms from the data .zip folder are used in optimization. If a user wishes to start with the optimization step, add --start_with_prepped_data tag. $ python RunUseCase.py --use_case [insert name of use case here] --start_with_prepped_data --start_with_image_and_segmentation_data : Some use cases can be run on just segmentations or on segmentations plus the corresponding imaging data. To carry the image through the grooming process with the segmentation, the --start_with_image_and_segmentation_data tag must be used. $ python RunUseCase.py --use_case [insert name of use case here] --start_with_image_and_segmentation_data --use_single_scale : Use cases can be run with multi-scale or single-scale optimization. In both cases, particles on each shape sample are initialized using the particle splitting strategy starting from a single particle (or a given set of landmarks) until reaching the required number of particles. The optimized particles at each scale are used to initialize the next scale. At each scale, particles undergo initialization and optimization stages. The multi-scale triggers both the initialization and optimization stages, while the single-scale mode uses the initialization stage at each scale and runs the optimization stage when the required number of particles is reached (i.e., at the last scale). The default mode is mutli-scale. To run single-scale, use the --use_single_scale tag. $ python RunUseCase.py --use_case [insert name of use case here] --use_single_scale --tiny_test : Users can run a fast version of the use case by using the --tiny_test tag . This runs on a subset of the data for fewer optimization iterations to verify ShapeWorks has been properly installed. It is meant to test that use cases can run, not to create a good correspondence model. $ python RunUseCase.py --use_case [insert name of use case here] --tiny_test --shapeworks_path : This can be used to automatically add the ShapeWorks executable to the user's PATH variable if it has not already been set. To use this tag, add the --shapeworks_path tag followed by the path to the bin folder that contains ShapeWorks executables. $ python RunUseCase.py --use_case [insert name of use case here] shapeworks_path [path to shapeworks bin folder here]","title":"Running Use Case"},{"location":"use-cases/use-cases.html#running-subsequent-analysis","text":"To run ShapeWorksStudio again without running the full pipeline, you must first navigate to the Examples/Python/ directory, and then run ShapeWorksStudio on the appropriate analysis XML file. For example: $ cd /path/to/shapeworks/Examples/Python $ ShapeWorksStudio TestEllipsoids/PointFiles/analyze.xml","title":"Running Subsequent Analysis"},{"location":"use-cases/use-cases.html#dataset-guidelines","text":"Check out How to Add New Datasets? for dataset upload instructions and guidelines.","title":"Dataset Guidelines"},{"location":"users/citation.html","text":"How to Cite ShapeWorks? Acknowledgements If you use ShapeWorks in work that leads to published research, we humbly ask that you to cite ShapeWorks , add the following to the 'Acknowledgments' section of your paper: \"The National Institutes of Health supported this work under grant numbers NIBIB-U24EB029011, NIAMS-R01AR076120, NHLBI-R01HL135568, NIBIB-R01EB016701, and NIGMS-P41GM103545.\" and add the following 'disclaimer': \"The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.\" Citation When referencing ShapeWorks, please include a bibliographical reference to the paper below, and, if possible, include a link to shapeworks.sci.utah.edu . Joshua Cates, Shireen Elhabian, Ross Whitaker. \"Shapeworks: particle-based shape correspondence and visualization software.\" Statistical Shape and Deformation Analysis. Academic Press, 2017. 257-298. @incollection{cates2017shapeworks, title = {Shapeworks: particle-based shape correspondence and visualization software}, author = {Cates, Joshua and Elhabian, Shireen and Whitaker, Ross}, booktitle = {Statistical Shape and Deformation Analysis}, pages = {257--298}, year = {2017}, publisher = {Elsevier} }","title":"How to Cite ShapeWorks?"},{"location":"users/citation.html#how-to-cite-shapeworks","text":"","title":"How to Cite ShapeWorks?"},{"location":"users/citation.html#acknowledgements","text":"If you use ShapeWorks in work that leads to published research, we humbly ask that you to cite ShapeWorks , add the following to the 'Acknowledgments' section of your paper: \"The National Institutes of Health supported this work under grant numbers NIBIB-U24EB029011, NIAMS-R01AR076120, NHLBI-R01HL135568, NIBIB-R01EB016701, and NIGMS-P41GM103545.\" and add the following 'disclaimer': \"The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.\"","title":"Acknowledgements"},{"location":"users/citation.html#citation","text":"When referencing ShapeWorks, please include a bibliographical reference to the paper below, and, if possible, include a link to shapeworks.sci.utah.edu . Joshua Cates, Shireen Elhabian, Ross Whitaker. \"Shapeworks: particle-based shape correspondence and visualization software.\" Statistical Shape and Deformation Analysis. Academic Press, 2017. 257-298. @incollection{cates2017shapeworks, title = {Shapeworks: particle-based shape correspondence and visualization software}, author = {Cates, Joshua and Elhabian, Shireen and Whitaker, Ross}, booktitle = {Statistical Shape and Deformation Analysis}, pages = {257--298}, year = {2017}, publisher = {Elsevier} }","title":"Citation"},{"location":"users/install.html","text":"How to Install ShapeWorks? ShapeWorks provides official user releases, the features of which can be seen at Release Notes . First, download the latest ShapeWorks binary release . We also provide up-to-date development builds from the master branch for: Windows Mac Linux Important Please understand that these are in-progress development builds, not official releases. Then follow the appropriate instructions for their platform: Windows Mac Linux Activate shapeworks environment Each time you use ShapeWorks from the command line , you must first activate its environment using the conda activate shapeworks command on the terminal. Installing ShapeWorks on Windows Please make sure that you download the latest ShapeWorks binary release , or up-to-date development builds from the master branch for Windows . Please understand that these are in-progress development builds, not official releases. To install: Download and install the \"Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019.\" : https://aka.ms/vs/16/release/vc_redist.x64.exe Download and install Miniconda for Windows: https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe Copy \"C:\\Program Files\\ShapeWorks\\Examples\" to another location Open \"Anaconda Prompt\" CD to \"C:\\Program Files\\ShapeWorks\" Run: $ conda_installs.bat Running conda_installs.bat If you already have anaconda/miniconda installed, this step may hang. If this step hangs, please uninstall anaconda/miniconda and re-install it and then run conda_installs.bat. CD to your copied Examples\\Python location To run an example: $ python RunUseCase.py --use_case [insert name of use case here] For names for the use cases that are currently released, run: $ python RunUseCase.py --help More information about running use cases can be found here . Running conda_installs.bat will activate the conda shapeworks environment into your current anaconda prompt. Activate shapeworks environment For subsequent usage, the shapeworks conda environment must be activated using the conda activate shapeworks command on the terminal. Note If you have installed ShapeWorks in a different location than \"C:\\Program Files\\ShapeWorks\", please add the path to the \"bin\" directory as the final argument to RunUseCase.py so that the path will be set correctly. Installing ShapeWorks on Mac Please make sure that you download the latest ShapeWorks binary release , or up-to-date development builds from the master branch for Mac . Please understand that these are in-progress development builds, not official releases. To use the installer version of ShapeWorks (.pkg): Install the ShapeWorks pkg file. Open a terminal, change directory to /Applications/ShapeWorks Type: $ source conda_installs.sh Copy \"/Applications/ShapeWorks/Examples to another location (e.g., $HOME/ShapeWorks-Examples) CD to your copied Examples/Python location To run an example: $ python RunUseCase.py --use_case [insert name of use case here] For names for the use cases that are currently released, run: $ python RunUseCase.py --help More information about running use cases can be found here . Activate shapeworks environment For subsequent usage, the shapeworks conda environment must be activated using the conda activate shapeworks command on the terminal. Installing ShapeWorks on Linux Please make sure that you download the latest ShapeWorks binary release , or up-to-date development builds from the master branch for Linux . Please understand that these are in-progress development builds, not official releases. ShapeWorks comes with python examples to get you started. To run them, you will need python with a few packages. The easiest way to install them is to run: $ source ./conda_installs.sh Then, to run the example: $ cd Examples/Python $ python RunUseCase.py --use_case [insert name of use case here] For names for the use cases that are currently released, run: $ python RunUseCase.py --help More information about running use cases can be found here . Activate shapeworks environment For subsequent usage, the shapeworks conda environment must be activated using the conda activate shapeworks command on the terminal.","title":"How to Install ShapeWorks?"},{"location":"users/install.html#how-to-install-shapeworks","text":"ShapeWorks provides official user releases, the features of which can be seen at Release Notes . First, download the latest ShapeWorks binary release . We also provide up-to-date development builds from the master branch for: Windows Mac Linux Important Please understand that these are in-progress development builds, not official releases. Then follow the appropriate instructions for their platform: Windows Mac Linux Activate shapeworks environment Each time you use ShapeWorks from the command line , you must first activate its environment using the conda activate shapeworks command on the terminal.","title":"How to Install ShapeWorks?"},{"location":"users/install.html#installing-shapeworks-on-windows","text":"Please make sure that you download the latest ShapeWorks binary release , or up-to-date development builds from the master branch for Windows . Please understand that these are in-progress development builds, not official releases. To install: Download and install the \"Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019.\" : https://aka.ms/vs/16/release/vc_redist.x64.exe Download and install Miniconda for Windows: https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe Copy \"C:\\Program Files\\ShapeWorks\\Examples\" to another location Open \"Anaconda Prompt\" CD to \"C:\\Program Files\\ShapeWorks\" Run: $ conda_installs.bat Running conda_installs.bat If you already have anaconda/miniconda installed, this step may hang. If this step hangs, please uninstall anaconda/miniconda and re-install it and then run conda_installs.bat. CD to your copied Examples\\Python location To run an example: $ python RunUseCase.py --use_case [insert name of use case here] For names for the use cases that are currently released, run: $ python RunUseCase.py --help More information about running use cases can be found here . Running conda_installs.bat will activate the conda shapeworks environment into your current anaconda prompt. Activate shapeworks environment For subsequent usage, the shapeworks conda environment must be activated using the conda activate shapeworks command on the terminal. Note If you have installed ShapeWorks in a different location than \"C:\\Program Files\\ShapeWorks\", please add the path to the \"bin\" directory as the final argument to RunUseCase.py so that the path will be set correctly.","title":"Installing ShapeWorks on Windows"},{"location":"users/install.html#installing-shapeworks-on-mac","text":"Please make sure that you download the latest ShapeWorks binary release , or up-to-date development builds from the master branch for Mac . Please understand that these are in-progress development builds, not official releases. To use the installer version of ShapeWorks (.pkg): Install the ShapeWorks pkg file. Open a terminal, change directory to /Applications/ShapeWorks Type: $ source conda_installs.sh Copy \"/Applications/ShapeWorks/Examples to another location (e.g., $HOME/ShapeWorks-Examples) CD to your copied Examples/Python location To run an example: $ python RunUseCase.py --use_case [insert name of use case here] For names for the use cases that are currently released, run: $ python RunUseCase.py --help More information about running use cases can be found here . Activate shapeworks environment For subsequent usage, the shapeworks conda environment must be activated using the conda activate shapeworks command on the terminal.","title":"Installing ShapeWorks on Mac"},{"location":"users/install.html#installing-shapeworks-on-linux","text":"Please make sure that you download the latest ShapeWorks binary release , or up-to-date development builds from the master branch for Linux . Please understand that these are in-progress development builds, not official releases. ShapeWorks comes with python examples to get you started. To run them, you will need python with a few packages. The easiest way to install them is to run: $ source ./conda_installs.sh Then, to run the example: $ cd Examples/Python $ python RunUseCase.py --use_case [insert name of use case here] For names for the use cases that are currently released, run: $ python RunUseCase.py --help More information about running use cases can be found here . Activate shapeworks environment For subsequent usage, the shapeworks conda environment must be activated using the conda activate shapeworks command on the terminal.","title":"Installing ShapeWorks on Linux"},{"location":"users/papers.html","text":"Relevant Papers Note If you would like to have your publication listed here: e-mail us! The following selected papers give background on the methods used in ShapeWorks, and a sample of the areas ShapeWorks has been applied: M. Jacxsens, S. Elhabian, S. Brady, P. Chalmers, R. Tashjian, and H. Henninger. \"Coracoacromial morphology: a contributor to recurrent traumatic anterior glenohumeral instability,\" Journal of Shoulder and Elbow Surgery, 1:1316-1325, 2019. P. R. Atkins, Y. Shin, P. Agrawal, S. Y. Elhabian, R. T. Whitaker, J. A. Weiss, S. K. Aoki, C. L. Peters ,and A. E. Anderson. \"Which Two-dimensional Radiographic Measurements of Cam Femoroacetabular Impingement Best Describe the Three-dimensional Shape of the Proximal Femur?,\" Clinical Orthopaedics and Related Research, 477(1):242\u2013253, 2019. T. Sodergren, R. Bhalodia, R. Whitaker, J. Cates, N. Marrouche, and S. Elhabian. \"Mixture modeling of global shape priors and autoencoding local intensity priors for left atrium segmentation,\" In STACOM- MICCAI: Statistical Atlases and Computational Modeling of the Heart workshop, page in press. Springer, 2018. A. Goparaju, I. Csecs, A. Morris, E. Kholmovski, N. Marrouche, R. Whitaker, and S. Elhabian. \"On the Evaluation and Validation of Off-the-Shelf Statistical Shape Modeling Tools: A Clinical Application,\" In International Workshop on Shape in Medical Imaging, pages 14\u201327. Springer, 2018. E. T. Bieging, A. Morris, B. D. Wilson, C. J. McGann, N. F. Marrouche, and J. Cates. \"Left atrial shape predicts recurrence after atrial fibrillation catheter ablation,\" Journal of Cardiovascular Electrophysiology, 2018. P. R. Atkins, S. Y. Elhabian, P. Agrawal, M. D. Harris, R. T. Whitaker, J. A. Weiss, C. L. Peters, and A. E. Anderson. \"Quantitative comparison of cortical bone thickness using correspondence-based shape modeling in patients with cam femoroacetabular impingement,\" Journal of Orthopaedic Research, 35(8):1743\u20131753, 2017. J. Cates, L. Nevell, S. I. Prajapati, L. D. Nelon, J. Y. Chang, M. E. Randolph, B. Wood, C. Keller, and R. T. Whitaker. \"Shape analysis of the basioccipital bone in Pax7-deficient mice,\" Scientific Reports, 7(1):17955, 2017. J. Cates, S. Elhabian, and R. Whitaker. \"ShapeWorks: particle-based shape correspondence and visualization software,\" In G. Zheng, S. Li, and G. Szekely, editors, Statistical Shape and Deformation Analysis: Methods, Implementation and Applications, 1st Edition, chapter 10. Academic Press, 2017. P. Agrawal, S.Y. Elhabian, R.T. Whitaker, \"Learning Deep Features for Automated Placement of Correspondence Points on Ensembles of Complex Shapes,\" In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 185-193. Springer, Cham, 2017. S. Sultana, P. Agrawal, S. Elhabian, R. Whitaker, T. Rashid, J. Blatt, J. Cetas, and M. Audette. \"Towards a statistical shape-aware deformable contour model for cranial nerve identification,\" In Workshop on Clinical Image-Based Procedures, pages 68\u201376. Springer, 2016. J. Cates, E. Bieging, A. Morris, G. Gardner, N. Akoum, E. Kholmovski, N. Marrouche, C. McGann, and R. S. MacLeod. \"Computational shape models characterize shape change of the left atrium in atrial fibrillation,\" Clinical Medicine Insights. Cardiology, 8(Suppl 1):99, 2015. Ken Museth. \"VDB: High-resolution sparse volumes with dynamic topology\" ACM Transactions on Graphics, July 2013, Article No.: 27 M. Datar, I. Lyu, S. Kim, J. Cates, M. Styner, R. Whitaker. \"Geodesic distances to landmarks for dense correspondence on ensembles of complex shapes,\" In International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, Berlin, Heidelberg, pp. 19-26, 2013. M.D. Harris, M. Datar, R.T. Whitaker, E.R. Jurrus, C.L. Peters, A.E. Anderson. \"Statistical Shape Modeling of Cam Femoroacetabular Impingement,\" In Journal of Orthopaedic Research, Vol. 31, No. 10, pp. 1620--1626. 2013. K.B. Jones, M. Datar, S. Ravichandran, H. Jin, E. Jurrus, R.T. Whitaker, M.R. Capecchi. \"Toward an Understanding of the Short Bone Phenotype Associated with Multiple Osteochondromas,\" In Journal of Orthopaedic Research, Vol. 31, No. 4, pp. 651--657. 2013. J. Cates, P.T. Fletcher, Z. Warnock, R.T. Whitaker. \"A Shape Analysis Framework for Small Animal Phenotyping with Application to Mice with a Targeted Disruption of Hoxd11,\" In Proceedings of the 5th IEEE International Symposium on Biomedical Imaging (ISBI '08), pp. 512--516. 2008. DOI: 10.1109/ISBI.2008.4541045 J. Cates, P.T. Fletcher, M. Styner, H. Hazlett, R.T. Whitaker. \"Particle-Based Shape Analysis of Multi-Object Complexes,\" In Proceedings of the 11th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI '08), Lecture Notes In Computer Science (LCNS), pp. 477--485. 2008. ISBN: 978-3-540-85987-1 J. Cates, P. T. Fletcher, M. Styner, M. Shenton, and R. Whitaker. \"Shape modeling and analysis with entropy-based particle systems,\" In Information Processing in Medical Imaging, pages 333\u2013345. Springer, 2007.","title":"Revelant Papers"},{"location":"users/papers.html#relevant-papers","text":"Note If you would like to have your publication listed here: e-mail us! The following selected papers give background on the methods used in ShapeWorks, and a sample of the areas ShapeWorks has been applied: M. Jacxsens, S. Elhabian, S. Brady, P. Chalmers, R. Tashjian, and H. Henninger. \"Coracoacromial morphology: a contributor to recurrent traumatic anterior glenohumeral instability,\" Journal of Shoulder and Elbow Surgery, 1:1316-1325, 2019. P. R. Atkins, Y. Shin, P. Agrawal, S. Y. Elhabian, R. T. Whitaker, J. A. Weiss, S. K. Aoki, C. L. Peters ,and A. E. Anderson. \"Which Two-dimensional Radiographic Measurements of Cam Femoroacetabular Impingement Best Describe the Three-dimensional Shape of the Proximal Femur?,\" Clinical Orthopaedics and Related Research, 477(1):242\u2013253, 2019. T. Sodergren, R. Bhalodia, R. Whitaker, J. Cates, N. Marrouche, and S. Elhabian. \"Mixture modeling of global shape priors and autoencoding local intensity priors for left atrium segmentation,\" In STACOM- MICCAI: Statistical Atlases and Computational Modeling of the Heart workshop, page in press. Springer, 2018. A. Goparaju, I. Csecs, A. Morris, E. Kholmovski, N. Marrouche, R. Whitaker, and S. Elhabian. \"On the Evaluation and Validation of Off-the-Shelf Statistical Shape Modeling Tools: A Clinical Application,\" In International Workshop on Shape in Medical Imaging, pages 14\u201327. Springer, 2018. E. T. Bieging, A. Morris, B. D. Wilson, C. J. McGann, N. F. Marrouche, and J. Cates. \"Left atrial shape predicts recurrence after atrial fibrillation catheter ablation,\" Journal of Cardiovascular Electrophysiology, 2018. P. R. Atkins, S. Y. Elhabian, P. Agrawal, M. D. Harris, R. T. Whitaker, J. A. Weiss, C. L. Peters, and A. E. Anderson. \"Quantitative comparison of cortical bone thickness using correspondence-based shape modeling in patients with cam femoroacetabular impingement,\" Journal of Orthopaedic Research, 35(8):1743\u20131753, 2017. J. Cates, L. Nevell, S. I. Prajapati, L. D. Nelon, J. Y. Chang, M. E. Randolph, B. Wood, C. Keller, and R. T. Whitaker. \"Shape analysis of the basioccipital bone in Pax7-deficient mice,\" Scientific Reports, 7(1):17955, 2017. J. Cates, S. Elhabian, and R. Whitaker. \"ShapeWorks: particle-based shape correspondence and visualization software,\" In G. Zheng, S. Li, and G. Szekely, editors, Statistical Shape and Deformation Analysis: Methods, Implementation and Applications, 1st Edition, chapter 10. Academic Press, 2017. P. Agrawal, S.Y. Elhabian, R.T. Whitaker, \"Learning Deep Features for Automated Placement of Correspondence Points on Ensembles of Complex Shapes,\" In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 185-193. Springer, Cham, 2017. S. Sultana, P. Agrawal, S. Elhabian, R. Whitaker, T. Rashid, J. Blatt, J. Cetas, and M. Audette. \"Towards a statistical shape-aware deformable contour model for cranial nerve identification,\" In Workshop on Clinical Image-Based Procedures, pages 68\u201376. Springer, 2016. J. Cates, E. Bieging, A. Morris, G. Gardner, N. Akoum, E. Kholmovski, N. Marrouche, C. McGann, and R. S. MacLeod. \"Computational shape models characterize shape change of the left atrium in atrial fibrillation,\" Clinical Medicine Insights. Cardiology, 8(Suppl 1):99, 2015. Ken Museth. \"VDB: High-resolution sparse volumes with dynamic topology\" ACM Transactions on Graphics, July 2013, Article No.: 27 M. Datar, I. Lyu, S. Kim, J. Cates, M. Styner, R. Whitaker. \"Geodesic distances to landmarks for dense correspondence on ensembles of complex shapes,\" In International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, Berlin, Heidelberg, pp. 19-26, 2013. M.D. Harris, M. Datar, R.T. Whitaker, E.R. Jurrus, C.L. Peters, A.E. Anderson. \"Statistical Shape Modeling of Cam Femoroacetabular Impingement,\" In Journal of Orthopaedic Research, Vol. 31, No. 10, pp. 1620--1626. 2013. K.B. Jones, M. Datar, S. Ravichandran, H. Jin, E. Jurrus, R.T. Whitaker, M.R. Capecchi. \"Toward an Understanding of the Short Bone Phenotype Associated with Multiple Osteochondromas,\" In Journal of Orthopaedic Research, Vol. 31, No. 4, pp. 651--657. 2013. J. Cates, P.T. Fletcher, Z. Warnock, R.T. Whitaker. \"A Shape Analysis Framework for Small Animal Phenotyping with Application to Mice with a Targeted Disruption of Hoxd11,\" In Proceedings of the 5th IEEE International Symposium on Biomedical Imaging (ISBI '08), pp. 512--516. 2008. DOI: 10.1109/ISBI.2008.4541045 J. Cates, P.T. Fletcher, M. Styner, H. Hazlett, R.T. Whitaker. \"Particle-Based Shape Analysis of Multi-Object Complexes,\" In Proceedings of the 11th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI '08), Lecture Notes In Computer Science (LCNS), pp. 477--485. 2008. ISBN: 978-3-540-85987-1 J. Cates, P. T. Fletcher, M. Styner, M. Shenton, and R. Whitaker. \"Shape modeling and analysis with entropy-based particle systems,\" In Information Processing in Medical Imaging, pages 333\u2013345. Springer, 2007.","title":"Relevant Papers"},{"location":"workflow/analyze.html","text":"How to Analyze Your Shape Model? Surface Correspondences ShapeWorks includes a Qt and VTK-based graphical user interface (GUI), ShapeWorksStudio , that allows visualizing the optimized particle system (i.e., correspondence model) for each shape sample where particle coloring is used to reflect correspondence among shapes. Particles' coloring reflects surface correspondences across different shape samples You can scroll through the dataset and zoom in and out to inspect fewer or more samples. ShapeWorksStudio keeps a consistent camera view across all shape samples to facilitate qualitative comparisons of different samples relative to each other. Correspondence model inspectation by scrolling though the dataset and zooming in & out Running ShapeWorksStudio When you open ShapeWorksStudio without a project, either from terminal or double-clicking on the application binary/exe file, the splash screen is displayed to enable you to load a recent project, open existing projects on your local machine, or create a new project. ShapeWorksStudio splash screen Using XML Files You can run ShapeWorksStudio using an XML parameter file that includes the following tags. <point_files> [required]: list of _world.particles files (output of the shapeworks optimize ) <group_ids> [optional]: the group id (1 or 2) in case the data contains groups <world_point_files> [optional]: list of _world.particles files (output of the shapeworks optimize ), same as <point_files> <local_point_files> [optional]: list of _local.particles files (output of the shapeworks optimize ) <distance_transform_files> [optional]: list of distance transforms (input of the shapeworks optimize ) Please note the following: If the xml file only contains the <point_files> list, VTK-based surface reconstruction is used for surface reconstruction. To enable particle-based surface reconstruction, <world_point_files> , <local_point_files> , and <distance_transform_files> lists should be given. Examples/Python/<use-case-name>.py include an analyze step that generates analyze.xml , which includes the path to the input data and corresponding particles. All lists should have the same order of shape samples Given the XML files, you can launch ShapeWorksStudio through the terminal using the following command. $ ShapeWorksStudio analyze.xml Using Spreadsheets ShapeWorksStudio can also load datasets (and shape models) using spreadsheets, a more user-friendly user-editable file format. See the ellipsoid studio example in Examples/Studio/ellipsoid.xlsx for an example. ShapeWorksStudio keeps track of results from different phases in individual sheets Surface Reconstruction For visualization purposes, the shape's particle system is used to reconstruct its surface mesh using a template-deformation approach to establish an inter-sample dense surface correspondence given a sparse set of optimized particles. To avoid introducing bias due to template choice, we use an unbiased framework for template mesh construction that entails: A generalized Procrustes alignment to define the mean particle system A nonlinear warping function built using the shape's particle system and the mean one as control points. A mean distance transform (DT) computed by averaging warped sample-specific DT. The template mesh will then be constructed by triangulating the isosurface of this mean DT. A warping function is constructed to deform the dense template mesh to the sample space using the sample's and mean particle systems as control points to reconstruct a sample-specific surface mesh. Particle-based surface reconstruction currently supported by ShapeWorksStudio (old) VTK-based surface reconstruction (new) particle-based surface reconstruction You can export any mesh using File -> Export -> Export Current Mesh ... . Statistical Analysis Statistical analysis is performed using principal component analysis (PCA), where the mean and modes of shape variation are computed based on the optimized correspondence model. Animation is allowed to watch the shape morph at various standard deviations along a user-selected variation mode. Animating the shape variation along with a principal component while passing through the mean shape You can also export the eigenvalues and each shape's PCA loadings in an excel sheet for further analysis. Group Differences If there are groups in the data, ShapeWorksStudio can visualize significant group differences. Statistical group differences for characterizing scapular morphology in Hill-Sachs patients If you are using an XML file to load your data into ShapeWorksStudio , a group id (1 or 2) should be given to each sample. If you are using a spreadsheet, specify group columns in the data (first) sheet in the spreadsheet with the prefix group_ , which enables adding multiple groups to your project and selecting the group-of-interest within ShapeWorksStudio. Groups as columns in the project spreadsheet In the Group tab of the Analyze panel, choose which column to indicate the group set of interest Then, select which groups you would like to compare (now support more than two groups) Feature Maps ShapeWorksStudio has the ability to integrate feature maps . A feature map is a 3d image volume that contains scalar values to be associated with each shape\u2019s surface. For example, this could be raw or processed CT/MRI data. The feature map can be displayed for each surface by choosing the desired feature map in the feature map combobox at the bottom of the screen. After the correspondence is generated, the average feature map can be displayed on the mean shape in the analysis tab.","title":"How to Analyze Your Shape Model?"},{"location":"workflow/analyze.html#how-to-analyze-your-shape-model","text":"","title":"How to Analyze Your Shape Model?"},{"location":"workflow/analyze.html#surface-correspondences","text":"ShapeWorks includes a Qt and VTK-based graphical user interface (GUI), ShapeWorksStudio , that allows visualizing the optimized particle system (i.e., correspondence model) for each shape sample where particle coloring is used to reflect correspondence among shapes. Particles' coloring reflects surface correspondences across different shape samples You can scroll through the dataset and zoom in and out to inspect fewer or more samples. ShapeWorksStudio keeps a consistent camera view across all shape samples to facilitate qualitative comparisons of different samples relative to each other. Correspondence model inspectation by scrolling though the dataset and zooming in & out","title":"Surface Correspondences"},{"location":"workflow/analyze.html#running-shapeworksstudio","text":"When you open ShapeWorksStudio without a project, either from terminal or double-clicking on the application binary/exe file, the splash screen is displayed to enable you to load a recent project, open existing projects on your local machine, or create a new project. ShapeWorksStudio splash screen","title":"Running ShapeWorksStudio"},{"location":"workflow/analyze.html#using-xml-files","text":"You can run ShapeWorksStudio using an XML parameter file that includes the following tags. <point_files> [required]: list of _world.particles files (output of the shapeworks optimize ) <group_ids> [optional]: the group id (1 or 2) in case the data contains groups <world_point_files> [optional]: list of _world.particles files (output of the shapeworks optimize ), same as <point_files> <local_point_files> [optional]: list of _local.particles files (output of the shapeworks optimize ) <distance_transform_files> [optional]: list of distance transforms (input of the shapeworks optimize ) Please note the following: If the xml file only contains the <point_files> list, VTK-based surface reconstruction is used for surface reconstruction. To enable particle-based surface reconstruction, <world_point_files> , <local_point_files> , and <distance_transform_files> lists should be given. Examples/Python/<use-case-name>.py include an analyze step that generates analyze.xml , which includes the path to the input data and corresponding particles. All lists should have the same order of shape samples Given the XML files, you can launch ShapeWorksStudio through the terminal using the following command. $ ShapeWorksStudio analyze.xml","title":"Using XML Files"},{"location":"workflow/analyze.html#using-spreadsheets","text":"ShapeWorksStudio can also load datasets (and shape models) using spreadsheets, a more user-friendly user-editable file format. See the ellipsoid studio example in Examples/Studio/ellipsoid.xlsx for an example. ShapeWorksStudio keeps track of results from different phases in individual sheets","title":"Using Spreadsheets"},{"location":"workflow/analyze.html#surface-reconstruction","text":"For visualization purposes, the shape's particle system is used to reconstruct its surface mesh using a template-deformation approach to establish an inter-sample dense surface correspondence given a sparse set of optimized particles. To avoid introducing bias due to template choice, we use an unbiased framework for template mesh construction that entails: A generalized Procrustes alignment to define the mean particle system A nonlinear warping function built using the shape's particle system and the mean one as control points. A mean distance transform (DT) computed by averaging warped sample-specific DT. The template mesh will then be constructed by triangulating the isosurface of this mean DT. A warping function is constructed to deform the dense template mesh to the sample space using the sample's and mean particle systems as control points to reconstruct a sample-specific surface mesh. Particle-based surface reconstruction currently supported by ShapeWorksStudio (old) VTK-based surface reconstruction (new) particle-based surface reconstruction You can export any mesh using File -> Export -> Export Current Mesh ... .","title":"Surface Reconstruction"},{"location":"workflow/analyze.html#statistical-analysis","text":"Statistical analysis is performed using principal component analysis (PCA), where the mean and modes of shape variation are computed based on the optimized correspondence model. Animation is allowed to watch the shape morph at various standard deviations along a user-selected variation mode. Animating the shape variation along with a principal component while passing through the mean shape You can also export the eigenvalues and each shape's PCA loadings in an excel sheet for further analysis.","title":"Statistical Analysis"},{"location":"workflow/analyze.html#group-differences","text":"If there are groups in the data, ShapeWorksStudio can visualize significant group differences. Statistical group differences for characterizing scapular morphology in Hill-Sachs patients If you are using an XML file to load your data into ShapeWorksStudio , a group id (1 or 2) should be given to each sample. If you are using a spreadsheet, specify group columns in the data (first) sheet in the spreadsheet with the prefix group_ , which enables adding multiple groups to your project and selecting the group-of-interest within ShapeWorksStudio. Groups as columns in the project spreadsheet In the Group tab of the Analyze panel, choose which column to indicate the group set of interest Then, select which groups you would like to compare (now support more than two groups)","title":"Group Differences"},{"location":"workflow/analyze.html#feature-maps","text":"ShapeWorksStudio has the ability to integrate feature maps . A feature map is a 3d image volume that contains scalar values to be associated with each shape\u2019s surface. For example, this could be raw or processed CT/MRI data. The feature map can be displayed for each surface by choosing the desired feature map in the feature map combobox at the bottom of the screen. After the correspondence is generated, the average feature map can be displayed on the mean shape in the analysis tab.","title":"Feature Maps"},{"location":"workflow/groom.html","text":"How to Groom Your Dataset? ShapeWorks needs suitable distance transforms for establishing shape correspondence. The groom stage has the pipeline to generate the distance transforms from input images (binary segmentation or mesh). It consists of image/mesh pre-processing and alignment tools. Image Pre-Processing Antialias This tool antialiases binary volumes. shapeworks readimage --name $1 antialias maxrmserror $2 iterations $3 layers $4 writeimage --name $5 Here is the list of arguments. maxrmserror: Maximum RMS error determines how fast the solver converges. Range [0.0, 1.0], larger is faster [default: 0.01]. iterations: Maximum number of iterations [default: 50]. layers: Number of layers around a 3d pixel to use for this computation [default: 3]. Resample This tool uses an interpolation process to resample anisotropic voxels into isotropic-sized voxels. shapeworks readimage --name $1 resample --isospacing $2 --spacex $3 --spacey $4 --spacez $5 --sizex $6 --sizey $7 --sizez $8 writeimage --name $9 Here is the list of arguments. isospacing: The isotropic spacing in all dimensions. spacex: Pixel spacing in x-direction [default: 1.0]. spacey: Pixel spacing in y-direction [default: 1.0]. spacez: Pixel spacing in z-direction [default: 1.0]. sizex: Output size in x-direction [default: calculated using current size and desired spacing]. sizey: Output size in y-direction [default: calculated using current size and desired spacing]. sizez: Output size in z-direction [default: calculated using current size and desired spacing]. Recenter This tool recenters an image by changing its origin in the image header to the physical coordinates of the center of the image. shapeworks readimage --name $1 recenter writeimage --name $2 Pad This tool pads pads an image with specified value by specified number of voxels in the x-, y-, and/or z- directions; origin remains at the same location (note: negative padding to shrink an image is permitted). shapeworks readimage --name $1 pad --padding $2 --value $3 writeimage --name $4 shapeworks readimage --name $1 pad --padx $2 --pady $3 --padz $4 writeimage --name $5 Here is the list of arguments. padding: Pad this many voxels in ALL directions (used if set) [default: 0]. padx: Pad this many voxels in the x-direction [default: 0]. pady: Pad this many voxels in the y-direction [default: 0]. padz: Pad this many voxels in the z-direction [default: 0]. value: Value used to fill padded voxels [default: 0.0]. Translate This tool translates image by specified physical (image space) distance. shapeworks readimage --name $1 translate --centerofmass writeimage --name $2 shapeworks readimage --name $1 translate --tx $2 --ty $3 --tz $4 writeimage --name $5 Here is the list of arguments. centerofmass: Use center of mass [default: false]. tx: X distance ty: Y distance tz: Z distance Scale This tool scales image by specified value. shapeworks readimage --name $1 scale --sx $2 --sy $3 --sz $4 writeimage --name $5 Here is the list of arguments. tx: X scale ty: Y scale tz: Z scale Rotate This tool rotates image by specified value. shapeworks readimage --name $1 rotate --rx $2 --radians $3 writeimage --name $4 shapeworks readimage --name $1 rotate --degrees $2 writeimage --name $3 Here is the list of arguments. rx: Physical axis around which to rotate [default: z-axis] ry: Physical axis around which to rotate [default: z-axis] rz: Physical axis around which to rotate [default: z-axis] radians: Angle in radians degrees: Angle in degrees Extract Label This tool extracts/isolates the specific voxel label from a given multi-label volume and outputs the corresponding binary image. shapeworks readimage --name $1 extractlabel label $2 writeimage --name $3 Here is the list of arguments. label: Label value to be extracted [default: 1.0]. Close Holes This tool closes holes in a volume defined by values larger than specified value. shapeworks readimage --name $1 closeholes writeimage --name $2 Binarize This tool sets portion of image greater than min and less than or equal to max to the specified value. shapeworks readimage --name $1 binarize --min $2 --max $3 --value $4 writeimage --name $5 Here is the list of arguments. min: Lower threshold level [default: 0.0]. max: Upper threshold level [default: inf ]. value: Value to set region [default: 1.0]. Compute DT This tool computes signed distance transform volume from an image at the specified isovalue. shapeworks readimage --name $1 computedt --isovalue $2 writeimage --name $3 Here is the list of arguments. isovalue: Level set value that defines the interface between foreground and background [default: 0.0]. Curvature This tool denoises an image using curvature driven flow using curvature flow image filter. shapeworks readimage --name $1 curvature --iterations $2 writeimage --name $3 Here is the list of arguments. iterations: Number of iterations [default: 10]. Gradient This tool computes gradient magnitude of an image region at each pixel using gradient magnitude filter. shapeworks readimage --name $1 curvature --iterations $2 writeimage --name $3 Here is the list of arguments. iterations: Number of iterations [default: 10]. Sigmoid This tool computes sigmoid function pixel-wise using sigmoid image filter. shapeworks readimage --name $1 sigmoid --alpha $2 --beta $3 writeimage --name $4 Here is the list of arguments. alpha: Value of alpha [default: 10.0]. beta: Value of beta [default: 10.0]. TPLevelSetFilter This tool segments structures in image using topology preserving geodesic active contour level set filter. shapeworks readimage --name $1 tplevelset --featureimage $2 --scaling $3 writeimage --name $4 Here is the list of arguments. featureimage: Path of feature image for filter scaling: Value of scale [default: 20.0]. TopologyPreservingFilter This tool is a helper command that applies gradient and sigmoid filters to create a feature image for the TPLevelSet filter; note that the curvature flow filter is sometimes applied to the image before this. shapeworks readimage --name $1 topopreservingsmooth --scaling $2 --alpha $3 --beta $4 writeimage --name $5 Here is the list of arguments. scaling: Scale for TPLevelSet level set filter [default: 20.0]. alpha: Value of alpha for sigmoid fitler [default: 10.0]. beta: Value of beta for sigmoid fitler [default: 10.0.0]. Blur This tool applies gaussian blur. shapeworks readimage --name $1 blur --sigma $2 writeimage --name $3 Here is the list of arguments. sigma: Value of sigma [default: 0.0]. ICPRigid This tool transform current image using iterative closest point (ICP) 3D rigid registration computed from source to target distance maps. shapeworks readimage --name $1 icp --source $2 --target $3 --isovalue $4 --iterations $5 writeimage --name $6 Here is the list of arguments. source: Distance map of source image. target: Distance map of target image. isovalue: Isovalue of distance maps used to create ICPtransform [default: 0.0]. iterations: Number of iterations run ICP registration [default: 20]. Bounding Box This tool computes the largest bounding box for the shape population needed for cropping all the images. shapeworks readimage --name $1 boundingbox --names $2 -- --padding $3 --isovalue $4 writeimage --name $5 Here is the list of arguments. names: Paths to images (must be followed by -- ), ex: \\\"bounding-box --names *.nrrd -- --isovalue 1.5\\\"). padding: Number of extra voxels in each direction to pad the largest bounding box [default: 0]. isovalue: Threshold value [default: 1.0]. Crop This tool crops image down to the current region (e.g., from bounding-box command), or the specified min/max in each direction [default: image dimensions]. shapeworks readimage --name $1 crop --xmin $2 --xmax $3 --ymin $4 --ymax $5 --zmin $6 --zmax $7 writeimage --name $8 Here is the list of arguments. xmin: Minimum X. xmax: Maximum X. ymin: Minimum Y. ymax: Maximum Y. zmin: Minimum Z. zmax: Maximum Z. Clip Volume This tool clips volume with the specified cutting planes defined by three 3D points shapeworks readimage --name $1 clip --x1 $2 --y1 $3 --z1 $4 --x2 $5 --y2 $6 --z2 $7 --x3 $8 --y3 $9 --z3 $10 writeimage --name $11 Here is the list of arguments. x1: Value of x1 for cutting plane [default: 0.0]. y1: Value of y1 for cutting plane [default: 0.0]. z1: Value of z1 for cutting plane [default: 0.0]. x2: Value of x2 for cutting plane [default: 0.0]. y2: Value of y2 for cutting plane [default: 0.0]. z2: Value of z2 for cutting plane [default: 0.0]. x3: Value of x3 for cutting plane [default: 0.0]. y3: Value of y3 for cutting plane [default: 0.0]. z3: Value of z3 for cutting plane [default: 0.0]. Reflect Images This tool reflects image with respect to logical image center and the specified axis. shapeworks readimage --name $1 reflect --axis $2 writeimage --name $3 Here is the list of arguments. axis: Axis along which to reflect (X, Y, or Z). Set Origin This tool sets origin. shapeworks readimage --name $1 setorigin --x $2 --y $3 --z $4 writeimage --name $5 Here is the list of arguments. x: x value of origin [default: 0.0]. y: y value of origin [default: 0.0]. z: z value of origin [default: 0.0]. Warp This tool finds the warp between the source and target landmarks and transforms image by this warp. shapeworks readimage --name $1 warpimage --source $2 --target $3 --landmarks $4 writeimage --name $5 Here is the list of arguments. source: Path to source landmarks. target: Path to target landmarks. landmarks: Every stride points will be used for warping [default: 1]. Compare This tool compares two images. shapeworks readimage --name $1 compare --name $2 --verifyall $4 --tolerance $4 --precision $5 writeimage --name $6 Here is the list of arguments. name: Compare this image with another verifyall: Also verify origin, spacing, and direction matches [default: true]. tolerance: Allowed percentage of pixel differences [default: 0.0]. precision: Allowed difference between two pixels for them to still be considered equal [default: 0.0]. Negate This tool negates the values in the given image. shapeworks readimage --name $1 negate writeimage --name $2 Add This tool adds a value to each pixel in the given image and/or add another image in a pixelwise manner. shapeworks readimage --name $1 add --value $2 --name $3 writeimage --name $4 Here is the list of arguments. value: Value to add to each pixel. name: Name of image to add pixelwise. Subtract This tool subtracts a value from each pixel in the given image and/or subtract another image in a pixelwise manner. shapeworks readimage --name $1 subtract --value $2 --name $3 writeimage --name $4 Here is the list of arguments. value: Value to add to each pixel. name: Name of image to subtract pixelwise. Multiply This tool multiplies an image by a constant. shapeworks readimage --name $1 multiply --value $2 writeimage --name $3 Here is the list of arguments. value: Value with which to multiply. Divide This tool divides an image by a constant. shapeworks readimage --name $1 divide --value $2 writeimage --name $3 Here is the list of arguments. value: Value with which to divide. ImageToMesh This tool converts the current image to a mesh. Here is the list of arguments. isovalue: Isovalue to determine mesh boundary [default: 1.0]. Mesh Pre-Processing Reflect Mesh This tool reflects a mesh segmentation. It can be used when you want to align all pairs of anatomies together. For example, if you have left and right femurs and want to align them all, you would first reflect one side or the other so they are all either left or right femurs. ReflectMesh --inFilename $1 --outFilename $2 --reflectCenterFilename $3 --inputDirection $4 --meshFormat $5 inFilename: The filename of the input mesh to be reflected. outFilename: The filename of the output reflected mesh. reflectCenterFilename: The filename where the image center about which reflection occured will be stored. inputDirection: Direction along which to reflect. meshFormat: Mesh file format such as \"vtk\" or \"ply\" Mesh to volume This tool generates a binary volume and distance transform from a \".ply\" mesh. If an image corresponding to the mesh exists, the origin, spacing, and size settings should be the same as this image. This information can be acquired using the WriteImageInfoToText tool. TopologyPreservingSmoothing parameterFile.xml Here is the list of all parameters in the parameter file. * mesh: The file name of the input mesh. * origin_x: The x origin of the output \".nrrd\" files. * origin_y: The y origin of the output \".nrrd\" files. * origin_z: The z origin of the output \".nrrd\" files. * size_x: The x size of the output \".nrrd\" files. * size_y: The y size of the output \".nrrd\" files. * size_z: The z size of the output \".nrrd\" files. * spacing_x: The x spacing of the output \".nrrd\" files. * spacing_y: The y spacing of the output \".nrrd\" files. * spacing_z: The z spacing of the output \".nrrd\" files. Alignment","title":"How to Groom Your Dataset?"},{"location":"workflow/groom.html#how-to-groom-your-dataset","text":"ShapeWorks needs suitable distance transforms for establishing shape correspondence. The groom stage has the pipeline to generate the distance transforms from input images (binary segmentation or mesh). It consists of image/mesh pre-processing and alignment tools.","title":"How to Groom Your Dataset?"},{"location":"workflow/groom.html#image-pre-processing","text":"","title":"Image Pre-Processing"},{"location":"workflow/groom.html#antialias","text":"This tool antialiases binary volumes. shapeworks readimage --name $1 antialias maxrmserror $2 iterations $3 layers $4 writeimage --name $5 Here is the list of arguments. maxrmserror: Maximum RMS error determines how fast the solver converges. Range [0.0, 1.0], larger is faster [default: 0.01]. iterations: Maximum number of iterations [default: 50]. layers: Number of layers around a 3d pixel to use for this computation [default: 3].","title":"Antialias"},{"location":"workflow/groom.html#resample","text":"This tool uses an interpolation process to resample anisotropic voxels into isotropic-sized voxels. shapeworks readimage --name $1 resample --isospacing $2 --spacex $3 --spacey $4 --spacez $5 --sizex $6 --sizey $7 --sizez $8 writeimage --name $9 Here is the list of arguments. isospacing: The isotropic spacing in all dimensions. spacex: Pixel spacing in x-direction [default: 1.0]. spacey: Pixel spacing in y-direction [default: 1.0]. spacez: Pixel spacing in z-direction [default: 1.0]. sizex: Output size in x-direction [default: calculated using current size and desired spacing]. sizey: Output size in y-direction [default: calculated using current size and desired spacing]. sizez: Output size in z-direction [default: calculated using current size and desired spacing].","title":"Resample"},{"location":"workflow/groom.html#recenter","text":"This tool recenters an image by changing its origin in the image header to the physical coordinates of the center of the image. shapeworks readimage --name $1 recenter writeimage --name $2","title":"Recenter"},{"location":"workflow/groom.html#pad","text":"This tool pads pads an image with specified value by specified number of voxels in the x-, y-, and/or z- directions; origin remains at the same location (note: negative padding to shrink an image is permitted). shapeworks readimage --name $1 pad --padding $2 --value $3 writeimage --name $4 shapeworks readimage --name $1 pad --padx $2 --pady $3 --padz $4 writeimage --name $5 Here is the list of arguments. padding: Pad this many voxels in ALL directions (used if set) [default: 0]. padx: Pad this many voxels in the x-direction [default: 0]. pady: Pad this many voxels in the y-direction [default: 0]. padz: Pad this many voxels in the z-direction [default: 0]. value: Value used to fill padded voxels [default: 0.0].","title":"Pad"},{"location":"workflow/groom.html#translate","text":"This tool translates image by specified physical (image space) distance. shapeworks readimage --name $1 translate --centerofmass writeimage --name $2 shapeworks readimage --name $1 translate --tx $2 --ty $3 --tz $4 writeimage --name $5 Here is the list of arguments. centerofmass: Use center of mass [default: false]. tx: X distance ty: Y distance tz: Z distance","title":"Translate"},{"location":"workflow/groom.html#scale","text":"This tool scales image by specified value. shapeworks readimage --name $1 scale --sx $2 --sy $3 --sz $4 writeimage --name $5 Here is the list of arguments. tx: X scale ty: Y scale tz: Z scale","title":"Scale"},{"location":"workflow/groom.html#rotate","text":"This tool rotates image by specified value. shapeworks readimage --name $1 rotate --rx $2 --radians $3 writeimage --name $4 shapeworks readimage --name $1 rotate --degrees $2 writeimage --name $3 Here is the list of arguments. rx: Physical axis around which to rotate [default: z-axis] ry: Physical axis around which to rotate [default: z-axis] rz: Physical axis around which to rotate [default: z-axis] radians: Angle in radians degrees: Angle in degrees","title":"Rotate"},{"location":"workflow/groom.html#extract-label","text":"This tool extracts/isolates the specific voxel label from a given multi-label volume and outputs the corresponding binary image. shapeworks readimage --name $1 extractlabel label $2 writeimage --name $3 Here is the list of arguments. label: Label value to be extracted [default: 1.0].","title":"Extract Label"},{"location":"workflow/groom.html#close-holes","text":"This tool closes holes in a volume defined by values larger than specified value. shapeworks readimage --name $1 closeholes writeimage --name $2","title":"Close Holes"},{"location":"workflow/groom.html#binarize","text":"This tool sets portion of image greater than min and less than or equal to max to the specified value. shapeworks readimage --name $1 binarize --min $2 --max $3 --value $4 writeimage --name $5 Here is the list of arguments. min: Lower threshold level [default: 0.0]. max: Upper threshold level [default: inf ]. value: Value to set region [default: 1.0].","title":"Binarize"},{"location":"workflow/groom.html#compute-dt","text":"This tool computes signed distance transform volume from an image at the specified isovalue. shapeworks readimage --name $1 computedt --isovalue $2 writeimage --name $3 Here is the list of arguments. isovalue: Level set value that defines the interface between foreground and background [default: 0.0].","title":"Compute DT"},{"location":"workflow/groom.html#curvature","text":"This tool denoises an image using curvature driven flow using curvature flow image filter. shapeworks readimage --name $1 curvature --iterations $2 writeimage --name $3 Here is the list of arguments. iterations: Number of iterations [default: 10].","title":"Curvature"},{"location":"workflow/groom.html#gradient","text":"This tool computes gradient magnitude of an image region at each pixel using gradient magnitude filter. shapeworks readimage --name $1 curvature --iterations $2 writeimage --name $3 Here is the list of arguments. iterations: Number of iterations [default: 10].","title":"Gradient"},{"location":"workflow/groom.html#sigmoid","text":"This tool computes sigmoid function pixel-wise using sigmoid image filter. shapeworks readimage --name $1 sigmoid --alpha $2 --beta $3 writeimage --name $4 Here is the list of arguments. alpha: Value of alpha [default: 10.0]. beta: Value of beta [default: 10.0].","title":"Sigmoid"},{"location":"workflow/groom.html#tplevelsetfilter","text":"This tool segments structures in image using topology preserving geodesic active contour level set filter. shapeworks readimage --name $1 tplevelset --featureimage $2 --scaling $3 writeimage --name $4 Here is the list of arguments. featureimage: Path of feature image for filter scaling: Value of scale [default: 20.0].","title":"TPLevelSetFilter"},{"location":"workflow/groom.html#topologypreservingfilter","text":"This tool is a helper command that applies gradient and sigmoid filters to create a feature image for the TPLevelSet filter; note that the curvature flow filter is sometimes applied to the image before this. shapeworks readimage --name $1 topopreservingsmooth --scaling $2 --alpha $3 --beta $4 writeimage --name $5 Here is the list of arguments. scaling: Scale for TPLevelSet level set filter [default: 20.0]. alpha: Value of alpha for sigmoid fitler [default: 10.0]. beta: Value of beta for sigmoid fitler [default: 10.0.0].","title":"TopologyPreservingFilter"},{"location":"workflow/groom.html#blur","text":"This tool applies gaussian blur. shapeworks readimage --name $1 blur --sigma $2 writeimage --name $3 Here is the list of arguments. sigma: Value of sigma [default: 0.0].","title":"Blur"},{"location":"workflow/groom.html#icprigid","text":"This tool transform current image using iterative closest point (ICP) 3D rigid registration computed from source to target distance maps. shapeworks readimage --name $1 icp --source $2 --target $3 --isovalue $4 --iterations $5 writeimage --name $6 Here is the list of arguments. source: Distance map of source image. target: Distance map of target image. isovalue: Isovalue of distance maps used to create ICPtransform [default: 0.0]. iterations: Number of iterations run ICP registration [default: 20].","title":"ICPRigid"},{"location":"workflow/groom.html#bounding-box","text":"This tool computes the largest bounding box for the shape population needed for cropping all the images. shapeworks readimage --name $1 boundingbox --names $2 -- --padding $3 --isovalue $4 writeimage --name $5 Here is the list of arguments. names: Paths to images (must be followed by -- ), ex: \\\"bounding-box --names *.nrrd -- --isovalue 1.5\\\"). padding: Number of extra voxels in each direction to pad the largest bounding box [default: 0]. isovalue: Threshold value [default: 1.0].","title":"Bounding Box"},{"location":"workflow/groom.html#crop","text":"This tool crops image down to the current region (e.g., from bounding-box command), or the specified min/max in each direction [default: image dimensions]. shapeworks readimage --name $1 crop --xmin $2 --xmax $3 --ymin $4 --ymax $5 --zmin $6 --zmax $7 writeimage --name $8 Here is the list of arguments. xmin: Minimum X. xmax: Maximum X. ymin: Minimum Y. ymax: Maximum Y. zmin: Minimum Z. zmax: Maximum Z.","title":"Crop"},{"location":"workflow/groom.html#clip-volume","text":"This tool clips volume with the specified cutting planes defined by three 3D points shapeworks readimage --name $1 clip --x1 $2 --y1 $3 --z1 $4 --x2 $5 --y2 $6 --z2 $7 --x3 $8 --y3 $9 --z3 $10 writeimage --name $11 Here is the list of arguments. x1: Value of x1 for cutting plane [default: 0.0]. y1: Value of y1 for cutting plane [default: 0.0]. z1: Value of z1 for cutting plane [default: 0.0]. x2: Value of x2 for cutting plane [default: 0.0]. y2: Value of y2 for cutting plane [default: 0.0]. z2: Value of z2 for cutting plane [default: 0.0]. x3: Value of x3 for cutting plane [default: 0.0]. y3: Value of y3 for cutting plane [default: 0.0]. z3: Value of z3 for cutting plane [default: 0.0].","title":"Clip Volume"},{"location":"workflow/groom.html#reflect-images","text":"This tool reflects image with respect to logical image center and the specified axis. shapeworks readimage --name $1 reflect --axis $2 writeimage --name $3 Here is the list of arguments. axis: Axis along which to reflect (X, Y, or Z).","title":"Reflect Images"},{"location":"workflow/groom.html#set-origin","text":"This tool sets origin. shapeworks readimage --name $1 setorigin --x $2 --y $3 --z $4 writeimage --name $5 Here is the list of arguments. x: x value of origin [default: 0.0]. y: y value of origin [default: 0.0]. z: z value of origin [default: 0.0].","title":"Set Origin"},{"location":"workflow/groom.html#warp","text":"This tool finds the warp between the source and target landmarks and transforms image by this warp. shapeworks readimage --name $1 warpimage --source $2 --target $3 --landmarks $4 writeimage --name $5 Here is the list of arguments. source: Path to source landmarks. target: Path to target landmarks. landmarks: Every stride points will be used for warping [default: 1].","title":"Warp"},{"location":"workflow/groom.html#compare","text":"This tool compares two images. shapeworks readimage --name $1 compare --name $2 --verifyall $4 --tolerance $4 --precision $5 writeimage --name $6 Here is the list of arguments. name: Compare this image with another verifyall: Also verify origin, spacing, and direction matches [default: true]. tolerance: Allowed percentage of pixel differences [default: 0.0]. precision: Allowed difference between two pixels for them to still be considered equal [default: 0.0].","title":"Compare"},{"location":"workflow/groom.html#negate","text":"This tool negates the values in the given image. shapeworks readimage --name $1 negate writeimage --name $2","title":"Negate"},{"location":"workflow/groom.html#add","text":"This tool adds a value to each pixel in the given image and/or add another image in a pixelwise manner. shapeworks readimage --name $1 add --value $2 --name $3 writeimage --name $4 Here is the list of arguments. value: Value to add to each pixel. name: Name of image to add pixelwise.","title":"Add"},{"location":"workflow/groom.html#subtract","text":"This tool subtracts a value from each pixel in the given image and/or subtract another image in a pixelwise manner. shapeworks readimage --name $1 subtract --value $2 --name $3 writeimage --name $4 Here is the list of arguments. value: Value to add to each pixel. name: Name of image to subtract pixelwise.","title":"Subtract"},{"location":"workflow/groom.html#multiply","text":"This tool multiplies an image by a constant. shapeworks readimage --name $1 multiply --value $2 writeimage --name $3 Here is the list of arguments. value: Value with which to multiply.","title":"Multiply"},{"location":"workflow/groom.html#divide","text":"This tool divides an image by a constant. shapeworks readimage --name $1 divide --value $2 writeimage --name $3 Here is the list of arguments. value: Value with which to divide.","title":"Divide"},{"location":"workflow/groom.html#imagetomesh","text":"This tool converts the current image to a mesh. Here is the list of arguments. isovalue: Isovalue to determine mesh boundary [default: 1.0].","title":"ImageToMesh"},{"location":"workflow/groom.html#mesh-pre-processing","text":"","title":"Mesh Pre-Processing"},{"location":"workflow/groom.html#reflect-mesh","text":"This tool reflects a mesh segmentation. It can be used when you want to align all pairs of anatomies together. For example, if you have left and right femurs and want to align them all, you would first reflect one side or the other so they are all either left or right femurs. ReflectMesh --inFilename $1 --outFilename $2 --reflectCenterFilename $3 --inputDirection $4 --meshFormat $5 inFilename: The filename of the input mesh to be reflected. outFilename: The filename of the output reflected mesh. reflectCenterFilename: The filename where the image center about which reflection occured will be stored. inputDirection: Direction along which to reflect. meshFormat: Mesh file format such as \"vtk\" or \"ply\"","title":"Reflect Mesh"},{"location":"workflow/groom.html#mesh-to-volume","text":"This tool generates a binary volume and distance transform from a \".ply\" mesh. If an image corresponding to the mesh exists, the origin, spacing, and size settings should be the same as this image. This information can be acquired using the WriteImageInfoToText tool. TopologyPreservingSmoothing parameterFile.xml Here is the list of all parameters in the parameter file. * mesh: The file name of the input mesh. * origin_x: The x origin of the output \".nrrd\" files. * origin_y: The y origin of the output \".nrrd\" files. * origin_z: The z origin of the output \".nrrd\" files. * size_x: The x size of the output \".nrrd\" files. * size_y: The y size of the output \".nrrd\" files. * size_z: The z size of the output \".nrrd\" files. * spacing_x: The x spacing of the output \".nrrd\" files. * spacing_y: The y spacing of the output \".nrrd\" files. * spacing_z: The z spacing of the output \".nrrd\" files.","title":"Mesh to volume"},{"location":"workflow/groom.html#alignment","text":"","title":"Alignment"},{"location":"workflow/optimize.html","text":"How to Optimize Your Shape Model? Particle-based Shape Modeling ShapeWorks constructs statistically optimal anatomical mapping across different shape samples by automatically computing a dense set of corresponding landmark positions that are geometrically consistent on a set of anatomy segmentations or surface meshes and does not rely on any specific surface parameterization . ShapeWorks uses a set of interacting particle systems , one for each shape, to produce optimal sets of surface correspondences in an ensemble. Particles interact with one another via mutually repelling forces to cover optimally and, therefore, describe surface geometry. Particles are positioned on surfaces automatically by optimizing the model's information content via an entropy optimization scheme. ShapeWorks optimizes landmark positions to minimize the overall information content of the model (first term) while maintaining a good sampling of surface geometry (second term) Particle-based Representation More formally, consider a cohort of shapes \\(\\mathcal{S} = \\{\\mathbf{z}_1, \\mathbf{z}_2, ..., \\mathbf{z}_N\\}\\) of \\(N\\) surfaces, each with its own set of \\(M\\) corresponding particles \\(\\mathbf{z}_n = [\\mathbf{z}_n^1, \\mathbf{z}_n^2, ..., \\mathbf{z}_n^M] \\in \\mathbb{R}^{dM}\\) where each particle \\(\\mathbf{z}_n^m \\in \\mathbb{R}^d\\) lives in a \\(d-\\) dimensional Cartesian space (typically \\(d=3\\) for anatomies), and whose ordering implies correspondence among shapes. Each of the particles is called a correspondence point, and is constrained to lie on the shape's surface. Collectively, the set of \\(M\\) particles is known as the configuration . Shape vs. Configuration Spaces This particle-based representation incorporates two types of random variables: a shape space variable \\(\\mathbf{Z} \\in \\mathbb{R}^{dM}\\) and a particle position variable \\(\\mathbf{X}_n \\in \\mathbb{R}^d\\) that encodes the distribution of particles on the \\(n-\\) th shape ( configuration space ). This particle-based representation avoids many of the problems inherent in parametric representations such as the limitation to specific topologies, processing steps necessary to construct parameterizations, and bias toward model initialization. World vs. Local Coordinates For groupwise modeling, shapes in the shape space should share the same world coordinate system. Hence, we use generalized Procrustes alignment to estimate a rigid transformation matrix \\(\\mathbf{T}_n\\) that can transform the particles in the \\(n-\\) th shape local coordinate \\(\\mathbf{x}_n^m\\) in the configuration space to the world common coordinate \\(\\mathbf{z}_n^m\\) in the shape space such that \\(\\mathbf{z}_n^m = \\mathbf{T}_n\\mathbf{x}_n^m\\) . Optimization Cost Function Correspondences are established by minimizing a combined shape correspondence and surface sampling cost function \\(Q = \\alpha H(\\mathbf{Z}) - \\sum_{n=1}^N H(\\mathbf{X}_n)\\) , where \\(H\\) is an entropy estimation of the shape distribution in the shape space, and \\(\\alpha\\) is the relative weighting of the correspondence term. In particular, ShapeWorks explicitly models the inherent trade-off between the statistical simplicity of the model (i.e., compactness or lowest entropy) in the shape space (i.e., inter-surface) and the accuracy of the shape representations (i.e., good surface samplings or highest entropy) in the configuration space (i.e., intra-surface). The cost function \\(Q\\) is minimized using gradient descent with an adaptive time step. Because correspondence points (or particles) in this formulation are not tied to a specific surface parameterization, the method operates directly on both volumetric data and triangular surface meshes. It can also be easily extended to arbitrary shapes, even nonmanifold surfaces. Particles Initialization & Optimization ShapeWorks entails a nonconvex optimization problem. Hence, it is not practical to perform the optimization of the configuration space (intra-surface) and the shape space (inter-surface) with a full set of correspondence points (or particles) in one step. We address this using a coarse-to-fine optimization scheme to speed up convergence to an acceptable local minimum. In particular, the optimization is performed as a multi-step process where particles are added via spitting each particle to produce a new, nearby particle at each step until the desired number of particles is reached. ShapeWorks uses a particle splitting strategy, in which the full set of particles is optimized in a multi-scale (i.e., coarse-to-fine) fashion For these steps, the optimization of the configuration space (intra-surface) and the shape space (inter-surface) is weighted to downplay the effect of the correspondence term (default \\(\\alpha = 0.05\\) ), which results in an evenly spaced distribution of particles on each surface. These steps are collectively called the initialization steps. At each scale, the initialization step is followed by an optimization step. For this step, the optimization of the configuration space (intra-surface) and the shape space (inter-surface) are weighted (equally or sometimes using \\(\\alpha > 1\\) to emphasize the correspondence term). Thus, the initialization proceeds simultaneously with the optimization in a multi-scale fashion, generating progressively more detailed correspondence models with each split. For both, the initialization and optimization steps, the weighting to the shape space may be set by the user. Further, as each step of the optimization is an iterative process, the number of iterations may be set by the user. At each scale, the number of iterations could impact the quality of the optimized model The first particle: The particle system is initialized with a single particle on each shape. The first particle is found by raster-scanning the signed distance map and finding the first zero crossing. The particle system can also be initialized using user-defined sparse corresponding landmarks across all shapes. On Algorithmic Parameters Optimizing the shape models entails several algorithmic parameters. Below, we highlight the most important ones that might need tuning depending on the dataset at hand. Correspondence Relative Weighting One difference between initialization and optimization steps is how important the correspondence (inter-surface) objective is compared to the surface sampling (intra-surface) term using a relative weighting factor for the correspondence term (i.e., \\(\\alpha\\) in \\(Q\\) ). Hence initial_relative_weighting is the weight (or \\(\\alpha\\) ) used in initialization steps and the relative_weighing is the weight (or \\(\\alpha\\) ) used for optimization steps. Typically initial_relative_weighting is selected to be small (in the order of 0.01) to enable particles to be uniformly distributed (i.e., evenly spaced) over each shape, and hence optimization starts with a good surface sampling. It can be noted that by allowing correspondence to dominate the optimization process (using higher relative weighting), particles tend to be distributed in regions with relatively small variability across the given population. As the relative weighting tends to infinity, particles will be cluttered in one spot on each surface, which means that all shapes will be represented as a point at the shape space origin. Also, using lower relative weighting, i.e., allowing surface sampling to dominate the optimization process, results in particles becoming out-of-correspondence. As we increase the relative_weighting , i.e., the correspondence term weight, particles tend to be distributed over surface regions that have less variability across shape samples; hence the shape distribution in the shape space tends to collapse to a single point (i.e., shape) Shape Statistics in Initialization and Optimization Steps At earlier scales, we do not have enough particles to describe the geometry of each surface. Hence, to quantify the notion of correspondence (inter-surface), we use mean energy (i.e., pushing all shapes in the shape space to the mean shape or, in other words, the covariance matrix is assumed to be identity). As more particles are added to the correspondence model, we use the entropy of the distribution of the shapes (assumed to be Gaussian distributed), where we have more particles that can reveal the covariance structure of the shape space. This behavior is controlled by the use_shape_statistics_after parameter, which specifies the number of particles, after which shape statistics can be used in the initialization and optimization steps. Using shape statistics (i.e., covaraince structure) results in a better correspondence over iterations, below we use use_shape_statistics_after after 1024 particles Starting and Ending Regularization Particle movement during optimization (due to the correspondence term) entails computing the covariance matrix's inverse. We regularize the covariance matrix to handle degenerate covariances. starting_regularization and ending_regularization parameters determine the covariance matrix's regularization for the shape-space entropy estimation. This regularization exponentially decays along with optimization iterations where better covariance structure can be estimated with a better correspondence model. Higher regularization values would undermine the ensemble's underlying covariance structure and favors all shapes to converge to the mean shape. Hence, it is recommended to use starting regularization value as ~5% of the covariance matrix's expected highest eigenvalue while ending regularization can be taken as ten times less than the starting value. This regularization can be considered as having a Gaussian ball in the shape space. Starting regularization pushes all samples to the mean and hides the underlying \u201cunoptimized\u201d covariance structure. Ending regularization should be small enough to reveal the optimized covariance structure. Optimizing Correspondences You can use either ShapeWorksStudio or shapeworks optimize <parameters.xml> command to optimize your shape model. Both use a set of algorithmic parameters to control the optimization process. XML Parameter File Here is the list of the parameters to be included in the <parameters.xml> file. <inputs> : List of surface meshes or distance transforms (i.e., groom stage output) that comprises your dataset. <output_dir> : The directory to save the output produced by the ShapeWorks optimization. <domain_type> : (default: image) The type of the domain in <inputs> , image for signed distance transforms, and mesh for triangular surface meshes. <domains_per_shape> : (default: 1) The number of domains for anatomies with multiple structures (domains), e.g., joints. The list of <inputs> should be ordered to list a consistent order of the domains (surface mesh or distance transform) of each shape (e.g., shape1-domain1, shape1-domain2, shape2-domain1, shape2-domain2 ... etc.). <narrow_band> : (default: 4.0 of <fixed_domains> is not active) The off-surface distance (in physical units) used to truncate (zero out) distance values for signed distance transforms beyond the narrow band radius (i.e., a narrow band of 4.0 preserve distance values within +/- 4.0 units off the surface). This is used to reduce the memory footprint required for keeping volumetric distance transforms in memory. If you get an error that particles are shooting outside the narrow band, please consider increasing this value. Narrow banding is disabled if <fixed_domains> is active. This does not significantly affect the memory footprint since distance transforms of the fixed domains are not loaded in memory. <number_of_particles> : The desired number of particles to be placed. ShapeWorks will produce the smallest power-of-2 number of particles greater than or equal the given <number_of_particles> . <iterations_per_split> : (default: 1000) The number of iterations in the initialization step for each split (i.e., scale). <optimization_iterations> : (default: 2000) Number of iterations for each optimization step. <save_init_splits> : (default: 1) A flag to save the particles for each split in the initialization steps. <use_xyz> : (default: 1) A flag to enable using the xyz coordinates for particles as a feature for correspondence. <use_normals> : (default: 0) A flag to consider surface normals (along with particles positions, i.e., <use_xyz> 1 </use_xyz> ) as a correspondence feature. <attribute_scales> : A vector of weights that scale each dimension considered in the correspondence entropy. For example, if only xyz coordinates are used for particles, so <attribute_scales> is a vector of three weights that scale the x- and y- and z- coordinates of the particle (default = 1). If surface normals are also used, <attribute_scales> should be a vector of 6 entries, 3 for the xyz coordinates, and 3 for the surface normal (usually in scale of 10), assuming 3D anatomies. <use_shape_statistics_after> : (default: -1) (e.g., 32 or 64) The number of particles after which to use shape space entropy (i.e., the covariance structure) in the initialization and optimization steps. Use -1 if you want shape statistics only used when the number of particles reach the desired <number_of_particles> . Hence, all initialization and optimization steps will use mean energy till the <number_of_particles> is reached. <starting_regularization> : (default: 1000) Sets the starting regularization value (usually high value). <ending_regularization> : (default: 1.0) Sets the ending regularization value (usually small value less than 1). <recompute_regularization_interval> : (default: 1) Skip interval (i.e., number of iterations) to exponentially decay the regularization value. <initial_relative_weighting> : (default: 0.05) The relative weight of the correspondence term in the initialization steps to make sure that optimization steps start with evenly spaced particle distributions that cover the entire surfaces (usually in the order of ~0.1 or 0.01). <relative_weighting> : (default: 1) The relative weight of the correspondence term in the optimization steps. <procrustes_scaling> : (default: 1) A flag to enable factoring out scaling in the shape space when performing Procrustes alignment. <procrustes_interval> : (default: 3) Number of iterations (interval) between performing Procrustes alignment, use 0 to turn Procrustes off. <mesh_based_attributes> : (default: 0) A flag that should be enabled when <use_normals> is enabled to cache and interpolate surface normals using isosurfaces. <keep_checkpoints> : (default: 0) A flag to save the shape (correspondence) models through the initialization/optimization steps for debugging and troubleshooting. <checkpointing_interval> : (default: 50) The interval (number of iterations) to be used to save the checkpoints. <verbosity> : (default: 0) '0' : almost zero verbosity (error messages only), '1': minimal verbosity (notification of running initialization/optimization steps), '2': additional details about parameters read from xml and files written, '3': full verbosity. <adaptivity_mode> : (default: 0) Used to change the expected behavior of the particles sampler, where the sampler is expected to distribute evenly spaced particles to cover all the surface. Currently, 0 is used to trigger the update project method of cutting planes. ' `: Number of cutting planes for each shape if constrained particle optimization is used. <cutting_planes> : A list of cutting planes to be used for all shapes. Each cutting plane is defined by three points in an order that indicates the plane's normal, i.e., the direction where particles are allowed to be distributed. Parameter Dictionary in Python In Examples/Python/<use-case-name.py> , we use a python dictionary to expose important algorithmic parameters and automatically generate the XML parameter file. Below is a list of the currently exposed algorithmic parameters. All the keys of this parameter dictionary correspond to the XML tags , except for \"normal_weight\" that sets the <attribute_scales> of the surface normal vector. { \"number_of_particles\": 1024, \"use_shape_statistics_after\": 32, \"use_normals\": 0, \"normal_weight\": 0.0, \"checkpointing_interval\" : 10000, \"keep_checkpoints\" : 0, \"iterations_per_split\" : 4000, \"optimization_iterations\" : 500, \"starting_regularization\" : 10, \"ending_regularization\" : 1, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"domain_type\" : 'mesh', \"relative_weighting\" : 10, \"initial_relative_weighting\" : 1, \"procrustes_interval\" : 1, \"procrustes_scaling\" : 1, \"save_init_splits\" : 0, \"verbosity\" : 2, } Correspondences on New Samples ShapeWorks supports an optimization mode, namely fixed domains , to place (i.e., optimize) correspondences on new shapes using a pre-existing shape model. In the fixed domains mode, particles on selected shapes that construct the pre-existing shape model are fixed, and particles on new shapes are optimized to represent them in the context of this shape model. See Fixed Domains for Ellipsoid: Correspondences on New Shape for an example. To enable the fixed domains mode, the XML should have the below additional tags. For this mode, you can use \"use_shape_statistics_after\": 0 to enable shape statistics in all the steps as the pre-existing shape model already has enough particles optimized to reflect the covariance structure in the shape space. <point_files> : A list of local.particles files to be fixed, i.e., the pre-existing shape model. The new (to be optimized) samples/domains should be initialized with the mean particles. <fixed_domains> : A list of domain ids (starting from 0) of the domains that are fixed (i.e., not optimized).","title":"How to Optimize Your Shape Model?"},{"location":"workflow/optimize.html#how-to-optimize-your-shape-model","text":"","title":"How to Optimize Your Shape Model?"},{"location":"workflow/optimize.html#particle-based-shape-modeling","text":"ShapeWorks constructs statistically optimal anatomical mapping across different shape samples by automatically computing a dense set of corresponding landmark positions that are geometrically consistent on a set of anatomy segmentations or surface meshes and does not rely on any specific surface parameterization . ShapeWorks uses a set of interacting particle systems , one for each shape, to produce optimal sets of surface correspondences in an ensemble. Particles interact with one another via mutually repelling forces to cover optimally and, therefore, describe surface geometry. Particles are positioned on surfaces automatically by optimizing the model's information content via an entropy optimization scheme. ShapeWorks optimizes landmark positions to minimize the overall information content of the model (first term) while maintaining a good sampling of surface geometry (second term)","title":"Particle-based Shape Modeling"},{"location":"workflow/optimize.html#particle-based-representation","text":"More formally, consider a cohort of shapes \\(\\mathcal{S} = \\{\\mathbf{z}_1, \\mathbf{z}_2, ..., \\mathbf{z}_N\\}\\) of \\(N\\) surfaces, each with its own set of \\(M\\) corresponding particles \\(\\mathbf{z}_n = [\\mathbf{z}_n^1, \\mathbf{z}_n^2, ..., \\mathbf{z}_n^M] \\in \\mathbb{R}^{dM}\\) where each particle \\(\\mathbf{z}_n^m \\in \\mathbb{R}^d\\) lives in a \\(d-\\) dimensional Cartesian space (typically \\(d=3\\) for anatomies), and whose ordering implies correspondence among shapes. Each of the particles is called a correspondence point, and is constrained to lie on the shape's surface. Collectively, the set of \\(M\\) particles is known as the configuration .","title":"Particle-based Representation"},{"location":"workflow/optimize.html#shape-vs-configuration-spaces","text":"This particle-based representation incorporates two types of random variables: a shape space variable \\(\\mathbf{Z} \\in \\mathbb{R}^{dM}\\) and a particle position variable \\(\\mathbf{X}_n \\in \\mathbb{R}^d\\) that encodes the distribution of particles on the \\(n-\\) th shape ( configuration space ). This particle-based representation avoids many of the problems inherent in parametric representations such as the limitation to specific topologies, processing steps necessary to construct parameterizations, and bias toward model initialization.","title":"Shape vs. Configuration Spaces"},{"location":"workflow/optimize.html#world-vs-local-coordinates","text":"For groupwise modeling, shapes in the shape space should share the same world coordinate system. Hence, we use generalized Procrustes alignment to estimate a rigid transformation matrix \\(\\mathbf{T}_n\\) that can transform the particles in the \\(n-\\) th shape local coordinate \\(\\mathbf{x}_n^m\\) in the configuration space to the world common coordinate \\(\\mathbf{z}_n^m\\) in the shape space such that \\(\\mathbf{z}_n^m = \\mathbf{T}_n\\mathbf{x}_n^m\\) .","title":"World vs. Local Coordinates"},{"location":"workflow/optimize.html#optimization-cost-function","text":"Correspondences are established by minimizing a combined shape correspondence and surface sampling cost function \\(Q = \\alpha H(\\mathbf{Z}) - \\sum_{n=1}^N H(\\mathbf{X}_n)\\) , where \\(H\\) is an entropy estimation of the shape distribution in the shape space, and \\(\\alpha\\) is the relative weighting of the correspondence term. In particular, ShapeWorks explicitly models the inherent trade-off between the statistical simplicity of the model (i.e., compactness or lowest entropy) in the shape space (i.e., inter-surface) and the accuracy of the shape representations (i.e., good surface samplings or highest entropy) in the configuration space (i.e., intra-surface). The cost function \\(Q\\) is minimized using gradient descent with an adaptive time step. Because correspondence points (or particles) in this formulation are not tied to a specific surface parameterization, the method operates directly on both volumetric data and triangular surface meshes. It can also be easily extended to arbitrary shapes, even nonmanifold surfaces.","title":"Optimization Cost Function"},{"location":"workflow/optimize.html#particles-initialization-optimization","text":"ShapeWorks entails a nonconvex optimization problem. Hence, it is not practical to perform the optimization of the configuration space (intra-surface) and the shape space (inter-surface) with a full set of correspondence points (or particles) in one step. We address this using a coarse-to-fine optimization scheme to speed up convergence to an acceptable local minimum. In particular, the optimization is performed as a multi-step process where particles are added via spitting each particle to produce a new, nearby particle at each step until the desired number of particles is reached. ShapeWorks uses a particle splitting strategy, in which the full set of particles is optimized in a multi-scale (i.e., coarse-to-fine) fashion For these steps, the optimization of the configuration space (intra-surface) and the shape space (inter-surface) is weighted to downplay the effect of the correspondence term (default \\(\\alpha = 0.05\\) ), which results in an evenly spaced distribution of particles on each surface. These steps are collectively called the initialization steps. At each scale, the initialization step is followed by an optimization step. For this step, the optimization of the configuration space (intra-surface) and the shape space (inter-surface) are weighted (equally or sometimes using \\(\\alpha > 1\\) to emphasize the correspondence term). Thus, the initialization proceeds simultaneously with the optimization in a multi-scale fashion, generating progressively more detailed correspondence models with each split. For both, the initialization and optimization steps, the weighting to the shape space may be set by the user. Further, as each step of the optimization is an iterative process, the number of iterations may be set by the user. At each scale, the number of iterations could impact the quality of the optimized model The first particle: The particle system is initialized with a single particle on each shape. The first particle is found by raster-scanning the signed distance map and finding the first zero crossing. The particle system can also be initialized using user-defined sparse corresponding landmarks across all shapes.","title":"Particles Initialization &amp; Optimization"},{"location":"workflow/optimize.html#on-algorithmic-parameters","text":"Optimizing the shape models entails several algorithmic parameters. Below, we highlight the most important ones that might need tuning depending on the dataset at hand.","title":"On Algorithmic Parameters"},{"location":"workflow/optimize.html#correspondence-relative-weighting","text":"One difference between initialization and optimization steps is how important the correspondence (inter-surface) objective is compared to the surface sampling (intra-surface) term using a relative weighting factor for the correspondence term (i.e., \\(\\alpha\\) in \\(Q\\) ). Hence initial_relative_weighting is the weight (or \\(\\alpha\\) ) used in initialization steps and the relative_weighing is the weight (or \\(\\alpha\\) ) used for optimization steps. Typically initial_relative_weighting is selected to be small (in the order of 0.01) to enable particles to be uniformly distributed (i.e., evenly spaced) over each shape, and hence optimization starts with a good surface sampling. It can be noted that by allowing correspondence to dominate the optimization process (using higher relative weighting), particles tend to be distributed in regions with relatively small variability across the given population. As the relative weighting tends to infinity, particles will be cluttered in one spot on each surface, which means that all shapes will be represented as a point at the shape space origin. Also, using lower relative weighting, i.e., allowing surface sampling to dominate the optimization process, results in particles becoming out-of-correspondence. As we increase the relative_weighting , i.e., the correspondence term weight, particles tend to be distributed over surface regions that have less variability across shape samples; hence the shape distribution in the shape space tends to collapse to a single point (i.e., shape)","title":"Correspondence Relative Weighting"},{"location":"workflow/optimize.html#shape-statistics-in-initialization-and-optimization-steps","text":"At earlier scales, we do not have enough particles to describe the geometry of each surface. Hence, to quantify the notion of correspondence (inter-surface), we use mean energy (i.e., pushing all shapes in the shape space to the mean shape or, in other words, the covariance matrix is assumed to be identity). As more particles are added to the correspondence model, we use the entropy of the distribution of the shapes (assumed to be Gaussian distributed), where we have more particles that can reveal the covariance structure of the shape space. This behavior is controlled by the use_shape_statistics_after parameter, which specifies the number of particles, after which shape statistics can be used in the initialization and optimization steps. Using shape statistics (i.e., covaraince structure) results in a better correspondence over iterations, below we use use_shape_statistics_after after 1024 particles","title":"Shape Statistics in Initialization and  Optimization Steps"},{"location":"workflow/optimize.html#starting-and-ending-regularization","text":"Particle movement during optimization (due to the correspondence term) entails computing the covariance matrix's inverse. We regularize the covariance matrix to handle degenerate covariances. starting_regularization and ending_regularization parameters determine the covariance matrix's regularization for the shape-space entropy estimation. This regularization exponentially decays along with optimization iterations where better covariance structure can be estimated with a better correspondence model. Higher regularization values would undermine the ensemble's underlying covariance structure and favors all shapes to converge to the mean shape. Hence, it is recommended to use starting regularization value as ~5% of the covariance matrix's expected highest eigenvalue while ending regularization can be taken as ten times less than the starting value. This regularization can be considered as having a Gaussian ball in the shape space. Starting regularization pushes all samples to the mean and hides the underlying \u201cunoptimized\u201d covariance structure. Ending regularization should be small enough to reveal the optimized covariance structure.","title":"Starting and Ending Regularization"},{"location":"workflow/optimize.html#optimizing-correspondences","text":"You can use either ShapeWorksStudio or shapeworks optimize <parameters.xml> command to optimize your shape model. Both use a set of algorithmic parameters to control the optimization process.","title":"Optimizing Correspondences"},{"location":"workflow/optimize.html#xml-parameter-file","text":"Here is the list of the parameters to be included in the <parameters.xml> file. <inputs> : List of surface meshes or distance transforms (i.e., groom stage output) that comprises your dataset. <output_dir> : The directory to save the output produced by the ShapeWorks optimization. <domain_type> : (default: image) The type of the domain in <inputs> , image for signed distance transforms, and mesh for triangular surface meshes. <domains_per_shape> : (default: 1) The number of domains for anatomies with multiple structures (domains), e.g., joints. The list of <inputs> should be ordered to list a consistent order of the domains (surface mesh or distance transform) of each shape (e.g., shape1-domain1, shape1-domain2, shape2-domain1, shape2-domain2 ... etc.). <narrow_band> : (default: 4.0 of <fixed_domains> is not active) The off-surface distance (in physical units) used to truncate (zero out) distance values for signed distance transforms beyond the narrow band radius (i.e., a narrow band of 4.0 preserve distance values within +/- 4.0 units off the surface). This is used to reduce the memory footprint required for keeping volumetric distance transforms in memory. If you get an error that particles are shooting outside the narrow band, please consider increasing this value. Narrow banding is disabled if <fixed_domains> is active. This does not significantly affect the memory footprint since distance transforms of the fixed domains are not loaded in memory. <number_of_particles> : The desired number of particles to be placed. ShapeWorks will produce the smallest power-of-2 number of particles greater than or equal the given <number_of_particles> . <iterations_per_split> : (default: 1000) The number of iterations in the initialization step for each split (i.e., scale). <optimization_iterations> : (default: 2000) Number of iterations for each optimization step. <save_init_splits> : (default: 1) A flag to save the particles for each split in the initialization steps. <use_xyz> : (default: 1) A flag to enable using the xyz coordinates for particles as a feature for correspondence. <use_normals> : (default: 0) A flag to consider surface normals (along with particles positions, i.e., <use_xyz> 1 </use_xyz> ) as a correspondence feature. <attribute_scales> : A vector of weights that scale each dimension considered in the correspondence entropy. For example, if only xyz coordinates are used for particles, so <attribute_scales> is a vector of three weights that scale the x- and y- and z- coordinates of the particle (default = 1). If surface normals are also used, <attribute_scales> should be a vector of 6 entries, 3 for the xyz coordinates, and 3 for the surface normal (usually in scale of 10), assuming 3D anatomies. <use_shape_statistics_after> : (default: -1) (e.g., 32 or 64) The number of particles after which to use shape space entropy (i.e., the covariance structure) in the initialization and optimization steps. Use -1 if you want shape statistics only used when the number of particles reach the desired <number_of_particles> . Hence, all initialization and optimization steps will use mean energy till the <number_of_particles> is reached. <starting_regularization> : (default: 1000) Sets the starting regularization value (usually high value). <ending_regularization> : (default: 1.0) Sets the ending regularization value (usually small value less than 1). <recompute_regularization_interval> : (default: 1) Skip interval (i.e., number of iterations) to exponentially decay the regularization value. <initial_relative_weighting> : (default: 0.05) The relative weight of the correspondence term in the initialization steps to make sure that optimization steps start with evenly spaced particle distributions that cover the entire surfaces (usually in the order of ~0.1 or 0.01). <relative_weighting> : (default: 1) The relative weight of the correspondence term in the optimization steps. <procrustes_scaling> : (default: 1) A flag to enable factoring out scaling in the shape space when performing Procrustes alignment. <procrustes_interval> : (default: 3) Number of iterations (interval) between performing Procrustes alignment, use 0 to turn Procrustes off. <mesh_based_attributes> : (default: 0) A flag that should be enabled when <use_normals> is enabled to cache and interpolate surface normals using isosurfaces. <keep_checkpoints> : (default: 0) A flag to save the shape (correspondence) models through the initialization/optimization steps for debugging and troubleshooting. <checkpointing_interval> : (default: 50) The interval (number of iterations) to be used to save the checkpoints. <verbosity> : (default: 0) '0' : almost zero verbosity (error messages only), '1': minimal verbosity (notification of running initialization/optimization steps), '2': additional details about parameters read from xml and files written, '3': full verbosity. <adaptivity_mode> : (default: 0) Used to change the expected behavior of the particles sampler, where the sampler is expected to distribute evenly spaced particles to cover all the surface. Currently, 0 is used to trigger the update project method of cutting planes. ' `: Number of cutting planes for each shape if constrained particle optimization is used. <cutting_planes> : A list of cutting planes to be used for all shapes. Each cutting plane is defined by three points in an order that indicates the plane's normal, i.e., the direction where particles are allowed to be distributed.","title":"XML Parameter File"},{"location":"workflow/optimize.html#parameter-dictionary-in-python","text":"In Examples/Python/<use-case-name.py> , we use a python dictionary to expose important algorithmic parameters and automatically generate the XML parameter file. Below is a list of the currently exposed algorithmic parameters. All the keys of this parameter dictionary correspond to the XML tags , except for \"normal_weight\" that sets the <attribute_scales> of the surface normal vector. { \"number_of_particles\": 1024, \"use_shape_statistics_after\": 32, \"use_normals\": 0, \"normal_weight\": 0.0, \"checkpointing_interval\" : 10000, \"keep_checkpoints\" : 0, \"iterations_per_split\" : 4000, \"optimization_iterations\" : 500, \"starting_regularization\" : 10, \"ending_regularization\" : 1, \"recompute_regularization_interval\" : 2, \"domains_per_shape\" : 1, \"domain_type\" : 'mesh', \"relative_weighting\" : 10, \"initial_relative_weighting\" : 1, \"procrustes_interval\" : 1, \"procrustes_scaling\" : 1, \"save_init_splits\" : 0, \"verbosity\" : 2, }","title":"Parameter Dictionary in Python"},{"location":"workflow/optimize.html#correspondences-on-new-samples","text":"ShapeWorks supports an optimization mode, namely fixed domains , to place (i.e., optimize) correspondences on new shapes using a pre-existing shape model. In the fixed domains mode, particles on selected shapes that construct the pre-existing shape model are fixed, and particles on new shapes are optimized to represent them in the context of this shape model. See Fixed Domains for Ellipsoid: Correspondences on New Shape for an example. To enable the fixed domains mode, the XML should have the below additional tags. For this mode, you can use \"use_shape_statistics_after\": 0 to enable shape statistics in all the steps as the pre-existing shape model already has enough particles optimized to reflect the covariance structure in the shape space. <point_files> : A list of local.particles files to be fixed, i.e., the pre-existing shape model. The new (to be optimized) samples/domains should be initialized with the mean particles. <fixed_domains> : A list of domain ids (starting from 0) of the domains that are fixed (i.e., not optimized).","title":"Correspondences on New Samples"}]}